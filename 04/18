{"id":"2104.08588","submitter":"Shengxuan Luo","authors":"Shengxuan Luo, Huaiyuan Ying, Jiao Li, Sheng Yu","title":"Sentence Alignment with Parallel Documents Facilitates Biomedical\n  Machine Translation","comments":"16 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Objective: Today's neural machine translation (NMT) can achieve near\nhuman-level translation quality and greatly facilitates international\ncommunications, but the lack of parallel corpora poses a key problem to the\ndevelopment of translation systems for highly specialized domains, such as\nbiomedicine. This work presents an unsupervised algorithm for deriving parallel\ncorpora from document-level translations by using sentence alignment and\nexplores how training materials affect the performance of biomedical NMT\nsystems. Materials and Methods: Document-level translations are mixed to train\nbilingual word embeddings (BWEs) for the evaluation of cross-lingual word\nsimilarity, and sentence distance is defined by combining semantic and\npositional similarities of the sentences. The alignment of sentences is\nformulated as an extended earth mover's distance problem. A Chinese-English\nbiomedical parallel corpus is derived with the proposed algorithm using\nbilingual articles from UpToDate and translations of PubMed abstracts, which is\nthen used for the training and evaluation of NMT. Results: On two manually\naligned translation datasets, the proposed algorithm achieved accurate sentence\nalignment in the 1-to-1 cases and outperformed competing algorithms in the\nmany-to-many cases. The NMT model fine-tuned on biomedical data significantly\nimproved the in-domain translation quality (zh-en: +17.72 BLEU; en-zh: +17.02\nBLEU). Both the size of the training data and the combination of different\ncorpora can significantly affect the model's performance. Conclusion: The\nproposed algorithm relaxes the assumption for sentence alignment and\neffectively generates accurate translation pairs that facilitate training high\nquality biomedical NMT models.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 16:09:30 GMT"},{"version":"v2","created":"Mon, 7 Feb 2022 14:25:55 GMT"}],"update_date":"2022-02-08"}
{"id":"2104.08589","submitter":"Zerui Liang","authors":"Ze-Rui Liang, Xiao-Yi Wu, De-Liang Yao","title":"Hunting for states in the recent LHCb di-$J/\\psi$ invariant mass\n  spectrum","comments":"More discussions added. Matching the version to be published in PRD","journal-ref":"Phys. Rev. D 104, 034034 (2021)","doi":"10.1103/PhysRevD.104.034034","report-no":null,"categories":"hep-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Partial wave analysis is performed, with effective potentials as dynamical\ninputs, to scrutinize the recent LHCb data on the di-$J/\\psi$ invariant mass\nspectrum. Coupled-channel effects are incorporated in the production amplitude\nvia final state interactions. The LHCb data can be well described. A dynamical\ngenerated pole structure, which can be identified as the $X(6900)$ state, is\nfound. Our analysis also provides hints for the existence of three other\npossible states: a bound state $X(6200)$, a broad resonant state $X(6680)$ and\na narrow resonant state $X(7200)$. The $J^{PC}$ quantum numbers of the\n$X(6680)$ and $X(6900)$ states should be $2^{++}$, while the $X(6200)$ and\n$X(7200)$ states prefer $0^{++}$. To determine the above states more precisely,\nmore experimental data for the channels, such as $J/\\psi\\psi(2S)$,\n$J/\\psi\\psi(3770)$, di-$\\psi(2S)$, are required.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 16:13:23 GMT"},{"version":"v2","created":"Tue, 31 Aug 2021 12:58:36 GMT"}],"update_date":"2021-09-08"}
{"id":"2104.08590","submitter":"Hongliang Li","authors":"Hongliang Li, Pingbing Ming and Huiyu Wang","title":"H$^2-$ Korn's Inequality and the Nonconforming Elements for The Strain\n  Gradient Elastic Model","comments":"arXiv admin note: text overlap with arXiv:1809.00819","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We establish a new H2 Korn's inequality and its discrete analog, which\ngreatly simplify the construction of nonconforming elements for a linear strain\ngradient elastic model. The Specht triangle [41] and the NZT tetrahedron [45]\nare analyzed as two typical representatives for robust nonconforming elements\nin the sense that the rate of convergence is independent of the small material\nparameter. We construct new regularized interpolation estimate and the\nenriching operator for both elements, and prove the error estimates under\nminimal smoothness assumption on the solution. Numerical results are consistent\nwith the theoretical prediction.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 16:19:23 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08591","submitter":"Michael Verostek Jr","authors":"Mike Verostek, Casey Miller, and Benjamin Zwickl","title":"Analyzing admissions metrics as predictors of graduate GPA and whether\n  graduate GPA mediates PhD completion","comments":"19 pages, 5 figures, 5 tables","journal-ref":"Phys. Rev. Phys. Educ. Res. 17, 020115 (2021)","doi":"10.1103/PhysRevPhysEducRes.17.020115","report-no":null,"categories":"physics.ed-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  An analysis of 1,955 physics graduate students from 19 PhD programs shows\nthat undergraduate grade point average predicts graduate grades and PhD\ncompletion more effectively than GRE scores. Students' undergraduate GPA (UGPA)\nand GRE Physics (GRE-P) scores are small but statistically significant\npredictors of graduate course grades, while GRE quantitative and GRE verbal\nscores are not. We also find that males and females score equally well in their\ngraduate coursework despite a statistically significant 18 percentile point gap\nin median GRE-P scores between genders. A counterfactual mediation analysis\ndemonstrates that among admission metrics tested only UGPA is a significant\npredictor of overall PhD completion, and that UGPA predicts PhD completion\nindirectly through graduate grades. Thus UGPA measures traits linked to\ngraduate course grades, which in turn predict graduate completion. Although\nGRE-P scores are not significantly associated with PhD completion, our results\nsuggest that any predictive effect they may have are also linked indirectly\nthrough graduate GPA. Overall our results indicate that among commonly used\nquantitative admissions metrics, UGPA offers the most insight into two\nimportant measures of graduate school success, while posing fewer concerns for\nequitable admissions practices.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 16:30:52 GMT"},{"version":"v2","created":"Tue, 18 May 2021 13:11:43 GMT"},{"version":"v3","created":"Thu, 1 Jul 2021 18:29:21 GMT"}],"update_date":"2021-09-15"}
{"id":"2104.08592","submitter":"Paulo Nuno Vicente","authors":"Loup M. Langton, Mercedes L. de Uriarte, Kim Grinfeder, Paulo Nuno\n  Vicente","title":"New Technology, New Rules for Journalism and a New World of Engagement","comments":"Proceedings WJEC 2019, Paris","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.MM","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The ways in which people learn, communicate and engage in discussion have\nchanged profoundly during the past decade. As Jenkins related in her book, The\nConvergence Crisis: An Impending Paradigm Shift in Advertising, Millenials do\nnot want to be told the whole story. Rather, they want someone to begin a\nconversation that will engage others to become participants in the development\nof that story (2015). Technology now allows that to happen, sometimes with\nunintended and/or ill consequences, but technology also generates a dynamic\npotential to create international and interactive discourse aimed at addressing\nshared global challenges.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 16:33:43 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08593","submitter":"Yuval Yarom","authors":"Ileana Buhan and Lejla Batina and Yuval Yarom and Patrick Schaumont","title":"SoK: Design Tools for Side-Channel-Aware Implementations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Side-channel attacks that leak sensitive information through a computing\ndevice's interaction with its physical environment have proven to be a severe\nthreat to devices' security, particularly when adversaries have unfettered\nphysical access to the device. Traditional approaches for leakage detection\nmeasure the physical properties of the device. Hence, they cannot be used\nduring the design process and fail to provide root cause analysis. An\nalternative approach that is gaining traction is to automate leakage detection\nby modeling the device. The demand to understand the scope, benefits, and\nlimitations of the proposed tools intensifies with the increase in the number\nof proposals.\n  In this SoK, we classify approaches to automated leakage detection based on\nthe model's source of truth. We classify the existing tools on two main\nparameters: whether the model includes measurements from a concrete device and\nthe abstraction level of the device specification used for constructing the\nmodel. We survey the proposed tools to determine the current knowledge level\nacross the domain and identify open problems. In particular, we highlight the\nabsence of evaluation methodologies and metrics that would compare proposals'\neffectiveness from across the domain. We believe that our results help\npractitioners who want to use automated leakage detection and researchers\ninterested in advancing the knowledge and improving automated leakage\ndetection.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 16:34:22 GMT"},{"version":"v2","created":"Mon, 14 Jun 2021 04:04:28 GMT"}],"update_date":"2021-06-15"}
{"id":"2104.08594","submitter":"Dominik Peters","authors":"Dominik Peters","title":"Proportionality and Strategyproofness in Multiwinner Elections","comments":"9 pages, AAMAS-18 paper with an error corrected (see note on first\n  page)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.GT cs.MA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Multiwinner voting rules can be used to select a fixed-size committee from a\nlarger set of candidates. We consider approval-based committee rules, which\nallow voters to approve or disapprove candidates. In this setting, several\nvoting rules such as Proportional Approval Voting (PAV) and Phragm\\'en's rules\nhave been shown to produce committees that are proportional, in the sense that\nthey proportionally represent voters' preferences; all of these rules are\nstrategically manipulable by voters. On the other hand, a generalisation of\nApproval Voting gives a non-proportional but strategyproof voting rule. We show\nthat there is a fundamental tradeoff between these two properties: we prove\nthat no multiwinner voting rule can simultaneously satisfy a weak form of\nproportionality (a weakening of justified representation) and a weak form of\nstrategyproofness. Our impossibility is obtained using a formulation of the\nproblem in propositional logic and applying SAT solvers; a human-readable\nversion of the computer-generated proof is obtained by extracting a minimal\nunsatisfiable set (MUS). We also discuss several related axiomatic questions in\nthe domain of committee elections.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 16:40:45 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08595","submitter":"Haim Bar","authors":"Haim Bar and James Booth and Martin T. Wells","title":"Mixed Effect Modeling and Variable Selection for Quantile Regression","comments":"arXiv admin note: text overlap with arXiv:1910.11479","journal-ref":null,"doi":"10.1177/1471082X211033490","report-no":null,"categories":"stat.ME","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  It is known that the estimating equations for quantile regression (QR) can be\nsolved using an EM algorithm in which the M-step is computed via weighted least\nsquares, with weights computed at the E-step as the expectation of independent\ngeneralized inverse-Gaussian variables. This fact is exploited here to extend\nQR to allow for random effects in the linear predictor. Convergence of the\nalgorithm in this setting is established by showing that it is a generalized\nalternating minimization (GAM) procedure. Another modification of the EM\nalgorithm also allows us to adapt a recently proposed method for variable\nselection in mean regression models to the QR setting. Simulations show the\nresulting method significantly outperforms variable selection in QR models\nusing the lasso penalty. Applications to real data include a frailty QR\nanalysis of hospital stays, and variable selection for age at onset of lung\ncancer and for riboflavin production rate using high-dimensional gene\nexpression arrays for prediction.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 16:45:50 GMT"}],"update_date":"2021-08-26"}
{"id":"2104.08596","submitter":"Francesco Mainardi","authors":"Alexander Apelblat, Armando Consiglio, Francesco Mainardi","title":"The Bateman Functions Revisited After 90 Years -- A Survey of Old and\n  New Results","comments":"32 pages, 12 figures","journal-ref":"Mathematics, Vol. 8, 1273 (2021)","doi":"10.3390/math9111273","report-no":null,"categories":"math.GM","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The Bateman functions and the allied Havelock functions were introduced as\nsolutions of some problems in hydrodynamics about ninety years ago, but after a\nperiod of one or two decades they were practically neglected. In handbooks, the\nBateman function is only mentioned as a particular case of the confluent\nhypergeometric function. In order to revive our knowledge on these functions\ntheir basic properties (recurrence functional and differential relations,\nseries, integrals and the Laplace transforms) are presented. Some new results\nare also included. Special attention is directed to the Bateman and Havelock\nfunctions with integer orders, to known in the literature generalizations of\nthese functions and to the Bateman-integral function.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 16:46:05 GMT"},{"version":"v2","created":"Thu, 13 May 2021 14:52:48 GMT"},{"version":"v3","created":"Wed, 2 Jun 2021 14:18:18 GMT"}],"update_date":"2021-06-03"}
{"id":"2104.08597","submitter":"Ahmed El-Kishky","authors":"Ahmed El-Kishky, Adithya Renduchintala, James Cross, Francisco\n  Guzm\\'an, Philipp Koehn","title":"XLEnt: Mining a Large Cross-lingual Entity Dataset with\n  Lexical-Semantic-Phonetic Word Alignment","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Cross-lingual named-entity lexica are an important resource to multilingual\nNLP tasks such as machine translation and cross-lingual wikification. While\nknowledge bases contain a large number of entities in high-resource languages\nsuch as English and French, corresponding entities for lower-resource languages\nare often missing. To address this, we propose Lexical-Semantic-Phonetic Align\n(LSP-Align), a technique to automatically mine cross-lingual entity lexica from\nmined web data. We demonstrate LSP-Align outperforms baselines at extracting\ncross-lingual entity pairs and mine 164 million entity pairs from 120 different\nlanguages aligned with English. We release these cross-lingual entity pairs\nalong with the massively multilingual tagged named entity corpus as a resource\nto the NLP community.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 16:58:05 GMT"},{"version":"v2","created":"Fri, 10 Sep 2021 16:21:22 GMT"}],"update_date":"2021-09-13"}
{"id":"2104.08598","submitter":"Yusra Naqvi","authors":"Yusra Naqvi, Siddhartha Sahi, Emily Sergel","title":"Interpolation polynomials, bar monomials, and their positivity","comments":"25 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO math.RT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We prove a positivity result for interpolation polynomials that was\nconjectured by Knop and Sahi. These polynomials were first introduced by Sahi\nin the context of the Capelli eigenvalue problem for Jordan algebras, and were\nlater shown to be related to Jack polynomials by Knop-Sahi and\nOkounkov-Olshanski. The positivity result proved here is an inhomogeneous\ngeneralization of Macdonald's positivity conjecture for Jack polynomials. We\nalso formulate and prove the non-symmetric version of the Knop-Sahi conjecture,\nand in fact we deduce everything from an even stronger positivity result. This\nlast result concerns certain inhomogeneous analogues of ordinary monomials that\nwe call bar monomials. Their positivity involves in an essential way a new\npartial order on compositions that we call the bar order, and a new operation\nthat we call a glissade.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 17:03:06 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08599","submitter":"Mohcine EL Baroudi","authors":"Mohcine El Baroudi","title":"A Stylistic Analysis of Honest Deception: The Case of Seinfeld TV Series\n  Sitcom","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Language is a powerful tool if used in the correct manner. It is the major\nmode of communication, and using the correct choice of words and styles can\nserve to have a long-lasting impact. Stylistics is the study of the use of\nvarious language styles in communication to pass a message with a bigger impact\nor to communicate indirectly. Stylistic analysis, therefore, is the study of\nthe use of linguistic styles in texts to determine how a style has been used,\nwhat is communicated and how it is communicated. Honest deception is the use of\na choice of words to imply something different from the literal meaning. A\nperson listening or reading a text where honest deception has been used and\nwith a literal understanding may completely miss out on the point. This is\nbecause the issue of honesty and falsehood arises. However, it would be better\nto understand that honest deception is used with the intention of having a\nlasting impact rather than to deceive the readers, viewers or listeners. The\nmajor styles used in honest deception are hyperboles, litotes, irony and\nsarcasm. The Seinfeld Sitcom TV series was a situational TV comedy show aired\nfrom 1990 to 1998. the show attempts to bring to the understanding the daily\nlife of a comedian and how comedian views life experiences and convert them\ninto hilarious jokes. It also shows Jerry's struggle with getting the right\npartner from the many women who come into his life. Reflecting on honest\ndeception in the Seinfeld sitcom TV series, this paper is going to investigate\nhow honest deception has been used in the series, why it has been used and what\nis being communicated. The study is going to use a recapitulative form to give\na better analysis and grouping of the different styles used in honest deception\nthroughout the series.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 17:17:03 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08600","submitter":"Judith Dineley Dr","authors":"Judith Dineley, Grace Lavelle, Daniel Leightley, Faith Matcham, Sara\n  Siddi, Maria Teresa Pe\\~narrubia-Mar\\'ia, Katie M. White, Alina Ivan, Carolin\n  Oetzmann, Sara Simblett, Erin Dawe-Lane, Stuart Bruce, Daniel Stahl, Yatharth\n  Ranjan, Zulqarnain Rashid, Pauline Conde, Amos A. Folarin, Josep Maria Haro,\n  Til Wykes, Richard J.B. Dobson, Vaibhav A. Narayan, Matthew Hotopf, Bj\\\"orn\n  W. Schuller, Nicholas Cummins, The RADAR-CNS Consortium","title":"Remote smartphone-based speech collection: acceptance and barriers in\n  individuals with major depressive disorder","comments":"Accepted to Interspeech 2021. Formatting changes + minor language\n  edits","journal-ref":"Proc. Interspeech 2021, pp. 631-635","doi":"10.21437/Interspeech.2021-1240","report-no":null,"categories":"cs.HC","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The ease of in-the-wild speech recording using smartphones has sparked\nconsiderable interest in the combined application of speech, remote measurement\ntechnology (RMT) and advanced analytics as a research and healthcare tool. For\nthis to be realised, the acceptability of remote speech collection to the user\nmust be established, in addition to feasibility from an analytical perspective.\nTo understand the acceptance, facilitators, and barriers of smartphone-based\nspeech recording, we invited 384 individuals with major depressive disorder\n(MDD) from the Remote Assessment of Disease and Relapse - Central Nervous\nSystem (RADAR-CNS) research programme in Spain and the UK to complete a survey\non their experiences recording their speech. In this analysis, we demonstrate\nthat study participants were more comfortable completing a scripted speech task\nthan a free speech task. For both speech tasks, we found depression severity\nand country to be significant predictors of comfort. Not seeing smartphone\nnotifications of the scheduled speech tasks, low mood and forgetfulness were\nthe most commonly reported obstacles to providing speech recordings.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 17:41:19 GMT"},{"version":"v2","created":"Mon, 30 Aug 2021 13:56:12 GMT"}],"update_date":"2021-08-31"}
{"id":"2104.08601","submitter":"Lu Ji","authors":"Lu Ji, Jing Li, Zhongyu Wei, Qi Zhang, Xuanjing Huang","title":"Who Responded to Whom: The Joint Effects of Latent Topics and Discourse\n  in Conversation Structure","comments":"10 pages, 7 figures, 3 tables submitted for emnlp2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Numerous online conversations are produced on a daily basis, resulting in a\npressing need to conversation understanding. As a basis to structure a\ndiscussion, we identify the responding relations in the conversation discourse,\nwhich link response utterances to their initiations. To figure out who\nresponded to whom, here we explore how the consistency of topic contents and\ndependency of discourse roles indicate such interactions, whereas most prior\nwork ignore the effects of latent factors underlying word occurrences. We\npropose a model to learn latent topics and discourse in word distributions, and\npredict pairwise initiation-response links via exploiting topic consistency and\ndiscourse dependency. Experimental results on both English and Chinese\nconversations show that our model significantly outperforms the previous state\nof the arts, such as 79 vs. 73 MRR on Chinese customer service dialogues. We\nfurther probe into our outputs and shed light on how topics and discourse\nindicate conversational user interactions.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 17:46:00 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08602","submitter":"Jan \\v{S}aroch","authors":"Manuel Cort\\'es-Izurdiaga and Jan \\v{S}aroch","title":"Module classes induced by complexes and $\\lambda$-pure-injective modules","comments":"24 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.RT math.CT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We prove that, if $\\mathcal{GP}$ is the class of all Gorenstein projective\nmodules over a ring $R$, then $\\mathfrak{GP}=(\\mathcal{GP},\\mathcal{GP}^\\perp)$\nis a cotorsion pair. Moreover, $\\mathfrak{GP}$ is complete when all projective\nmodules are $\\lambda$-pure-injective for some infinite regular cardinal\n$\\lambda$ (in particular, if $R$ is right $\\Sigma$-pure-injective).\n  We obtain these results, on the one hand, studying the class of totally\nacyclic complexes over $R$. We prove that, when $R$ is $\\Sigma$-pure-injective,\nthis class is deconstructible and forms a coreflective subcategory of the\nhomotopy category of the projective modules. On the other hand, we use results\nabout $\\lambda$-pure-injective modules for infinite regular cardinals\n$\\lambda$.\n  Finally, under different set-theoretical hypotheses, we show that for an\narbitrary ring $R$, the following hold: (1) There exists an infinite regular\ncardinal number $\\lambda$ such that every projective module is\n$\\lambda$-pure-injective (and $\\mathfrak{GP}$ is complete). (2) $R$ is right\npure-semisimple if and only if there exists a regular uncountable $\\lambda$\nsuch that $\\mathrm{Mod}$-$R$ has enough $\\lambda$-pure-injective objects.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 17:53:54 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08603","submitter":"P. A. Gonzalez","authors":"Almendra Arag\\'on, P. A. Gonz\\'alez, Joel Saavedra, Yerko V\\'asquez","title":"Scalar quasinormal modes for 2+1-dimensional Coulomb like AdS black\n  holes from non lineal electrodynamics","comments":"Corrected version, and references added","journal-ref":null,"doi":"10.1007/s10714-021-02864-6","report-no":null,"categories":"gr-qc hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study the propagation of scalar fields in the background of\n$2+1$-dimensional Coulomb like AdS black holes, and we show that such\npropagation is stable under Dirichlet boundary conditions. Then, we solve the\nKlein-Gordon equation by using the pseudospectral Chevyshev method, and we find\nthe quasinormal frequencies. Mainly, we find that the quasinormal frequencies\nare purely imaginary for a null angular number and they are complex and purely\nimaginary for a non null value of the angular number, which depend on the black\nhole charge, angular number and overtone number. On the other hand, the effect\nof the inclusion of a Coulomb like field from non lineal electrodynamics to\nGeneral Relativity for a vanishing angular number is the emergence of two\nbranches of quasinormal frequencies in contrast with the static BTZ black hole.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 17:59:15 GMT"},{"version":"v2","created":"Thu, 29 Apr 2021 14:10:26 GMT"}],"update_date":"2021-10-27"}
{"id":"2104.08604","submitter":"Jingfeng Wu","authors":"Haoran Li, Aditya Krishnan, Jingfeng Wu, Soheil Kolouri, Praveen K.\n  Pilly, Vladimir Braverman","title":"Lifelong Learning with Sketched Structural Regularization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Preventing catastrophic forgetting while continually learning new tasks is an\nessential problem in lifelong learning. Structural regularization (SR) refers\nto a family of algorithms that mitigate catastrophic forgetting by penalizing\nthe network for changing its \"critical parameters\" from previous tasks while\nlearning a new one. The penalty is often induced via a quadratic regularizer\ndefined by an \\emph{importance matrix}, e.g., the (empirical) Fisher\ninformation matrix in the Elastic Weight Consolidation framework. In practice\nand due to computational constraints, most SR methods crudely approximate the\nimportance matrix by its diagonal. In this paper, we propose \\emph{Sketched\nStructural Regularization} (Sketched SR) as an alternative approach to compress\nthe importance matrices used for regularizing in SR methods. Specifically, we\napply \\emph{linear sketching methods} to better approximate the importance\nmatrices in SR algorithms. We show that sketched SR: (i) is computationally\nefficient and straightforward to implement, (ii) provides an approximation\nerror that is justified in theory, and (iii) is method oblivious by\nconstruction and can be adapted to any method that belongs to the structural\nregularization class. We show that our proposed approach consistently improves\nvarious SR algorithms' performance on both synthetic experiments and benchmark\ncontinual learning tasks, including permuted-MNIST and CIFAR-100.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:07:23 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08605","submitter":"Sangita Das","authors":"Sangita Das and Suchandan Kayal","title":"Ordering results between the largest claims arising from two general\n  heterogeneous portfolios","comments":"22 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"q-fin.RM math.PR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This work is entirely devoted to compare the largest claims from two\nheterogeneous portfolios. It is assumed that the claim amounts in an insurance\nportfolio are nonnegative absolutely continuous random variables and belong to\na general family of distributions. The largest claims have been compared based\non various stochastic orderings. The established sufficient conditions are\nassociated with the matrices and vectors of model parameters. Applications of\nthe results are provided for the purpose of illustration.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:12:08 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08606","submitter":"Alexander Patkowski","authors":"Alexander E. Patkowski","title":"A remark on Fine's arithmetic functions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this note, we take another look at some arithmetic identities of N.J. Fine\nassociated with divisor functions. We connect these functions with indefinite\nquadratic forms using a result due to Andrews. As a consequence, arithmetic\ntheorems are extracted.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:14:54 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08607","submitter":"Laura Lauerbach","authors":"Laura Lauerbach and Anja Schl\\\"omerkemper","title":"Derivation of a variational model for brittle fracture from a random\n  heterogeneous particle chain","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  A mathematical continuum limit of the interaction energy of a random particle\nchain is shown to yield new insight into the effect of microscopic\nheterogeneities on macroscopic fracture laws in brittle materials. We derive a\nformula which yields that either elastic behaviour or a crack is energetically\npreferred. The formula explicitly shows the dependence on the boundary\ncondition and the microstructure of the chain. The mathematical analysis is\nbased on a variational convergence $\\Gamma$-convergence of convex-concave\npotentials together with ergodic theorems which are common tools in stochastic\nhomogenization.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:15:02 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08608","submitter":"Frank Ortmann","authors":"Michel Panhans, Frank Ortmann","title":"Efficient Time-Domain Approach for Linear Response Functions","comments":null,"journal-ref":"Phys. Rev. Lett. 127, 016601 (2021)","doi":"10.1103/PhysRevLett.127.016601","report-no":null,"categories":"cond-mat.mes-hall","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  We derive the general Kubo formula in a form that solely utilizes the time\nevolution of displacement operators. The derivation is based on the\ndecomposition of the linear response function into its time symmetric and time\nanti-symmetric part. We relate this form to the well-known\nfluctuation-dissipation formula and discuss theoretical and numerical aspects\nof it. The approach is illustrated with an analytical example for magnetic\nresonance as well as a numerical example where we analyze the electrical\nconductivity tensor and the Chern insulating state of the disordered Haldane\nmodel. We introduce a highly efficient time-domain approach that describes the\nquantum dynamics of the resistivity of this model with an at least 1000-fold\nbetter performance in comparison to existing time-evolution schemes.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:15:48 GMT"}],"update_date":"2021-07-07"}
{"id":"2104.08609","submitter":"Caio Henrique Silva De Souza","authors":"Josnei Antonio Novacoski and Caio Henrique Silva de Souza","title":"On truncations of valuations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper we study the truncation $\\nu_q$ of a valuation $\\nu$ on a\npolynomial $q$. It is known that when $q$ is a key polynomial, then $\\nu_q$ is\na valuation. It is also known that the converse does not hold. We show that\nwhen $q$ is a key polynomial, then $\\nu_q$ is the restriction of the truncation\ngiven by an optimizing root of $q$. We also discuss which conditions assure\nthat $\\nu_q=\\nu$. Finally, we assume that $\\nu_q$ is a valuation and present\nsome conditions, given in terms of the graded algebra, to assure that $q$ is a\nkey polynomial.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:23:04 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08610","submitter":"Michael Glass","authors":"Michael Glass, Gaetano Rossiello, Alfio Gliozzo","title":"Zero-shot Slot Filling with DPR and RAG","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The ability to automatically extract Knowledge Graphs (KG) from a given\ncollection of documents is a long-standing problem in Artificial Intelligence.\nOne way to assess this capability is through the task of slot filling. Given an\nentity query in form of [Entity, Slot, ?], a system is asked to `fill' the slot\nby generating or extracting the missing value from a relevant passage or\npassages. This capability is crucial to create systems for automatic knowledge\nbase population, which is becoming in ever-increasing demand, especially in\nenterprise applications. Recently, there has been a promising direction in\nevaluating language models in the same way we would evaluate knowledge bases,\nand the task of slot filling is the most suitable to this intent. The recent\nadvancements in the field try to solve this task in an end-to-end fashion using\nretrieval-based language models. Models like Retrieval Augmented Generation\n(RAG) show surprisingly good performance without involving complex information\nextraction pipelines. However, the results achieved by these models on the two\nslot filling tasks in the KILT benchmark are still not at the level required by\nreal-world information extraction systems. In this paper, we describe several\nstrategies we adopted to improve the retriever and the generator of RAG in\norder to make it a better slot filler. Our KGI0 system (available at\nhttps://github.com/IBM/retrieve-write-slot-filling) reached the top-1 position\non the KILT leaderboard on both T-REx and zsRE dataset with a large margin.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:24:51 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08611","submitter":"Sangita Das","authors":"Sangita Das and Suchandan Kayal","title":"Some new ordering results on stochastic comparisons of second largest\n  order statistics from independent and interdependent heterogeneous\n  distributions","comments":"23 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"math.ST stat.ME stat.TH","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The second-largest order statistic is of special importance in reliability\ntheory since it represents the time to failure of a $2$-out-of-$n$ system.\nConsider two $2$-out-of-$n$ systems with heterogeneous random lifetimes. The\nlifetimes are assumed to follow heterogeneous general exponentiated\nlocation-scale models. In this communication, the usual stochastic and reversed\nhazard rate orders between the systems' lifetimes are established under two\ncases. For the case of independent random lifetimes, the usual stochastic order\nand the reversed hazard rate order between the second-largest order statistics\nare obtained by using the concept of vector majorization and related orders.\nFor the dependent case, the conditions under which the usual stochastic order\nbetween the second-largest order statistics holds are investigated. To\nillustrate the theoretical findings, some special cases of the exponentiated\nlocation-scale model are considered.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:26:10 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08612","submitter":"Muhammad Zafrullah","authors":"Muhammad Zafrullah","title":"On super $v$-domains","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  An integral domain $D,$ with quotient field $K,$ is a $v$-domain if for each\nnonzero finitely generated ideal $A$ of $D$ we have $(AA^{-1})^{-1}=D.$ It is\nwell known that if $D$ is a $v$-domain$,$ then some quotient ring $D_{S}$ of\n$D$ may not be a $v$-domain. Calling $D$ a super $v$-domain if every quotient\nring of $D$ is a $v$-domain we characterize super $v$-domains as locally\n$v$-domains. Using techniques from factorization theory we show that $D$ is a\nsuper $v$-domain if and only if $D[X]$ is a super $v$-domain if and only if\n$D+XK[X]$ is a super $v$-domain and give new examples of super $v$ -domains\nthat are strictly between $v$-domains and P-domains that were studied in\n[Manuscripta Math. 35(1981)1-26]\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:28:08 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08613","submitter":"Omar Sharif","authors":"Avishek Das, Omar Sharif, Mohammed Moshiul Hoque, Iqbal H. Sarker","title":"Emotion Classification in a Resource Constrained Language Using\n  Transformer-based Approach","comments":"Accepted in NAACL-SRW 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Although research on emotion classification has significantly progressed in\nhigh-resource languages, it is still infancy for resource-constrained languages\nlike Bengali. However, unavailability of necessary language processing tools\nand deficiency of benchmark corpora makes the emotion classification task in\nBengali more challenging and complicated. This work proposes a\ntransformer-based technique to classify the Bengali text into one of the six\nbasic emotions: anger, fear, disgust, sadness, joy, and surprise. A Bengali\nemotion corpus consists of 6243 texts is developed for the classification task.\nExperimentation carried out using various machine learning (LR, RF, MNB, SVM),\ndeep neural networks (CNN, BiLSTM, CNN+BiLSTM) and transformer (Bangla-BERT,\nm-BERT, XLM-R) based approaches. Experimental outcomes indicate that XLM-R\noutdoes all other techniques by achieving the highest weighted $f_1$-score of\n$69.73\\%$ on the test data. The dataset is publicly available at\nhttps://github.com/omar-sharif03/NAACL-SRW-2021.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:28:39 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08614","submitter":"Michael Bronstein","authors":"Jacob Andreas, Ga\\v{s}per Begu\\v{s}, Michael M. Bronstein, Roee\n  Diamant, Denley Delaney, Shane Gero, Shafi Goldwasser, David F. Gruber, Sarah\n  de Haas, Peter Malkin, Roger Payne, Giovanni Petri, Daniela Rus, Pratyusha\n  Sharma, Dan Tchernov, Pernille T{\\o}nnesen, Antonio Torralba, Daniel Vogt,\n  Robert J. Wood","title":"Cetacean Translation Initiative: a roadmap to deciphering the\n  communication of sperm whales","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.AI cs.CL cs.LG cs.RO eess.AS","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The past decade has witnessed a groundbreaking rise of machine learning for\nhuman language analysis, with current methods capable of automatically\naccurately recovering various aspects of syntax and semantics - including\nsentence structure and grounded word meaning - from large data collections.\nRecent research showed the promise of such tools for analyzing acoustic\ncommunication in nonhuman species. We posit that machine learning will be the\ncornerstone of future collection, processing, and analysis of multimodal\nstreams of data in animal communication studies, including bioacoustic,\nbehavioral, biological, and environmental data. Cetaceans are unique non-human\nmodel species as they possess sophisticated acoustic communications, but\nutilize a very different encoding system that evolved in an aquatic rather than\nterrestrial medium. Sperm whales, in particular, with their highly-developed\nneuroanatomical features, cognitive abilities, social structures, and discrete\nclick-based encoding make for an excellent starting point for advanced machine\nlearning tools that can be applied to other animals in the future. This paper\ndetails a roadmap toward this goal based on currently existing technology and\nmultidisciplinary scientific community effort. We outline the key elements\nrequired for the collection and processing of massive bioacoustic data of sperm\nwhales, detecting their basic communication units and language-like\nhigher-level structures, and validating these models through interactive\nplayback experiments. The technological capabilities developed by such an\nundertaking are likely to yield cross-applications and advancements in broader\ncommunities investigating non-human communication and animal behavioral\nresearch.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:39:22 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08615","submitter":"Kun Wang","authors":"Kun Wang, Canzhe Zhao, Shuai Li, Shuo Shao","title":"Conservative Contextual Combinatorial Cascading Bandit","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG stat.ML","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Conservative mechanism is a desirable property in decision-making problems\nwhich balance the tradeoff between the exploration and exploitation. We propose\nthe novel \\emph{conservative contextual combinatorial cascading bandit\n($C^4$-bandit)}, a cascading online learning game which incorporates the\nconservative mechanism. At each time step, the learning agent is given some\ncontexts and has to recommend a list of items but not worse than the base\nstrategy and then observes the reward by some stopping rules. We design the\n$C^4$-UCB algorithm to solve the problem and prove its n-step upper regret\nbound for two situations: known baseline reward and unknown baseline reward.\nThe regret in both situations can be decomposed into two terms: (a) the upper\nbound for the general contextual combinatorial cascading bandit; and (b) a\nconstant term for the regret from the conservative mechanism. We also improve\nthe bound of the conservative contextual combinatorial bandit as a by-product.\nExperiments on synthetic data demonstrate its advantages and validate our\ntheoretical analysis.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:42:28 GMT"},{"version":"v2","created":"Fri, 23 Apr 2021 05:59:29 GMT"}],"update_date":"2021-04-26"}
{"id":"2104.08616","submitter":"Nils Morawietz","authors":"Nils Morawietz and Petra Wolf","title":"A Timecop's Chase Around the Table","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider the cops and robber game variant consisting of one cop and one\nrobber on time-varying graphs (TVG). The considered TVGs are edge periodic\ngraphs, i.e., for each edge, a binary string $s_e$ determines in which time\nstep the edge is present, namely the edge $e$ is present in time step $t$ if\nand only if the string $s_e$ contains a $1$ at position $t \\mod |s_e|$. This\nperiodicity allows for a compact representation of the infinite TVG. We proof\nthat even for very simple underlying graphs, i.e., directed and undirected\ncycles the problem whether a cop-winning strategy exists is NP-hard and\nW[1]-hard parameterized by the number of vertices. Our second main result are\nmatching lower bounds for the ratio between the length of the underlying cycle\nand the least common multiple (LCM) of the lengths of binary strings describing\nedge-periodicies over which the graph is robber-winning. Our third main result\nimproves the previously known EXPTIME upper bound for Periodic Cop and Robber\non general edge periodic graphs to PSPACE-membership.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:42:55 GMT"},{"version":"v2","created":"Sun, 11 Jul 2021 22:09:42 GMT"}],"update_date":"2021-07-13"}
{"id":"2104.08617","submitter":"Luis M. Nieto","authors":"M. Gadella, J. Hern\\'andez-Mu\\~noz, L.M. Nieto, and C. San Mill\\'an","title":"Supersymmetric Partners of the One-Dimensional Infinite Square Well\n  Hamiltonian","comments":"17 pages, 1 table and 5 figures","journal-ref":"Symmetry 2021, 13, 350","doi":"10.3390/sym13020350","report-no":null,"categories":"math-ph math.MP quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We find supersymmetric partners of a family of self-adjoint operators which\nare self-adjoint extensions of the differential operator $-d^2/dx^2$ on\n$L^2[-a,a]$, $a>0$, that is, the one dimensional infinite square well. First of\nall, we classify these self-adjoint extensions in terms of several choices of\nthe parameters determining each of the extensions. There are essentially two\nbig groups of extensions. In one, the ground state has strictly positive\nenergy. On the other, either the ground state has zero or negative energy. In\nthe present paper, we show that each of the extensions belonging to the first\ngroup (energy of ground state strictly positive) has an infinite sequence of\nsupersymmetric partners, such that the $\\ell$-th order partner differs in one\nenergy level from both the $(\\ell-1)$-th and the $(\\ell+1)$-th order partners.\nIn general, the eigenvalues for each of the self-adjoint extensions of\n$-d^2/dx^2$ come from a transcendental equation and are all infinite. For the\ncase under our study, we determine the eigenvalues, which are also infinite,\n{all the extensions have a purely discrete spectrum,} and their respective\neigenfunctions for all of its $\\ell$-th supersymmetric partners of each\nextension.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:50:37 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08618","submitter":"Kiavash Satvat","authors":"Kiavash Satvat, Rigel Gjomemo and V.N. Venkatakrishnan","title":"EXTRACTOR: Extracting Attack Behavior from Threat Reports","comments":"6th IEEE European Symposium on Security and Privacy","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.AI","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The knowledge on attacks contained in Cyber Threat Intelligence (CTI) reports\nis very important to effectively identify and quickly respond to cyber threats.\nHowever, this knowledge is often embedded in large amounts of text, and\ntherefore difficult to use effectively. To address this challenge, we propose a\nnovel approach and tool called EXTRACTOR that allows precise automatic\nextraction of concise attack behaviors from CTI reports. EXTRACTOR makes no\nstrong assumptions about the text and is capable of extracting attack behaviors\nas provenance graphs from unstructured text. We evaluate EXTRACTOR using\nreal-world incident reports from various sources as well as reports of DARPA\nadversarial engagements that involve several attack campaigns on various OS\nplatforms of Windows, Linux, and FreeBSD. Our evaluation results show that\nEXTRACTOR can extract concise provenance graphs from CTI reports and show that\nthese graphs can successfully be used by cyber-analytics tools in\nthreat-hunting.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:51:00 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08619","submitter":"Guillermo Navas Palencia","authors":"Guillermo Navas-Palencia","title":"Optimal Counterfactual Explanations for Scorecard modelling","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Counterfactual explanations is one of the post-hoc methods used to provide\nexplainability to machine learning models that have been attracting attention\nin recent years. Most examples in the literature, address the problem of\ngenerating post-hoc explanations for black-box machine learning models after\nthe rejection of a loan application. In contrast, in this work, we investigate\nmathematical programming formulations for scorecard models, a type of\ninterpretable model predominant within the banking industry for lending. The\nproposed mixed-integer programming formulations combine objective functions to\nensure close, realistic and sparse counterfactuals using multi-objective\noptimization techniques for a binary, probability or continuous outcome.\nMoreover, we extend these formulations to generate multiple optimal\ncounterfactuals simultaneously while guaranteeing diversity. Experiments on two\nreal-world datasets confirm that the presented approach can generate optimal\ndiverse counterfactuals addressing desired properties with assumable CPU times\nfor practice use.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:51:50 GMT"},{"version":"v2","created":"Sun, 9 May 2021 17:20:17 GMT"}],"update_date":"2021-05-11"}
{"id":"2104.08620","submitter":"Joshua Rozner","authors":"Joshua Rozner, Christopher Potts, Kyle Mahowald","title":"Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as\n  a Target for NLP","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Cryptic crosswords, the dominant crossword variety in the UK, are a promising\ntarget for advancing NLP systems that seek to process semantically complex,\nhighly compositional language. Cryptic clues read like fluent natural language\nbut are adversarially composed of two parts: a definition and a wordplay cipher\nrequiring character-level manipulations. Expert humans use creative\nintelligence to solve cryptics, flexibly combining linguistic, world, and\ndomain knowledge. In this paper, we make two main contributions. First, we\npresent a dataset of cryptic clues as a challenging new benchmark for NLP\nsystems that seek to process compositional language in more creative,\nhuman-like ways. After showing that three non-neural approaches and T5, a\nstate-of-the-art neural language model, do not achieve good performance, we\nmake our second main contribution: a novel curriculum approach, in which the\nmodel is first fine-tuned on related tasks such as unscrambling words.We also\nintroduce a challenging data split, examine the meta-linguistic capabilities of\nsubword-tokenized models, and investigate model systematicity by perturbing the\nwordplay part of clues, showing that T5 exhibits behavior partially consistent\nwith human solving strategies. Although our curricular approach considerably\nimproves on the T5 baseline, our best-performing model still fails to\ngeneralize to the extent that humans can. Thus, cryptic crosswords remain an\nunsolved challenge for NLP systems and a potential source of future innovation.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:54:00 GMT"},{"version":"v2","created":"Fri, 11 Jun 2021 23:20:46 GMT"},{"version":"v3","created":"Fri, 5 Nov 2021 16:34:26 GMT"}],"update_date":"2021-11-08"}
{"id":"2104.08621","submitter":"Matthew Heffernan","authors":"Matthew Heffernan","title":"How about that Bayes: Bayesian techniques and the simple pendulum","comments":"11 pages, 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.ed-ph physics.data-an","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Physics increasingly uses Bayesian techniques for systematic data analysis\nand model-to-data comparison. This paper describes how these methods can be\nimplemented to answer questions of relevance to teaching laboratories. It\ndemonstrates the Bayesian approach to statistical modeling and model selection\nin a step-by-step workflow. The simple pendulum provides a demonstration with\nthe precision commonly seen in the introductory laboratory. This is used to\nprovide realistic, quantitative guidance for model preference between the small\nangle approximation and more complicated formula. This extends the simple\npendulum literature's focus beyond comparing individual idealized assessments\nof different approximations and provides actionable, data-driven guidance for\nteaching laboratory design.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:55:01 GMT"},{"version":"v2","created":"Wed, 25 Aug 2021 20:14:50 GMT"},{"version":"v3","created":"Wed, 26 Jan 2022 18:56:13 GMT"},{"version":"v4","created":"Wed, 20 Jul 2022 16:26:49 GMT"}],"update_date":"2022-07-21"}
{"id":"2104.08622","submitter":"Or Katz","authors":"Yahel Horowicz, Or Katz, Oren Raz, Ofer Firstenberg","title":"Critical dynamics and phase transition of a strongly interacting warm\n  spin-gas","comments":"Y.H. and O.K. contributed equally","journal-ref":"PNAS 118 (43), e2106400118 (2021)","doi":"10.1073/pnas.2106400118","report-no":null,"categories":"quant-ph cond-mat.dis-nn cond-mat.stat-mech physics.atom-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Phase transitions are emergent phenomena where microscopic interactions drive\na disordered system into a collectively ordered phase. Near the boundary\nbetween two phases, the system can exhibit critical, scale-invariant behavior.\nHere, we report on a second-order phase transition accompanied by critical\nbehavior in a system of warm cesium spins driven by linearly-polarized light.\nThe ordered phase exhibits macroscopic magnetization when the interactions\nbetween the spins become dominant. We measure the phase diagram of the system\nand observe the collective behavior near the phase boundaries, including\npower-law dependence of the magnetization and divergence of the susceptibility.\nOut of equilibrium, we observe a critical slow-down of the spin response time\nby two orders of magnitude, exceeding five seconds near the phase boundary.\nThis work establishes a controlled platform for investigating equilibrium and\nnonequilibrium properties of magnetic phases.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:02:33 GMT"},{"version":"v2","created":"Sat, 9 Oct 2021 10:03:51 GMT"}],"update_date":"2021-10-27"}
{"id":"2104.08623","submitter":"Junyu Chen","authors":"Junyu Chen, Ye Li, Licia P. Luna, Hyun Woo Chung, Steven P. Rowe, Yong\n  Du, Lilja B. Solnes, Eric C. Frey","title":"Learning Fuzzy Clustering for SPECT/CT Segmentation via Convolutional\n  Neural Networks","comments":"This manuscript has been published by Medical Physics (2021)","journal-ref":null,"doi":"10.1002/mp.14903","report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Quantitative bone single-photon emission computed tomography (QBSPECT) has\nthe potential to provide a better quantitative assessment of bone metastasis\nthan planar bone scintigraphy due to its ability to better quantify activity in\noverlapping structures. An important element of assessing response of bone\nmetastasis is accurate image segmentation. However, limited by the properties\nof QBSPECT images, the segmentation of anatomical regions-of-interests (ROIs)\nstill relies heavily on the manual delineation by experts. This work proposes a\nfast and robust automated segmentation method for partitioning a QBSPECT image\ninto lesion, bone, and background. We present a new unsupervised segmentation\nloss function and its semi- and supervised variants for training a\nconvolutional neural network (ConvNet). The loss functions were developed based\non the objective function of the classical Fuzzy C-means (FCM) algorithm. We\nconducted a comprehensive study to compare our proposed methods with ConvNets\ntrained using supervised loss functions and conventional clustering methods.\nThe Dice similarity coefficient (DSC) and several other metrics were used as\nfigures of merit as applied to the task of delineating lesion and bone in both\nsimulated and clinical SPECT/CT images. We experimentally demonstrated that the\nproposed methods yielded good segmentation results on a clinical dataset even\nthough the training was done using realistic simulated images. A ConvNet-based\nimage segmentation method that uses novel loss functions was developed and\nevaluated. The method can operate in unsupervised, semi-supervised, or\nfully-supervised modes depending on the availability of annotated training\ndata. The results demonstrated that the proposed method provides fast and\nrobust lesion and bone segmentation for QBSPECT/CT. The method can potentially\nbe applied to other medical image segmentation applications.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:03:52 GMT"},{"version":"v2","created":"Thu, 22 Apr 2021 03:36:41 GMT"},{"version":"v3","created":"Fri, 28 May 2021 14:43:16 GMT"}],"update_date":"2021-05-31"}
{"id":"2104.08624","submitter":"Amir Moradifam","authors":"Amir Moradifam, Alexander Rowell","title":"Existence and structure of P-area minimizing surfaces in the Heisenberg\n  group","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.DG math.AP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We study existence and structure of $P-$area minimizing surfaces in the\nHeisenberg group under Dirichlet and Neumann boundary conditions. We show that\nthere exists an underlying vector field $N$ that characterized existence and\nstructure of $P$-area minimizing surfaces. This vector field exists even if\nthere is no $P$-area minimizing surface satisfying the prescribed boundary\nconditions. We prove that if $\\partial \\Omega$ satisfies a so called Barrier\ncondition, it is sufficient to guarantee existence of such surfaces. Our\napproach is completely different from previous methods in the literature and\nmakes major progress in understanding existence of $P$-area minimizing\nsurfaces.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:10:46 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08625","submitter":"Afsoon Afzal","authors":"Afsoon Afzal, Claire Le Goues, Christopher S. Timperley","title":"GzScenic: Automatic Scene Generation for Gazebo Simulator","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Testing robotic and cyberphysical systems in simulation require\nspecifications of the simulated environments (i.e., scenes). The Scenic\ndomain-specific language provides a high-level probabilistic programming\nlanguage that allows users to specify scenarios for simulation. Scenic\nautomatically generates concrete scenes that can be rendered by simulators.\nHowever, Scenic is mainly designed for autonomous vehicle simulation and does\nnot support the most popular general-purpose simulator: Gazebo. In this work,\nwe present GzScenic; a tool that automatically generates scenes for simulation\nin Gazebo. GzScenic automatically generates both the models required for\nrunning Scenic on the scenarios, and the models that Gazebo requires for\nrunning the simulation.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:14:55 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08626","submitter":"Taufik Abrao PhD","authors":"Alex Mussi and Taufik Abr\\~ao","title":"Mixed Gibbs Sampling Detector in High-Order Modulation Large-Scale MIMO\n  Systems","comments":"28 pages, 7 figures, 3 tables and 3 pseudo-codes","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT eess.SP math.IT stat.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  A neighborhood restricted Mixed Gibbs Sampling (MGS) based approach is\nproposed for low-complexity high-order modulation large-scale Multiple-Input\nMultiple-Output (LS-MIMO) detection. The proposed LS-MIMO detector applies a\nneighborhood limitation (NL) on the noisy solution from the MGS at a distance d\n- thus, named d-simplified MGS (d-sMGS) - in order to mitigate its impact,\nwhich can be harmful when a high order modulation is considered. Numerical\nsimulation results considering 64-QAM demonstrated that the proposed detection\nmethod can substantially improve the MGS algorithm convergence, whereas no\nextra computational complexity per iteration is required. The proposed\nd-sMGS-based detector suitable for high-order modulation LS-MIMO further\nexhibits improved performance vs. complexity tradeoff when the system loading\nis high, i.e., when K >= 0.75. N. Also, with increasing the number of\ndimensions, i.e., increasing the number of antennas and/or modulation order, a\nsmaller restriction of 2-sMGS was shown to be a more interesting choice than\n1-sMGS.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:17:11 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08627","submitter":"Ka Wai Ho","authors":"Ka Wai Ho and Alex Lazarian","title":"Intermittency of Fast MHD Modes and Regions of Anomalous Gradient\n  Orientation in Low-beta Plasmas","comments":"14 pages, 13 figures","journal-ref":"ApJ 2021 911 53","doi":"10.3847/1538-4357/abe713","report-no":null,"categories":"astro-ph.GA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The strong alignment of small-scale turbulent Alfv\\'enic motions with the\ndirection of the magnetic field that percolates the small-scale eddies and\nimprints the direction of the magnetic field is a property that follows from\nthe MHD theory and the theory of turbulent reconnection. The Alfv\\'enic eddies\nmix magnetic fields perpendicular to the direction of the local magnetic field,\nand this type of motion is used to trace magnetic fields with the velocity\ngradient technique (VGT). The other type of turbulent motion, fast modes,\ninduces anisotropies orthogonal to Alfv\\'enic eddies and interferes with the\ntracing of the magnetic field with the VGT. We report a new effect, i.e., in a\nmagnetically dominated low-\\beta subsonic medium, fast modes are very\nintermittent, and in a volume, with a small filling factor the fast modes\ndominate other turbulent motions. We identify these localized regions as the\ncause of the occasional change of direction of gradients in our synthetic\nobservations. We show that the new technique of measuring the gradients of\ngradient amplitudes suppresses the contribution from the fast-mode-dominated\nregions, improving the magnetic field tracing. In addition, we show that the\ndistortion of the gradient measurements by fast modes is also applicable to the\nsynchrotron intensity gradients, but the effect is reduced compared to the VGT.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:17:55 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08628","submitter":"Pierre-Etienne Druet","authors":"Dieter Bothe, Wolfgang Dreyer and Pierre-Etienne Druet","title":"Multicomponent incompressible fluids -- An asymptotic study","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math-ph math.MP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper investigates the asymptotic behavior of the Helmholtz free energy\nof mixtures at small compressibility. We start from a general representation\nfor the local free energy that is valid in stable subregions of the phase\ndiagram. On the basis of this representation we classify the admissible data to\nconstruct a thermodynamically consistent constitutive model. We then analyze\nthe incompressible limit, where the molar volume becomes independent of\npressure. Here we are confronted with two problems:\n  (i) Our study shows that the physical system at hand cannot remain\nincompressible for arbitrary large deviations from a reference pressure unless\nits volume is linear in the composition.\n  (ii) As a consequence of the second law of thermodynamics, the incompressible\nlimit implies that the molar volume becomes independent of temperature as well.\nMost applications, however, reveal the non-appropriateness of this property.\n  According to our mathematical treatment, the free energy as a function of\ntemperature and partial masses tends to a limit in the sense of epi- or\nGamma-convergence. In the context of the first problem, we study the mixing of\ntwo fluids to compare the linearity with experimental observations. The second\nproblem will be treated by considering the asymptotic behavior of both a\ngeneral inequality relating thermal expansion and compressibility and a\nPDE-system relying on the equations of balance for partial masses, momentum and\nthe internal energy.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:20:57 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08629","submitter":"Hung D. Nguyen","authors":"David P. Herzog and Hung D. Nguyen","title":"Stability and invariant measure asymptotics in a model for heavy\n  particles in rough turbulent flows","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study a system of Skorokhod stochastic differential equations (SDEs)\nmodeling the pairwise dispersion (in spatial dimension $d=2$) of heavy\nparticles transported by a rough self-similar, turbulent flow with H\\\"{o}lder\nexponent $h\\in (0,1)$. Under the assumption that $h>0$ is sufficiently small,\nwe use Lyapunov methods and control theory to show that the Markovian system is\nnonexplosive and has a unique, exponentially attractive invariant probability\nmeasure. Furthermore, our Lyapunov construction is radially sharp and gives\npartial confirmation on a predicted asymptotic behavior with respect to the\nH\\\"{o}lder exponent $h$ of the invariant probability measure. A physical\ninterpretation of the asymptotics is that intermittent clustering is weakened\nwhen the carrier flow is sufficiently rough.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:23:45 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08630","submitter":"Bricker Ostler","authors":"Bricker Ostler, Nikolai Yampolsky, Quinn Marksteiner","title":"Statistical analysis of electron bunch longitudinal profile\n  reconstructions using the Gerchberg-Saxton algorithm","comments":"The following article has been submitted to Review of Scientific\n  Instruments","journal-ref":null,"doi":"10.1063/5.0054322","report-no":null,"categories":"physics.acc-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Knowledge of longitudinal electron bunch profiles is vital to optimize the\nperformance of plasma wakefield accelerators and x-ray free electron laser\nlinacs. Because of their importance to these novel applications, noninvasive\nfrequency domain techniques are often employed to reconstruct longitudinal\nbunch profiles from coherent synchrotron, transition, or undulator radiation\nmeasurements. In this paper, we detail several common reconstruction techniques\ninvolving the Kramers-Kronig phase relationship and Gerchberg-Saxton algorithm.\nThrough statistical analysis, we draw general conclusions about the accuracy of\nthese reconstruction techniques and the most suitable candidate for\nlongitudinal bunch reconstruction from spectroscopic data.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:28:37 GMT"}],"update_date":"2021-09-20"}
{"id":"2104.08631","submitter":"Marina Y Aoyama","authors":"Marina Y. Aoyama, Matthew Howard","title":"Training Humans to Train Robots Dynamic Motor Skills","comments":"6 pages, 4 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.AI cs.HC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Learning from demonstration (LfD) is commonly considered to be a natural and\nintuitive way to allow novice users to teach motor skills to robots. However,\nit is important to acknowledge that the effectiveness of LfD is heavily\ndependent on the quality of teaching, something that may not be assured with\nnovices. It remains an open question as to the most effective way of guiding\ndemonstrators to produce informative demonstrations beyond ad hoc advice for\nspecific teaching tasks. To this end, this paper investigates the use of\nmachine teaching to derive an index for determining the quality of\ndemonstrations and evaluates its use in guiding and training novices to become\nbetter teachers. Experiments with a simple learner robot suggest that guidance\nand training of teachers through the proposed approach can lead to up to 66.5%\ndecrease in error in the learnt skill.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:39:07 GMT"},{"version":"v2","created":"Thu, 13 May 2021 06:01:30 GMT"}],"update_date":"2021-05-14"}
{"id":"2104.08632","submitter":"Yichen Wang","authors":"Yichen Wang, Jason Shuo Zhang, Xu Han, Qin Lv","title":"Jump on the Bandwagon? -- Characterizing Bandwagon Phenomenon in Online\n  NBA Fan Communities","comments":"16 pages, 5 figures, accepted to SocInfo 2020","journal-ref":null,"doi":"10.1007/978-3-030-60975-7_30","report-no":null,"categories":"cs.CY cs.SI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Understanding user dynamics in online communities has become an active\nresearch topic and can provide valuable insights for human behavior analysis\nand community management. In this work, we investigate the \"bandwagon fan\"\nphenomenon, a special case of user dynamics, to provide a large-scale\ncharacterization of online fan loyalty in the context of professional sports\nteams. We leverage the existing structure of NBA-related discussion forums on\nReddit, investigate the general bandwagon patterns, and trace the behavior of\nbandwagon fans to capture latent behavioral characteristics. We observe that\nbetter teams attract more bandwagon fans, but they do not necessarily come from\nweak teams. Our analysis of bandwagon fan flow also shows different trends for\ndifferent teams, as the playoff season progresses. Furthermore, we compare\nbandwagon users with non-bandwagon users in terms of their activity and\nlanguage usage. We find that bandwagon users write shorter comments but receive\nbetter feedback, and use words that show less attachment to their affiliated\nteams. Our observations allow for more effective identification of bandwagon\nusers and prediction of users' future bandwagon behavior in a season, as\ndemonstrated by the significant improvement over the baseline method in our\nevaluation results.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:39:49 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08633","submitter":"Caroline Pacheco Do Espirito Silva","authors":"Caroline Pacheco do Esp\\'irito Silva, Jos\\'e A. M. Felippe De Souza,\n  Antoine Vacavant, Thierry Bouwmans, Andrews Cordolino Sobral","title":"Automated Mathematical Equation Structure Discovery for Visual Analysis","comments":"25 pages, 8 figures, submitted to JMLR","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Finding the best mathematical equation to deal with the different challenges\nfound in complex scenarios requires a thorough understanding of the scenario\nand a trial and error process carried out by experts. In recent years, most\nstate-of-the-art equation discovery methods have been widely applied in\nmodeling and identification systems. However, equation discovery approaches can\nbe very useful in computer vision, particularly in the field of feature\nextraction. In this paper, we focus on recent AI advances to present a novel\nframework for automatically discovering equations from scratch with little\nhuman intervention to deal with the different challenges encountered in\nreal-world scenarios. In addition, our proposal can reduce human bias by\nproposing a search space design through generative network instead of\nhand-designed. As a proof of concept, the equations discovered by our framework\nare used to distinguish moving objects from the background in video sequences.\nExperimental results show the potential of the proposed approach and its\neffectiveness in discovering the best equation in video sequences. The code and\ndata are available at:\nhttps://github.com/carolinepacheco/equation-discovery-scene-analysis\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:42:06 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08634","submitter":"Fawad Ahmad","authors":"Fawad Ahmad, Christina Shin, Eugene Chai, Karthik Sundaresan, and\n  Ramesh Govindan","title":"ARES: Accurate, Autonomous, Near Real-time 3D Reconstruction using\n  Drones","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.SY eess.SY","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Drones will revolutionize 3D modeling. A 3D model represents an accurate\nreconstruction of an object or structure. This paper explores the design and\nimplementation of ARES, which provides near real-time, accurate reconstruction\nof 3D models using a drone-mounted LiDAR; such a capability can be useful to\ndocument construction or check aircraft integrity between flights. Accurate\nreconstruction requires high drone positioning accuracy, and, because GPS can\nbe in accurate, ARES uses SLAM. However, in doing so it must deal with several\ncompeting constraints: drone battery and compute resources, SLAM error\naccumulation, and LiDAR resolution. ARES uses careful trajectory design to find\na sweet spot in this constraint space, a fast reconnaissance flight to narrow\nthe search area for structures, and offloads expensive computations to the\ncloud by streaming compressed LiDAR data over LTE. ARES reconstructs large\nstructures to within 10s of cms and incurs less than 100 ms compute latency.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:42:10 GMT"},{"version":"v2","created":"Tue, 20 Apr 2021 22:37:47 GMT"}],"update_date":"2021-04-22"}
{"id":"2104.08635","submitter":"Andrei Paraschiv","authors":"Andrei Paraschiv, Dumitru-Clementin Cercel, Mihai Dascalu","title":"UPB at SemEval-2021 Task 5: Virtual Adversarial Training for Toxic Spans\n  Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The real-world impact of polarization and toxicity in the online sphere\nmarked the end of 2020 and the beginning of this year in a negative way.\nSemeval-2021, Task 5 - Toxic Spans Detection is based on a novel annotation of\na subset of the Jigsaw Unintended Bias dataset and is the first language\ntoxicity detection task dedicated to identifying the toxicity-level spans. For\nthis task, participants had to automatically detect character spans in short\ncomments that render the message as toxic. Our model considers applying Virtual\nAdversarial Training in a semi-supervised setting during the fine-tuning\nprocess of several Transformer-based models (i.e., BERT and RoBERTa), in\ncombination with Conditional Random Fields. Our approach leads to performance\nimprovements and more robust models, enabling us to achieve an F1-score of\n65.73% in the official submission and an F1-score of 66.13% after further\ntuning during post-evaluation.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:42:12 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08636","submitter":"Christoph Riedl","authors":"Michael Foley, Rory Smead, Patrick Forber, Christoph Riedl","title":"Avoiding the bullies: The resilience of cooperation among unequals","comments":null,"journal-ref":"PLoS Computational Biology 17(4): e1008847, 2021","doi":"10.1371/journal.pcbi.1008847","report-no":null,"categories":"physics.soc-ph cs.GT econ.TH nlin.AO q-bio.PE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Can egalitarian norms or conventions survive the presence of dominant\nindividuals who are ensured of victory in conflicts? We investigate the\ninteraction of power asymmetry and partner choice in games of conflict over a\ncontested resource. We introduce three models to study the emergence and\nresilience of cooperation among unequals when interaction is random, when\nindividuals can choose their partners, and where power asymmetries dynamically\ndepend on accumulated payoffs. We find that the ability to avoid bullies with\nhigher competitive ability afforded by partner choice mostly restores\ncooperative conventions and that the competitive hierarchy never forms. Partner\nchoice counteracts the hyper dominance of bullies who are isolated in the\nnetwork and eliminates the need for others to coordinate in a coalition. When\ncompetitive ability dynamically depends on cumulative payoffs, complex cycles\nof coupled network-strategy-rank changes emerge. Effective collaborators gain\npopularity (and thus power), adopt aggressive behavior, get isolated, and\nultimately lose power. Neither the network nor behavior converge to a stable\nequilibrium. Despite the instability of power dynamics, the cooperative\nconvention in the population remains stable overall and long-term inequality is\ncompletely eliminated. The interaction between partner choice and dynamic power\nasymmetry is crucial for these results: without partner choice, bullies cannot\nbe isolated, and without dynamic power asymmetry, bullies do not lose their\npower even when isolated. We analytically identify a single critical point that\nmarks a phase transition in all three iterations of our models. This critical\npoint is where the first individual breaks from the convention and cycles start\nto emerge.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 19:55:26 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08637","submitter":"Vassilis N. Ioannidis","authors":"Konstantinos D. Polyzos, Costas Mavromatis, Vassilis N. Ioannidis, and\n  Georgios B. Giannakis","title":"Unveiling Anomalous Edges and Nominal Connectivity of Attributed\n  Networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SI cs.AI eess.SP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Uncovering anomalies in attributed networks has recently gained popularity\ndue to its importance in unveiling outliers and flagging adversarial behavior\nin a gamut of data and network science applications including {the Internet of\nThings (IoT)}, finance, security, to list a few. The present work deals with\nuncovering anomalous edges in attributed graphs using two distinct formulations\nwith complementary strengths, which can be easily distributed, and hence\nefficient. The first relies on decomposing the graph data matrix into low rank\nplus sparse components to markedly improve performance. The second broadens the\nscope of the first by performing robust recovery of the unperturbed graph,\nwhich enhances the anomaly identification performance. The novel methods not\nonly capture anomalous edges linking nodes of different communities, but also\nspurious connections between any two nodes with different features. Experiments\nconducted on real and synthetic data corroborate the effectiveness of both\nmethods in the anomaly identification task.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 20:00:40 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08638","submitter":"Priyanka Bose","authors":"Priyanka Bose, Dipanjan Das, Yanju Chen, Yu Feng, Christopher Kruegel,\n  Giovanni Vigna","title":"SAILFISH: Vetting Smart Contract State-Inconsistency Bugs in Seconds","comments":null,"journal-ref":"IEEE Symposium on Security & Privacy, May 2022","doi":null,"report-no":null,"categories":"cs.CR cs.PL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper presents SAILFISH, a scalable system for automatically finding\nstate-inconsistency bugs in smart contracts. To make the analysis tractable, we\nintroduce a hybrid approach that includes (i) a light-weight exploration phase\nthat dramatically reduces the number of instructions to analyze, and (ii) a\nprecise refinement phase based on symbolic evaluation guided by our novel\nvalue-summary analysis, which generates extra constraints to over-approximate\nthe side effects of whole-program execution, thereby ensuring the precision of\nthe symbolic evaluation. We developed a prototype of SAILFISH and evaluated its\nability to detect two state-inconsistency flaws, viz., reentrancy and\ntransaction order dependence (TOD) in Ethereum smart contracts. Further, we\npresent detection rules for other kinds of smart contract flaws that SAILFISH\ncan be extended to detect.\n  Our experiments demonstrate the efficiency of our hybrid approach as well as\nthe benefit of the value summary analysis. In particular, we show that S\nSAILFISH outperforms five state-of-the-art smart contract analyzers (SECURITY,\nMYTHRIL, OYENTE, SEREUM and VANDAL ) in terms of performance, and precision. In\ntotal, SAILFISH discovered 47 previously unknown vulnerable smart contracts out\nof 89,853 smart contracts from ETHERSCAN .\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 20:21:07 GMT"},{"version":"v2","created":"Mon, 13 Dec 2021 04:23:57 GMT"}],"update_date":"2021-12-14"}
{"id":"2104.08639","submitter":"Qianchu Liu","authors":"Qianchu Liu, Edoardo M. Ponti, Diana McCarthy, Ivan Vuli\\'c, Anna\n  Korhonen","title":"AM2iCo: Evaluating Word Meaning in Context across Low-Resource Languages\n  with Adversarial Examples","comments":"EMNLP 2021 long paper","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Capturing word meaning in context and distinguishing between correspondences\nand variations across languages is key to building successful multilingual and\ncross-lingual text representation models. However, existing multilingual\nevaluation datasets that evaluate lexical semantics \"in-context\" have various\nlimitations. In particular, 1) their language coverage is restricted to\nhigh-resource languages and skewed in favor of only a few language families and\nareas, 2) a design that makes the task solvable via superficial cues, which\nresults in artificially inflated (and sometimes super-human) performances of\npretrained encoders, on many target languages, which limits their usefulness\nfor model probing and diagnostics, and 3) little support for cross-lingual\nevaluation. In order to address these gaps, we present AM2iCo (Adversarial and\nMultilingual Meaning in Context), a wide-coverage cross-lingual and\nmultilingual evaluation set; it aims to faithfully assess the ability of\nstate-of-the-art (SotA) representation models to understand the identity of\nword meaning in cross-lingual contexts for 14 language pairs. We conduct a\nseries of experiments in a wide range of setups and demonstrate the challenging\nnature of AM2iCo. The results reveal that current SotA pretrained encoders\nsubstantially lag behind human performance, and the largest gaps are observed\nfor low-resource languages and languages dissimilar to English.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 20:23:45 GMT"},{"version":"v2","created":"Sun, 19 Sep 2021 22:11:00 GMT"}],"update_date":"2021-09-21"}
{"id":"2104.08640","submitter":"Oleg Boyarkin","authors":"G.G.Boyarkina, O.M.Boyarkin","title":"The $(g-2)_{\\mu}$ anomaly, Higgs bosons and heavy neutrinos","comments":null,"journal-ref":"Phys. Rev D 67 (2003) 073023","doi":"10.1103/PhysRevD.67.073023","report-no":null,"categories":"hep-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Within the model based on the $SU (2)_L\\times SU (2)_R\\times U (1)_{B-L} $\ngauge group and having the bidoublet and two triplets of the Higgs fields\n(left-right model) the Higgs sector impact on the value of the muon anomalous\nmagnetic moment is considered. The contributions coming from the doubly charged\nHiggs bosons, the singly charged Higgs bosons and the lightest neutral Higgs\nboson are taken into account. The obtained value of the muon anomalous magnetic\nmoment is the function of the Higgs boson masses and the Higgs boson couplings\nconstants (CC's). We express the most of part of the CC's as the functions of\nthe heavy neutrino sector parameters. We show that at the particular parameters\nvalues the model under study could explain the BNL'00 result.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 20:31:01 GMT"}],"update_date":"2021-04-28"}
{"id":"2104.08641","submitter":"Diego Perez Liebana Dr.","authors":"Diego Perez-Liebana, Cristina Guerrero-Romero, Alexander Dockhorn,\n  Linjie Xu, Jorge Hurtado, Dominik Jeurissen","title":"Generating Diverse and Competitive Play-Styles for Strategy Games","comments":"8 pages, 2 figures, published in Proc. IEEE CoG 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Designing agents that are able to achieve different play-styles while\nmaintaining a competitive level of play is a difficult task, especially for\ngames for which the research community has not found super-human performance\nyet, like strategy games. These require the AI to deal with large action\nspaces, long-term planning and partial observability, among other well-known\nfactors that make decision-making a hard problem. On top of this, achieving\ndistinct play-styles using a general algorithm without reducing playing\nstrength is not trivial. In this paper, we propose Portfolio Monte Carlo Tree\nSearch with Progressive Unpruning for playing a turn-based strategy game\n(Tribes) and show how it can be parameterized so a quality-diversity algorithm\n(MAP-Elites) is used to achieve different play-styles while keeping a\ncompetitive level of play. Our results show that this algorithm is capable of\nachieving these goals even for an extensive collection of game levels beyond\nthose used for training.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 20:33:24 GMT"},{"version":"v2","created":"Mon, 28 Jun 2021 08:59:31 GMT"}],"update_date":"2021-06-29"}
{"id":"2104.08642","submitter":"Bogdan Lobodzinski","authors":"Bogdan {\\L}obodzi\\'nski","title":"Customized determination of stop words using Random Matrix Theory\n  approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The distances between words calculated in word units are studied and compared\nwith the distributions of the Random Matrix Theory (RMT). It is found that the\ndistribution of distance between the same words can be well described by the\nsingle-parameter Brody distribution. Using the Brody distribution fit, we found\nthat the distance between given words in a set of texts can show mixed\ndynamics, coexisting regular and chaotic regimes. It is found that\ndistributions correctly fitted by the Brody distribution with a certain\ngoodness of the fit threshold can be identifid as stop words, usually\nconsidered as the uninformative part of the text. By applying various threshold\nvalues for the goodness of fit, we can extract uninformative words from the\ntexts under analysis to the desired extent. On this basis we formulate a fully\nagnostic recipe that can be used in the creation of a customized set of stop\nwords for texts in any language based on words.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 20:42:28 GMT"},{"version":"v2","created":"Tue, 26 Oct 2021 16:09:49 GMT"}],"update_date":"2021-10-27"}
{"id":"2104.08643","submitter":"Benjamin Lynch","authors":"Benjamin J. Lynch, Erika Palmerio, C. Richard DeVore, Maria D.\n  Kazachenko, Joel T. Dahlin, Jens Pomoell, Emilia K. J. Kilpua","title":"Modeling a Coronal Mass Ejection from an Extended Filament Channel. I.\n  Eruption and Early Evolution","comments":"22 pages, 14 figures, 2 tables, accepted for publication in ApJ","journal-ref":"Astrophys J., 914, 39, 2021","doi":"10.3847/1538-4357/abf9a9","report-no":null,"categories":"astro-ph.SR physics.space-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present observations and modeling of the magnetic field configuration,\nmorphology, and dynamics of a large-scale, high-latitude filament eruption\nobserved by the Solar Dynamics Observatory. We analyze the 2015 July 9-10\nfilament eruption and the evolution of the resulting coronal mass ejection\n(CME) through the solar corona. The slow streamer-blowout CME leaves behind an\nelongated post-eruption arcade above the extended polarity inversion line that\nis only poorly visible in extreme ultraviolet (EUV) disk observations and does\nnot resemble a typical bright flare-loop system. Magnetohydrodynamic (MHD)\nsimulation results from our data-inspired modeling of this eruption compare\nfavorably with the EUV and white-light coronagraph observations. We estimate\nthe reconnection flux from the simulation's flare-arcade growth and examine the\nmagnetic-field orientation and evolution of the erupting prominence,\nhighlighting the transition from an erupting sheared-arcade filament channel\ninto a streamer-blowout flux-rope CME. Our results represent the first\nnumerical modeling of a global-scale filament eruption where multiple ambiguous\nand complex observational signatures in EUV and white light can be fully\nunderstood and explained with the MHD simulation. In this context, our findings\nalso suggest that the so-called \"stealth CME\" classification, as a driver of\nunexpected or \"problem\" geomagnetic storms, belongs more to a continuum of\nobservable/non-observable signatures than to separate or distinct eruption\nprocesses.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 21:07:14 GMT"}],"update_date":"2021-06-17"}
{"id":"2104.08644","submitter":"Avery Miller","authors":"Colin Krisko, Avery Miller","title":"Labeling Schemes for Deterministic Radio Multi-Broadcast","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider the multi-broadcast problem in arbitrary connected radio networks\nconsisting of $n$ nodes. There are $k$ designated source nodes for some fixed\n$k \\in \\{1,\\ldots,n\\}$, and each source node has a distinct piece of\ninformation that it wants to share with all nodes in the network. We set out to\ndetermine the shortest possible labels so that multi-broadcast can be solved\ndeterministically in the labeled radio network by some universal deterministic\ndistributed algorithm.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 21:19:12 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08645","submitter":"Kuan-Hao Huang","authors":"Kuan-Hao Huang, Wasi Uddin Ahmad, Nanyun Peng, Kai-Wei Chang","title":"Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training","comments":"EMNLP 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Pre-trained multilingual language encoders, such as multilingual BERT and\nXLM-R, show great potential for zero-shot cross-lingual transfer. However,\nthese multilingual encoders do not precisely align words and phrases across\nlanguages. Especially, learning alignments in the multilingual embedding space\nusually requires sentence-level or word-level parallel corpora, which are\nexpensive to be obtained for low-resource languages. An alternative is to make\nthe multilingual encoders more robust; when fine-tuning the encoder using\ndownstream task, we train the encoder to tolerate noise in the contextual\nembedding spaces such that even if the representations of different languages\nare not aligned well, the model can still achieve good performance on zero-shot\ncross-lingual transfer. In this work, we propose a learning strategy for\ntraining robust models by drawing connections between adversarial examples and\nthe failure cases of zero-shot cross-lingual transfer. We adopt two widely used\nrobust training methods, adversarial training and randomized smoothing, to\ntrain the desired robust model. The experimental results demonstrate that\nrobust training improves zero-shot cross-lingual transfer on text\nclassification tasks. The improvement is more significant in the generalized\ncross-lingual transfer setting, where the pair of input sentences belong to two\ndifferent languages.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 21:21:53 GMT"},{"version":"v2","created":"Fri, 10 Sep 2021 06:33:53 GMT"}],"update_date":"2021-09-13"}
{"id":"2104.08646","submitter":"Matt Gardner","authors":"Matt Gardner, William Merrill, Jesse Dodge, Matthew E. Peters, Alexis\n  Ross, Sameer Singh, Noah A. Smith","title":"Competency Problems: On Finding and Removing Artifacts in Language Data","comments":"EMNLP 2021. This version fixes an error in Proposition 1 and adds\n  discussion (the EMNLP camera ready version is unfixed) (and v3 adds the\n  acknowledgements that we forgot to put into v2)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Much recent work in NLP has documented dataset artifacts, bias, and spurious\ncorrelations between input features and output labels. However, how to tell\nwhich features have \"spurious\" instead of legitimate correlations is typically\nleft unspecified. In this work we argue that for complex language understanding\ntasks, all simple feature correlations are spurious, and we formalize this\nnotion into a class of problems which we call competency problems. For example,\nthe word \"amazing\" on its own should not give information about a sentiment\nlabel independent of the context in which it appears, which could include\nnegation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of\ncreating data for competency problems when human bias is taken into account,\nshowing that realistic datasets will increasingly deviate from competency\nproblems as dataset size increases. This analysis gives us a simple statistical\ntest for dataset artifacts, which we use to show more subtle biases than were\ndescribed in prior work, including demonstrating that models are\ninappropriately affected by these less extreme biases. Our theoretical\ntreatment of this problem also allows us to analyze proposed solutions, such as\nmaking local edits to dataset instances, and to give recommendations for future\ndata collection and model design efforts that target competency problems.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 21:34:10 GMT"},{"version":"v2","created":"Tue, 30 Nov 2021 21:00:33 GMT"},{"version":"v3","created":"Tue, 28 Dec 2021 20:03:28 GMT"}],"update_date":"2021-12-30"}
{"id":"2104.08647","submitter":"Matan Hasson","authors":"Matan Hasson and Jonathan Berant","title":"Question Decomposition with Dependency Graphs","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  QDMR is a meaning representation for complex questions, which decomposes\nquestions into a sequence of atomic steps. While state-of-the-art QDMR parsers\nuse the common sequence-to-sequence (seq2seq) approach, a QDMR structure\nfundamentally describes labeled relations between spans in the input question,\nand thus dependency-based approaches seem appropriate for this task. In this\nwork, we present a QDMR parser that is based on dependency graphs (DGs), where\nnodes in the graph are words and edges describe logical relations that\ncorrespond to the different computation steps. We propose (a) a\nnon-autoregressive graph parser, where all graph edges are computed\nsimultaneously, and (b) a seq2seq parser that uses gold graph as auxiliary\nsupervision. We find that a graph parser leads to a moderate reduction in\nperformance (0.47 to 0.44), but to a 16x speed-up in inference time due to the\nnon-autoregressive nature of the parser, and to improved sample complexity\ncompared to a seq2seq model. Second, a seq2seq model trained with auxiliary\ngraph supervision has better generalization to new domains compared to a\nseq2seq model, and also performs better on questions with long sequences of\ncomputation steps.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 21:35:31 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08648","submitter":"Trinh Van Chien","authors":"Trinh Van Chien and Hien Quoc Ngo and Symeon Chatzinotas and Marco Di\n  Renzo and Bj\\\"orn Ottersten","title":"Reconfigurable Intelligent Surface-Assisted Cell-Free Massive MIMO\n  Systems Over Spatially-Correlated Channels","comments":"22 pages, 12 figures. Accepted by the IEEE TWC","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IT math.IT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Cell-Free Massive multiple-input multiple-output (MIMO) and reconfigurable\nintelligent surface (RIS) are two promising technologies for application to\nbeyond-5G networks. This paper considers Cell-Free Massive MIMO systems with\nthe assistance of an RIS for enhancing the system performance under the\npresence of spatial correlation among the engineered scattering elements of the\nRIS. Distributed maximum-ratio processing is considered at the access points\n(APs). We introduce an aggregated channel estimation approach that provides\nsufficient information for data processing with the main benefit of reducing\nthe overhead required for channel estimation. The considered system is studied\nby using asymptotic analysis which lets the number of APs and/or the number of\nRIS elements grow large. A lower bound for the channel capacity is obtained for\na finite number of APs and engineered scattering elements of the RIS, and\nclosed-form expressions for the uplink and downlink ergodic net throughput are\nformulated in terms of only the channel statistics. Based on the obtained\nanalytical frameworks, we unveil the impact of channel correlation, the number\nof RIS elements, and the pilot contamination on the net throughput of each\nuser. In addition, a simple control scheme for optimizing the configuration of\nthe engineered scattering elements of the RIS is proposed, which is shown to\nincrease the channel estimation quality, and, hence, the system performance.\nNumerical results demonstrate the effectiveness of the proposed system design\nand performance analysis. In particular, the performance benefits of using RISs\nin Cell-Free Massive MIMO systems are confirmed, especially if the direct links\nbetween the APs and the users are of insufficient quality with high\nprobability.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 21:45:06 GMT"},{"version":"v2","created":"Tue, 10 Aug 2021 03:57:03 GMT"},{"version":"v3","created":"Fri, 17 Dec 2021 13:46:01 GMT"}],"update_date":"2021-12-20"}
{"id":"2104.08649","submitter":"Ryan Vogt","authors":"Ryan H. Vogt, Sarah Strikwerda","title":"Solving Bang-Bang Problems Using The Immersed Interface Method and\n  Integer Programming","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.NA math.DS math.NA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper we study numerically solving optimal control problems with\nbang-bang control functions. We present a formal Lagrangian approach for\nsolving the optimal control problem, and address difficulties encountered when\nnumerically solving the state and adjoint equations by using the immersed\ninterface method. We note that our numerical approach does not approximate the\ndiscontinuous control function with smooth functions, instead we solve the true\nbang-bang optimal control problem. Our approach for solving the optimal control\nproblem uses an adjoint-based gradient. We use the gradient in our first-order\ntrust-region method to generate a local minimizing control. We present detailed\nnumerical results to demonstrate the effectiveness of our method.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 21:48:26 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08650","submitter":"Daniel Blaschke","authors":"Daniel N. Blaschke, Leonid Burakovsky, and Dean L. Preston","title":"On the temperature and density dependence of dislocation drag from\n  phonon wind","comments":"21 pages, 8 figures, 8 tables; v2+v3 minor revision","journal-ref":"J. Appl. Phys. 130 (2021) 015901","doi":"10.1063/5.0054536","report-no":"LA-UR-21-23521","categories":"cond-mat.mtrl-sci","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  At extreme strain rates, where fast moving dislocations govern plastic\ndeformation, anharmonic phonon scattering imparts a drag force on the\ndislocations. In this paper, we present calculations of the dislocation drag\ncoefficients of aluminum and copper as functions of temperature and density. We\ndiscuss the sensitivity of the drag coefficients to changes in the third-order\nelastic constants with temperature and density.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 21:57:12 GMT"},{"version":"v2","created":"Wed, 26 May 2021 16:01:09 GMT"},{"version":"v3","created":"Sat, 12 Jun 2021 20:57:17 GMT"}],"update_date":"2021-07-05"}
{"id":"2104.08651","submitter":"Weizhao Jin","authors":"Weizhao Jin, Xiaoyu Ji, Ruiwen He, Zhou Zhuang, Wenyuan Xu and Yuan\n  Tian","title":"SMS Goes Nuclear: Fortifying SMS-Based MFA in Online Account Ecosystem","comments":"11 pages, 11 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.NI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  With the rapid growth of online services, the number of online accounts\nproliferates. The security of a single user account no longer depends merely on\nits own service provider but also the accounts on other service platforms(We\nrefer to this online account environment as Online Account Ecosystem). In this\npaper, we first uncover the vulnerability of Online Account Ecosystem, which\nstems from the defective multi-factor authentication (MFA), specifically the\nones with SMS-based verification, and dependencies among accounts on different\nplatforms. We propose Chain Reaction Attack that exploits the weakest point in\nOnline Account Ecosystem and can ultimately compromise the most secure\nplatform. Furthermore, we design and implement ActFort, a systematic approach\nto detect the vulnerability of Online Account Ecosystem by analyzing the\nauthentication credential factors and sensitive personal information as well as\nevaluating the dependency relationships among online accounts. We evaluate our\nsystem on hundreds of representative online services listed in Alexa in\ndiversified fields. Based on the analysis from ActFort, we provide several\npragmatic insights into the current Online Account Ecosystem and propose\nseveral feasible countermeasures including the online account exposed\ninformation protection mechanism and the built-in authentication to fortify the\nsecurity of Online Account Ecosystem.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 22:20:16 GMT"},{"version":"v2","created":"Mon, 19 Jul 2021 20:44:26 GMT"}],"update_date":"2021-07-21"}
{"id":"2104.08652","submitter":"Karthik Menon","authors":"Karthik Menon, Rajat Mittal","title":"Significance of the strain-dominated region around a vortex on induced\n  aerodynamic loads","comments":null,"journal-ref":null,"doi":"10.1017/jfm.2021.359","report-no":null,"categories":"physics.flu-dyn","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The ability of vortices to induce aerodynamic loads on proximal surfaces\nplays a significant role in a wide variety of flows. However, most studies of\nvortex-induced effects primarily focus on analyzing the influence of the\nrotation-dominated cores of vortices. In this work, we show that not only are\nvortices in viscous flows surrounded by strain-dominated regions, but that\nthese regions are dynamically important and can sometimes even dictate the\ninduced aerodynamic loads. We demonstrate this for a pitching airfoil, which\nexhibits dynamic stall and generates several force-inducing vortices. Using a\ndata-driven force partitioning method, we quantify the influence of vortices as\nwell as vortex-associated strain to show that our current understanding of\nvortex-dominated phenomena, such as dynamic stall, is incomplete without\nconsidering the substantial effect of strain-dominated regions that are\nassociated with vortices.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 22:22:17 GMT"}],"update_date":"2021-05-26"}
{"id":"2104.08653","submitter":"Baban Gain","authors":"Baban Gain, Dibyanayan Bandyopadhyay, Tanik Saikh, Asif Ekbal","title":"IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT","comments":"5 pages. IITP@ COLIEE 2019: legal information retrieval using BM25\n  and BERT. Proceedings of the 6th Competition on Legal Information\n  Extraction/Entailment. COLIEE","journal-ref":null,"doi":"10.13140/RG.2.2.24136.44804","report-no":null,"categories":"cs.CL cs.AI cs.IR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Natural Language Processing (NLP) and Information Retrieval (IR) in the\njudicial domain is an essential task. With the advent of availability\ndomain-specific data in electronic form and aid of different Artificial\nintelligence (AI) technologies, automated language processing becomes more\ncomfortable, and hence it becomes feasible for researchers and developers to\nprovide various automated tools to the legal community to reduce human burden.\nThe Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in\nassociation with the International Conference on Artificial Intelligence and\nLaw (ICAIL)-2019 has come up with few challenging tasks. The shared defined\nfour sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to\nprovide few automated systems to the judicial system. The paper presents our\nworking note on the experiments carried out as a part of our participation in\nall the sub-tasks defined in this shared task. We make use of different\nInformation Retrieval(IR) and deep learning based approaches to tackle these\nproblems. We obtain encouraging results in all these four sub-tasks.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 22:28:15 GMT"},{"version":"v2","created":"Thu, 29 Apr 2021 19:07:25 GMT"},{"version":"v3","created":"Tue, 22 Jun 2021 08:39:42 GMT"},{"version":"v4","created":"Thu, 24 Jun 2021 14:40:18 GMT"}],"update_date":"2021-07-07"}
{"id":"2104.08654","submitter":"Ioanna Stamou","authors":"Ioanna D. Stamou","title":"Mechanisms of Producing Primordial Black Holes By Breaking The\n  SU(2,1)/SU(2)$\\times$U(1) Symmetry","comments":"36 pages, 12 figures","journal-ref":"Phys. Rev. D 103, 083512 (2021)","doi":"10.1103/PhysRevD.103.083512","report-no":null,"categories":"hep-ph astro-ph.CO hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper we present a class of potentials derived by no-scale\nsupergravity in order to explain the production of primordial black holes\n(PBHs) in the Universe. By breaking the SU(2,1)/SU(2)$\\times$U(1) symmetry we\nfix one of the two chiral fields and we derive effective scalar potentials\nwhich are capable of generating PBHs. Specifically, we modify well-known\nsuperpotentials, which reduce to Starobinsky-like effective scalar potentials.\nThus, we derive scalar potentials which, on the one hand, explain the\nproduction of PBHs and, on the other hand, they conserve the transformation\nlaws, which yield from the parametrization of the coset\nSU(2,1)/SU(2)$\\times$U(1). Moreover, we generate PBHs by modifying the kinetic\nterm of the Langrangian (or the K\\\"ahler potential) and we keep the\nsuperpotentials unmodified. In all cases we evaluate the fractional abundances\nof PBHs by comparing Press-Schechter approach and peak theory, while focusing\non explaining the dark matter in the Universe. All models are in complete\nconsistence with Planck constraints.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 22:39:27 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08655","submitter":"Abhyuday Bhartiya","authors":"Abhyuday Bhartiya, Kartikeya Badola, Mausam","title":"DiS-ReX: A Multilingual Dataset for Distantly Supervised Relation\n  Extraction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Distant supervision (DS) is a well established technique for creating\nlarge-scale datasets for relation extraction (RE) without using human\nannotations. However, research in DS-RE has been mostly limited to the English\nlanguage. Constraining RE to a single language inhibits utilization of large\namounts of data in other languages which could allow extraction of more diverse\nfacts. Very recently, a dataset for multilingual DS-RE has been released.\nHowever, our analysis reveals that the proposed dataset exhibits unrealistic\ncharacteristics such as 1) lack of sentences that do not express any relation,\nand 2) all sentences for a given entity pair expressing exactly one relation.\nWe show that these characteristics lead to a gross overestimation of the model\nperformance. In response, we propose a new dataset, DiS-ReX, which alleviates\nthese issues. Our dataset has more than 1.5 million sentences, spanning across\n4 languages with 36 relation classes + 1 no relation (NA) class. We also modify\nthe widely used bag attention models by encoding sentences using mBERT and\nprovide the first benchmark results on multilingual DS-RE. Unlike the competing\ndataset, we show that our dataset is challenging and leaves enough room for\nfuture research to take place in this field.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 22:44:38 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08656","submitter":"Wenxuan Zhou","authors":"Wenxuan Zhou, Muhao Chen","title":"Learning from Noisy Labels for Entity-Centric Information Extraction","comments":"Accepted at EMNLP 2021. Code available at\n  https://github.com/wzhouad/NLL-IE","journal-ref":null,"doi":"10.18653/v1/2021.emnlp-main.437","report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recent information extraction approaches have relied on training deep neural\nmodels. However, such models can easily overfit noisy labels and suffer from\nperformance degradation. While it is very costly to filter noisy labels in\nlarge learning resources, recent studies show that such labels take more\ntraining steps to be memorized and are more frequently forgotten than clean\nlabels, therefore are identifiable in training. Motivated by such properties,\nwe propose a simple co-regularization framework for entity-centric information\nextraction, which consists of several neural models with identical structures\nbut different parameter initialization. These models are jointly optimized with\nthe task-specific losses and are regularized to generate similar predictions\nbased on an agreement loss, which prevents overfitting on noisy labels.\nExtensive experiments on two widely used but noisy benchmarks for information\nextraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework.\nWe release our code to the community for future research.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 22:49:12 GMT"},{"version":"v2","created":"Tue, 7 Sep 2021 19:08:22 GMT"}],"update_date":"2022-01-24"}
{"id":"2104.08657","submitter":"Jiang Yu Zheng Dr.","authors":"Jiang Yu Zheng","title":"IUPUI Driving Videos and Images in All Weather and Illumination\n  Conditions","comments":"10 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This document describes an image and video dataset of driving views captured\nin all weather and illumination conditions. The data set has been submitted to\nCDVL.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 22:59:15 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08658","submitter":"Sa\\'ul Herrera Sa\\'ul A. Herrera","authors":"Sa\\'ul A. Herrera and Gerardo G. Naumis","title":"Optoelectronic Fingerprints of Interference between Different Charge\n  Carriers in Graphene Superlattices and Analogies to Twisted Graphene Bilayers","comments":null,"journal-ref":"Phys. Rev. B 104, 115424 (2021)","doi":"10.1103/PhysRevB.104.115424","report-no":null,"categories":"cond-mat.mes-hall","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Motivated by recent experimental findings on the low-energy spectrum of\nKekul\\'e-patterned graphene, the optoelectronic signatures of graphene\nsuperlattices with a spatial modulation that triples the size of the unit cell\nand folds the valleys to the center of the Brillouin zone are studied. For\nsuperlattices like those visualized in recent experiments, the optoelectronic\nresponse reveals multiple species of carriers distinguished by their effective\nmasses or Fermi velocities. Their signatures are similar to those of systems\nhosting multifold fermions in which different frequency intervals are dominated\nby different types of quasiparticles. Remarkably, the response of these systems\nexhibits a characteristic peak in the optical conductivity suggesting a kind of\ninterference between the different species of carriers. We also discuss a\nrelated superlattice that exhibits merging Dirac cones and band flattening,\nwith a Hamiltonian that resembles a version of the chiral model for twisted\nbilayer graphene where the long-range moir\\'e modulation has been substituted\nby a two-parameter bias.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 22:59:42 GMT"},{"version":"v2","created":"Fri, 27 Aug 2021 16:56:33 GMT"}],"update_date":"2021-09-22"}
{"id":"2104.08659","submitter":"Zeming Chen","authors":"Zeming Chen, Qiyue Gao","title":"Monotonicity Marking from Universal Dependency Trees","comments":"10 pages, 3 figures, The 14th International Conference on\n  Computational Semantics (IWCS 2021)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Dependency parsing is a tool widely used in the field of Natural language\nprocessing and computational linguistics. However, there is hardly any work\nthat connects dependency parsing to monotonicity, which is an essential part of\nlogic and linguistic semantics. In this paper, we present a system that\nautomatically annotates monotonicity information based on Universal Dependency\nparse trees. Our system utilizes surface-level monotonicity facts about\nquantifiers, lexical items, and token-level polarity information. We compared\nour system's performance with existing systems in the literature, including\nNatLog and ccg2mono, on a small evaluation dataset. Results show that our\nsystem outperforms NatLog and ccg2mono.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 23:01:10 GMT"},{"version":"v2","created":"Wed, 12 May 2021 21:53:25 GMT"}],"update_date":"2021-05-14"}
{"id":"2104.08660","submitter":"Masatoshi Hirabayashi","authors":"M. Hirabayashi, Y. Mimasu, N. Sakatani, S. Watanabe, Y. Tsuda, T.\n  Saiki, S. Kikuchi, T. Kouyama, M. Yoshikawa, S. Tanaka, S. Nakazawa, Y.\n  Takei, F. Terui, H. Takeuchi, A. Fujii, T. Iwata, K. Tsumura, S. Matsuura, Y.\n  Shimaki, S. Urakawa, Y. Ishibashi, S. Hasegawa, M. Ishiguro, D. Kuroda, S.\n  Okumura, S. Sugita, T. Okada, S. Kameda, S. Kamata, A. Higuchi, H. Senshu, H.\n  Noda, K. Matsumoto, R. Suetsugu, T. Hirai, K. Kitazato, D. Farnocchia, S.P.\n  Naidu, D.J. Tholen, C.W. Hergenrother, R.J. Whiteley, N. A. Moskovitz, P.A.\n  Abell, the Hayabusa2 extended mission study group","title":"Hayabusa2 Extended Mission: New Voyage to Rendezvous with a Small\n  Asteroid Rotating with a Short Period","comments":"51 pages, 9 figures, and 10 tables. The article is in press in\n  Advances in Space Research","journal-ref":null,"doi":"10.1016/j.asr.2021.03.030","report-no":null,"categories":"astro-ph.IM astro-ph.EP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Hayabusa2 is the Japanese Asteroid Return Mission and targeted the\ncarbonaceous asteroid Ryugu, conducted by the Japan Aerospace Exploration\nAgency (JAXA). The goal of this mission was to conduct proximity operations\nincluding remote sensing observations, material sampling, and a Small Carry-On\nImpact experiment, as well as sample analyses. As of September 2020, the\nspacecraft is on the way back to Earth with samples from Ryugu with no critical\nissues after the successful departure in November 2019. Here, we propose an\nextended mission in which the spacecraft will rendezvous with a small asteroid\nwith ~30 m - ~40 m in diameter that is rotating at a spin period of ~10 min\nafter an additional ~10-year cruise phase. We introduce that two scenarios are\nsuitable for the extended mission. In the first scenario, the spacecraft will\nperform swing-by maneuvers at Venus once and Earth twice to arrive at asteroid\n2001 AV43. In the second scenario, it will perform swing-by maneuvers at Earth\ntwice to reach asteroid 1998 KY26. In both scenarios, the mission will continue\nuntil the early 2030s. JAXA recently released the decision that the spacecraft\nwill rendezvous with 1998 KY26. This paper focuses on our scientific\nassessments of the two scenarios but leaves the decision process to go to 1998\nKY26 for future reports. Rendezvous operations will be planned to detail the\nphysical properties and surrounding environments of the target, one of the\nsmallest elements of small planetary bodies. By achieving the planned\noperations, the mission will provide critical hints on the violent histories of\ncollisions and accumulations of small bodies in the solar system. Furthermore,\nthe established scientific knowledge and techniques will advance key\ntechnologies for planetary defense.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 23:05:03 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08661","submitter":"Bhavana Dalvi Mishra","authors":"Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah\n  Smith, Leighanna Pipatanangkura, Peter Clark","title":"Explaining Answers with Entailment Trees","comments":"published in EMNLP 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Our goal, in the context of open-domain textual question-answering (QA), is\nto explain answers by showing the line of reasoning from what is known to the\nanswer, rather than simply showing a fragment of textual evidence (a\n\"rationale'\"). If this could be done, new opportunities for understanding and\ndebugging the system's reasoning become possible. Our approach is to generate\nexplanations in the form of entailment trees, namely a tree of multipremise\nentailment steps from facts that are known, through intermediate conclusions,\nto the hypothesis of interest (namely the question + answer). To train a model\nwith this skill, we created ENTAILMENTBANK, the first dataset to contain\nmultistep entailment trees. Given a hypothesis (question + answer), we define\nthree increasingly difficult explanation tasks: generate a valid entailment\ntree given (a) all relevant sentences (b) all relevant and some irrelevant\nsentences, or (c) a corpus. We show that a strong language model can partially\nsolve these tasks, in particular when the relevant sentences are included in\nthe input (e.g., 35% of trees for (a) are perfect), and with indications of\ngeneralization to other domains. This work is significant as it provides a new\ntype of dataset (multistep entailments) and baselines, offering a new avenue\nfor the community to generate richer, more systematic explanations.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 23:13:56 GMT"},{"version":"v2","created":"Sun, 19 Sep 2021 07:32:44 GMT"},{"version":"v3","created":"Sat, 28 May 2022 05:35:37 GMT"}],"update_date":"2022-05-31"}
{"id":"2104.08662","submitter":"David Buckley","authors":"K.P. Singh, V. Girish, J. Tiwari, P.E. Barrett, D.A.H. Buckley, S.B.\n  Potter, E. Schlegel, V. Rana and G. Stewart","title":"Observations of AR Sco with $Chandra$ and $AstroSat$ Soft X-ray\n  Telescope","comments":"9 pages, 8 figures, to be published in the Journal of Astrophysics\n  and Astronomy","journal-ref":null,"doi":"10.1007/s12036-021-09756-w","report-no":null,"categories":"astro-ph.HE astro-ph.SR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present our $AstroSat$ soft X-ray observations of a compact binary system,\nAR Sco, and analysis of its X-ray observations with $Chandra$ that were taken\nonly about a week before the $AstroSat$ observations. An analysis of the soft\nX-ray ($0.3-2.0$ keV) data limits the modulation of the spin, orbital, or beat\nperiods to less than 0.03 counts s$^{-1}$ or $<$10\\% of the average count rate.\nThe X-ray flux obtained from both observatories is found to be almost identical\n(within a few percent) in flux, and about 30\\% lower than reported from the\nnine months older observations with $XMM-Newton$. A two-temperature thermal\nplasma model with the same spectral parameters fit $Chandra$ and $AstroSat$\ndata very well, and requires very little absorption in the line of sight to the\nsource. The low-temperature component has the same temperature ($\\sim$1 keV) as\nreported earlier, but the high-temperature component has a lower temperature of\n5.0$^{+0.8}_{-0.7}$ keV as compared to 8.0 keV measured earlier, however, the\ndifference is not statistically significant.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 23:26:05 GMT"}],"update_date":"2021-08-11"}
{"id":"2104.08663","submitter":"Nandan Thakur","authors":"Nandan Thakur, Nils Reimers, Andreas R\\\"uckl\\'e, Abhishek Srivastava,\n  Iryna Gurevych","title":"BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information\n  Retrieval Models","comments":"Accepted at NeurIPS 2021 Dataset and Benchmark Track","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR cs.AI cs.CL","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Existing neural information retrieval (IR) models have often been studied in\nhomogeneous and narrow settings, which has considerably limited insights into\ntheir out-of-distribution (OOD) generalization capabilities. To address this,\nand to facilitate researchers to broadly evaluate the effectiveness of their\nmodels, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous\nevaluation benchmark for information retrieval. We leverage a careful selection\nof 18 publicly available datasets from diverse text retrieval tasks and domains\nand evaluate 10 state-of-the-art retrieval systems including lexical, sparse,\ndense, late-interaction and re-ranking architectures on the BEIR benchmark. Our\nresults show BM25 is a robust baseline and re-ranking and\nlate-interaction-based models on average achieve the best zero-shot\nperformances, however, at high computational costs. In contrast, dense and\nsparse-retrieval models are computationally more efficient but often\nunderperform other approaches, highlighting the considerable room for\nimprovement in their generalization capabilities. We hope this framework allows\nus to better evaluate and understand existing retrieval systems, and\ncontributes to accelerating progress towards better robust and generalizable\nsystems in the future. BEIR is publicly available at\nhttps://github.com/UKPLab/beir.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 23:29:55 GMT"},{"version":"v2","created":"Wed, 28 Apr 2021 13:59:17 GMT"},{"version":"v3","created":"Tue, 7 Sep 2021 20:33:14 GMT"},{"version":"v4","created":"Thu, 21 Oct 2021 01:18:28 GMT"}],"update_date":"2021-10-22"}
{"id":"2104.08664","submitter":"Michaela Socolof","authors":"Michaela Socolof, Jackie Chi Kit Cheung, Michael Wagner, Timothy J.\n  O'Donnell","title":"Characterizing Idioms: Conventionality and Contingency","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Idioms are unlike most phrases in two important ways. First, the words in an\nidiom have non-canonical meanings. Second, the non-canonical meanings of words\nin an idiom are contingent on the presence of other words in the idiom.\nLinguistic theories differ on whether these properties depend on one another,\nas well as whether special theoretical machinery is needed to accommodate\nidioms. We define two measures that correspond to the properties above, and we\nimplement them using BERT (Devlin et al., 2019) and XLNet(Yang et al., 2019).\nWe show that idioms fall at the expected intersection of the two dimensions,\nbut that the dimensions themselves are not correlated. Our results suggest that\nspecial machinery to handle idioms may not be warranted.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 23:46:57 GMT"},{"version":"v2","created":"Wed, 16 Mar 2022 20:33:24 GMT"},{"version":"v3","created":"Wed, 14 Sep 2022 22:01:46 GMT"}],"update_date":"2022-09-16"}
{"id":"2104.08665","submitter":"Tsung-Ming Tai","authors":"Tsung-Ming Tai, Giuseppe Fiameni, Cheng-Kuang Lee, Oswald Lanz","title":"Higher Order Recurrent Space-Time Transformer for Video Action\n  Prediction","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Endowing visual agents with predictive capability is a key step towards video\nintelligence at scale. The predominant modeling paradigm for this is sequence\nlearning, mostly implemented through LSTMs. Feed-forward Transformer\narchitectures have replaced recurrent model designs in ML applications of\nlanguage processing and also partly in computer vision. In this paper we\ninvestigate on the competitiveness of Transformer-style architectures for video\npredictive tasks. To do so we propose HORST, a novel higher order recurrent\nlayer design whose core element is a spatial-temporal decomposition of\nself-attention for video. HORST achieves state of the art competitive\nperformance on Something-Something early action recognition and EPIC-Kitchens\naction anticipation, showing evidence of predictive capability that we\nattribute to our recurrent higher order design of self-attention.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 23:51:05 GMT"},{"version":"v2","created":"Sun, 19 Sep 2021 11:15:08 GMT"},{"version":"v3","created":"Tue, 21 Sep 2021 05:25:42 GMT"}],"update_date":"2021-09-22"}
{"id":"2104.08666","submitter":"Tejas Srinivasan","authors":"Tejas Srinivasan, Yonatan Bisk","title":"Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language\n  Models","comments":"Accepted to 4th Workshop on Gender Bias in Natural Language\n  Processing, NAACL 2022","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Numerous works have analyzed biases in vision and pre-trained language models\nindividually - however, less attention has been paid to how these biases\ninteract in multimodal settings. This work extends text-based bias analysis\nmethods to investigate multimodal language models, and analyzes intra- and\ninter-modality associations and biases learned by these models. Specifically,\nwe demonstrate that VL-BERT (Su et al., 2020) exhibits gender biases, often\npreferring to reinforce a stereotype over faithfully describing the visual\nscene. We demonstrate these findings on a controlled case-study and extend them\nfor a larger set of stereotypically gendered entities.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 00:02:32 GMT"},{"version":"v2","created":"Fri, 20 May 2022 00:45:40 GMT"}],"update_date":"2022-05-23"}
{"id":"2104.08667","submitter":"Satwik Kottur","authors":"Satwik Kottur, Seungwhan Moon, Alborz Geramifard, Babak Damavandi","title":"SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal\n  Conversations","comments":"10 pages, 7 figures, 5 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Next generation task-oriented dialog systems need to understand\nconversational contexts with their perceived surroundings, to effectively help\nusers in the real-world multimodal environment. Existing task-oriented dialog\ndatasets aimed towards virtual assistance fall short and do not situate the\ndialog in the user's multimodal context. To overcome, we present a new dataset\nfor Situated and Interactive Multimodal Conversations, SIMMC 2.0, which\nincludes 11K task-oriented user<->assistant dialogs (117K utterances) in the\nshopping domain, grounded in immersive and photo-realistic scenes.\n  The dialogs are collected using a two-phase pipeline: (1) A novel multimodal\ndialog simulator generates simulated dialog flows, with an emphasis on\ndiversity and richness of interactions, (2) Manual paraphrasing of the\ngenerated utterances to collect diverse referring expressions. We provide an\nin-depth analysis of the collected dataset, and describe in detail the four\nmain benchmark tasks we propose. Our baseline model, powered by the\nstate-of-the-art language model, shows promising results, and highlights new\nchallenges and directions for the community to study.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 00:14:29 GMT"},{"version":"v2","created":"Wed, 20 Oct 2021 23:42:35 GMT"}],"update_date":"2021-10-22"}
{"id":"2104.08668","submitter":"Darsh Shah","authors":"Darsh J Shah and Regina Barzilay","title":"Generating Related Work","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Communicating new research ideas involves highlighting similarities and\ndifferences with past work. Authors write fluent, often long sections to survey\nthe distinction of a new paper with related work. In this work we model\ngenerating related work sections while being cognisant of the motivation behind\nciting papers. Our content planning model generates a tree of cited papers\nbefore a surface realization model lexicalizes this skeleton. Our model\noutperforms several strong state-of-the-art summarization and multi-document\nsummarization models on generating related work on an ACL Anthology (AA) based\ndataset which we contribute.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 00:19:37 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08669","submitter":"Sungwoo Jeong","authors":"Alan Edelman and Sungwoo Jeong","title":"Fifty Three Matrix Factorizations: A systematic approach","comments":"73 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The success of matrix factorizations such as the singular value decomposition\n(SVD) has motivated the search for even more factorizations. We catalog 53\nmatrix factorizations, most of which we believe to be new. Our systematic\napproach, inspired by the generalized Cartan decomposition of Lie theory, also\nencompasses known factorizations such as the SVD, the symmetric\neigendecomposition, the CS decomposition, the hyperbolic SVD, structured SVDs,\nthe Takagi factorization, and others thereby covering familiar matrix\nfactorizations as well as ones that were waiting to be discovered. We suggest\nthat Lie theory has one way or another been lurking hidden in the foundations\nof the very successful field of matrix computations with applications routinely\nused in so many areas of computation. In this paper, we investigate\nconsequences of the Cartan decomposition and the little known generalized\nCartan decomposition for matrix factorizations. We believe that these\nfactorizations once properly identified can lead to further work on algorithmic\ncomputations and applications.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 00:20:29 GMT"},{"version":"v2","created":"Mon, 7 Jun 2021 15:42:46 GMT"},{"version":"v3","created":"Tue, 8 Jun 2021 14:06:19 GMT"},{"version":"v4","created":"Mon, 13 Dec 2021 21:20:29 GMT"},{"version":"v5","created":"Mon, 14 Feb 2022 16:14:43 GMT"}],"update_date":"2022-02-15"}
{"id":"2104.08670","submitter":"Yong Zheng","authors":"Yong Zheng","title":"Solution of one-dimensional Bose Hubbard model in large-$U$ limit","comments":null,"journal-ref":null,"doi":"10.7566/JPSJ.91.014002","report-no":null,"categories":"cond-mat.str-el","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The one-dimensional Bose-Hubbard model in large-$U$ limit has been studied\nvia reducing and mapping the Hamiltonian to a simpler one. The eigenstates and\neigenvalues have been obtained exactly in the subspaces with fixed numbers of\nsingle- and double-occupancies but without multiple-occupancies, and the\nthermodynamic properties of the system have been calculated further. These\neigenstates and eigenvalues also enable us to develop a new perturbation\ntreatment of the model, with which the ground-state energy has been calculated\nexactly to first order in $1/U$.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 00:22:49 GMT"}],"update_date":"2021-12-21"}
{"id":"2104.08671","submitter":"Lucia Zheng","authors":"Lucia Zheng, Neel Guha, Brandon R. Anderson, Peter Henderson, Daniel\n  E. Ho","title":"When Does Pretraining Help? Assessing Self-Supervised Learning for Law\n  and the CaseHOLD Dataset","comments":"ICAIL 2021. Code & data available at\n  https://github.com/reglab/casehold","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  While self-supervised learning has made rapid advances in natural language\nprocessing, it remains unclear when researchers should engage in\nresource-intensive domain-specific pretraining (domain pretraining). The law,\npuzzlingly, has yielded few documented instances of substantial gains to domain\npretraining in spite of the fact that legal language is widely seen to be\nunique. We hypothesize that these existing results stem from the fact that\nexisting legal NLP tasks are too easy and fail to meet conditions for when\ndomain pretraining can help. To address this, we first present CaseHOLD (Case\nHoldings On Legal Decisions), a new dataset comprised of over 53,000+ multiple\nchoice questions to identify the relevant holding of a cited case. This dataset\npresents a fundamental task to lawyers and is both legally meaningful and\ndifficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second,\nwe assess performance gains on CaseHOLD and existing legal NLP datasets. While\na Transformer architecture (BERT) pretrained on a general corpus (Google Books\nand Wikipedia) improves performance, domain pretraining (using corpus of\napproximately 3.5M decisions across all courts in the U.S. that is larger than\nBERT's) with a custom legal vocabulary exhibits the most substantial\nperformance gains with CaseHOLD (gain of 7.2% on F1, representing a 12%\nimprovement on BERT) and consistent performance gains across two other legal\ntasks. Third, we show that domain pretraining may be warranted when the task\nexhibits sufficient similarity to the pretraining corpus: the level of\nperformance increase in three legal tasks was directly tied to the domain\nspecificity of the task. Our findings inform when researchers should engage\nresource-intensive pretraining and show that Transformer-based architectures,\ntoo, learn embeddings suggestive of distinct legal language.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 00:57:16 GMT"},{"version":"v2","created":"Mon, 17 May 2021 22:45:11 GMT"},{"version":"v3","created":"Tue, 6 Jul 2021 00:56:00 GMT"}],"update_date":"2021-07-07"}
{"id":"2104.08672","submitter":"Bilel Hamil","authors":"Bilel Hamil and Bekir Can L\\\"utf\\\"uo\\u{g}lu","title":"Black hole thermodynamics in the presence of a maximal length and\n  minimum measurable in momentum","comments":"5 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this work, incorporating the effect of the minimum measurable in momentum\nand maximal length, we studied thermodynamics property of Schwarzschild black\nhole and the Unruh effect. {\\color{red} According to this scenario, we see that\nthe black hole temperature cannot be smaller than a certain minimum value of $\nT_{\\min} $. Moreover, we find that black hole mass cannot be larger than a\nmaximum mass value of $M_{\\max }$. Considering these findings first we compute\nthe corrected Hawking temperature versus the mass and examine its\ncharacteristic behavior. Then, we derive the black hole's entropy and heat\ncapacity. We find that the black hole is stable when $\\frac{M_{\\max\n}}{\\sqrt{3}}<M<M_{\\max }$. Finally, we examined the modified Unruh effect. We\nfind that the modified Unruh temperature explicitly depends on $\\alpha$.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 01:14:22 GMT"},{"version":"v2","created":"Tue, 20 Apr 2021 15:46:10 GMT"},{"version":"v3","created":"Wed, 21 Apr 2021 14:13:05 GMT"},{"version":"v4","created":"Wed, 26 May 2021 20:49:57 GMT"}],"update_date":"2021-05-28"}
{"id":"2104.08673","submitter":"Zihan Wang","authors":"Zihan Wang and Chengyu Dong and Jingbo Shang","title":"\"Average\" Approximates \"First Principal Component\"? An Empirical\n  Analysis on Representations from Neural Language Models","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Contextualized representations based on neural language models have furthered\nthe state of the art in various NLP tasks. Despite its great success, the\nnature of such representations remains a mystery. In this paper, we present an\nempirical property of these representations -- \"average\" approximates \"first\nprincipal component\". Specifically, experiments show that the average of these\nrepresentations shares almost the same direction as the first principal\ncomponent of the matrix whose columns are these representations. We believe\nthis explains why the average representation is always a simple yet strong\nbaseline. Our further examinations show that this property also holds in more\nchallenging scenarios, for example, when the representations are from a model\nright after its random initialization. Therefore, we conjecture that this\nproperty is intrinsic to the distribution of representations and not\nnecessarily related to the input structure. We realize that these\nrepresentations empirically follow a normal distribution for each dimension,\nand by assuming this is true, we demonstrate that the empirical property can be\nin fact derived mathematically.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 01:15:40 GMT"},{"version":"v2","created":"Mon, 7 Feb 2022 23:30:10 GMT"}],"update_date":"2022-02-09"}
{"id":"2104.08674","submitter":"Akaki Tikaradze","authors":"Akaki Tikaradze","title":"Noncommutative Noether's problem is almost equivalent to the classical\n  Noether's problem","comments":"4 pages, to appear in Advances in Math","journal-ref":null,"doi":null,"report-no":null,"categories":"math.QA math.RT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Motivated by the classical Noether's problem, J. Alev and F. Dumas proposed\nthe following question, commonly referred to as the noncommutative Noether's\nproblem: Let a finite group $G$ act linearly on $\\mathbb{C}^n,$ inducing the\naction on $\\text{Frac}(A_n(\\mathbb{C}))$-the skew field of fractions of the\n$n$-th Weyl algebra $A_n(\\mathbb{C}),$ then is $\\text{Frac}(A_n(\\mathbb{C}))^G$\nisomorphic to $\\text{Frac}(A_n(\\mathbb{C}))?$ In this note we show that if\n$\\text{Frac}(A_n(\\mathbb{C}))^{G}\\cong \\text{Frac}(A_n(\\mathbb{C})),$ then for\nany algebraically closed field $k$ of large enough characteristic, field\n$k(x_1,\\cdots, x_n)^G$ is stably rational. This result allows us to produce\ncounterexamples to the noncommutative Noether's problem based on well-known\ncounterexamples to the Noether's problem for algebraically closed fields.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 01:15:46 GMT"},{"version":"v2","created":"Thu, 9 Dec 2021 21:39:07 GMT"}],"update_date":"2021-12-13"}
{"id":"2104.08675","submitter":"Xingyi Cheng","authors":"Xingyi Cheng","title":"Dual-View Distilled BERT for Sentence Embedding","comments":"Accepted at SIGIR 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Recently, BERT realized significant progress for sentence matching via\nword-level cross sentence attention. However, the performance significantly\ndrops when using siamese BERT-networks to derive two sentence embeddings, which\nfall short in capturing the global semantic since the word-level attention\nbetween two sentences is absent. In this paper, we propose a Dual-view\ndistilled BERT~(DvBERT) for sentence matching with sentence embeddings. Our\nmethod deals with a sentence pair from two distinct views, i.e., Siamese View\nand Interaction View. Siamese View is the backbone where we generate sentence\nembeddings. Interaction View integrates the cross sentence interaction as\nmultiple teachers to boost the representation ability of sentence embeddings.\nExperiments on six STS tasks show that our method outperforms the\nstate-of-the-art sentence embedding methods significantly.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 01:20:11 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08676","submitter":"Xiang Zhou","authors":"Xiang Zhou, Yixin Nie, Mohit Bansal","title":"Distributed NLI: Learning to Predict Human Opinion Distributions for\n  Language Reasoning","comments":"ACL 2022 Findings (16 pages)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We introduce distributed NLI, a new NLU task with a goal to predict the\ndistribution of human judgements for natural language inference. We show that\nby applying additional distribution estimation methods, namely, Monte Carlo\n(MC) Dropout, Deep Ensemble, Re-Calibration, and Distribution Distillation,\nmodels can capture human judgement distribution more effectively than the\nsoftmax baseline. We show that MC Dropout is able to achieve decent performance\nwithout any distribution annotations while Re-Calibration can give further\nimprovements with extra distribution annotations, suggesting the value of\nmultiple annotations for one example in modeling the distribution of human\njudgements. Despite these improvements, the best results are still far below\nthe estimated human upper-bound, indicating that predicting the distribution of\nhuman judgements is still an open, challenging problem with a large room for\nimprovements. We showcase the common errors for MC Dropout and Re-Calibration.\nFinally, we give guidelines on the usage of these methods with different levels\nof data availability and encourage future work on modeling the human opinion\ndistribution for language reasoning. Our code and data are publicly available\nat https://github.com/easonnie/ChaosNLI\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 01:25:19 GMT"},{"version":"v2","created":"Thu, 7 Apr 2022 01:02:25 GMT"}],"update_date":"2022-04-08"}
{"id":"2104.08677","submitter":"Mehdi Rezagholizadeh","authors":"Krtin Kumar, Peyman Passban, Mehdi Rezagholizadeh, Yiu Sing Lau, Qun\n  Liu","title":"From Fully Trained to Fully Random Embeddings: Improving Neural Machine\n  Translation with Compact Word Embedding Tables","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Embedding matrices are key components in neural natural language processing\n(NLP) models that are responsible to provide numerical representations of input\ntokens.\\footnote{In this paper words and subwords are referred to as\n\\textit{tokens} and the term \\textit{embedding} only refers to embeddings of\ninputs.} In this paper, we analyze the impact and utility of such matrices in\nthe context of neural machine translation (NMT). We show that detracting\nsyntactic and semantic information from word embeddings and running NMT systems\nwith random embeddings is not as damaging as it initially sounds. We also show\nhow incorporating only a limited amount of task-specific knowledge from\nfully-trained embeddings can boost the performance NMT systems. Our findings\ndemonstrate that in exchange for negligible deterioration in performance, any\nNMT model can be run with partially random embeddings. Working with such\nstructures means a minimal memory requirement as there is no longer need to\nstore large embedding tables, which is a significant gain in industrial and\non-device settings. We evaluated our embeddings in translating {English} into\n{German} and {French} and achieved a $5.3$x compression rate. Despite having a\nconsiderably smaller architecture, our models in some cases are even able to\noutperform state-of-the-art baselines.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 01:57:38 GMT"},{"version":"v2","created":"Mon, 18 Apr 2022 04:06:31 GMT"}],"update_date":"2022-04-19"}
{"id":"2104.08678","submitter":"Max Bartolo","authors":"Max Bartolo, Tristan Thrush, Robin Jia, Sebastian Riedel, Pontus\n  Stenetorp, Douwe Kiela","title":"Improving Question Answering Model Robustness with Synthetic Adversarial\n  Data Generation","comments":"EMNLP 2021","journal-ref":"Proceedings of the 2021 Conference on Empirical Methods in Natural\n  Language Processing, p.8830-8848. Association for Computational Linguistics","doi":"10.18653/v1/2021.emnlp-main.696","report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Despite recent progress, state-of-the-art question answering models remain\nvulnerable to a variety of adversarial attacks. While dynamic adversarial data\ncollection, in which a human annotator tries to write examples that fool a\nmodel-in-the-loop, can improve model robustness, this process is expensive\nwhich limits the scale of the collected data. In this work, we are the first to\nuse synthetic adversarial data generation to make question answering models\nmore robust to human adversaries. We develop a data generation pipeline that\nselects source passages, identifies candidate answers, generates questions,\nthen finally filters or re-labels them to improve quality. Using this approach,\nwe amplify a smaller human-written adversarial dataset to a much larger set of\nsynthetic question-answer pairs. By incorporating our synthetic data, we\nimprove the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve\nmodel generalisation on nine of the twelve MRQA datasets. We further conduct a\nnovel human-in-the-loop evaluation to show that our models are considerably\nmore robust to new human-written adversarial examples: crowdworkers can fool\nour model only 8.8% of the time on average, compared to 17.6% for a model\ntrained without synthetic data.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 02:00:06 GMT"},{"version":"v2","created":"Thu, 16 Sep 2021 14:46:03 GMT"},{"version":"v3","created":"Tue, 15 Mar 2022 17:47:56 GMT"}],"update_date":"2022-03-16"}
{"id":"2104.08679","submitter":"Shahab Raji","authors":"Shahab Raji, Gerard de Melo","title":"Guilt by Association: Emotion Intensities in Lexical Representations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  What do word vector representations reveal about the emotions associated with\nwords? In this study, we consider the task of estimating word-level emotion\nintensity scores for specific emotions, exploring unsupervised, supervised, and\nfinally a self-supervised method of extracting emotional associations from word\nvector representations. Overall, we find that word vectors carry substantial\npotential for inducing fine-grained emotion intensity scores, showing a far\nhigher correlation with human ground truth ratings than achieved by\nstate-of-the-art emotion lexicons.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 02:03:52 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08680","submitter":"Sai Abitha Srinivas","authors":"Sai Abitha Srinivas (1 and 2), Stephen F Cauley (3 and 4), Jason P\n  Stockmann (3 and 4), Charlotte R Sappo (1 and 2), Christopher E Vaughn (1 and\n  2), Lawrence L Wald (3,4 and 5), William A Grissom (1,2,6 and 7) and Clarissa\n  Z Cooley (3 and 4) ((1) Vanderbilt University Institute of imaging science,\n  Nashville, TN, United States (2) Department of Biomedical Engineering,\n  Vanderbilt University, Nashville, TN, United States (3) Harvard Medical\n  School, Boston, MA, United States (4) Dept. of Radiology, Massachusetts\n  General Hospital, Athinoula A Martinos Center for Biomedical Imaging, Boston,\n  MA, United States (5) Harvard-MIT Division of Health Sciences and Technology,\n  Cambridge, MA, United States (6) Department of Electrical Engineering,\n  Vanderbilt University, Nashville, TN, United States (7) Department of\n  Radiology, Vanderbilt University, Nashville, TN, United States)","title":"External Dynamic InTerference Estimation and Removal (EDITER) for low\n  field MRI","comments":"Submitted to Magnetic Resonance in Medicine","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Purpose: Point-of-care MRI requires operation outside of a faraday shielded\nroom normally used to block image-degrading electromagnetic Interference (EMI).\nTo address this, we introduce the EDITER method, an external sensor based\ndynamic EMI estimation and removal method to retrospectively remove\ntime-varying external interference sources.\n  Theory and Methods: The method acquires data from multiple EMI detectors\n(tuned receive coils and electrodes placed on the body) simultaneous with the\nprimary MR coil during image data acquisition. We dynamically calculate impulse\nresponse functions that map the data from the detectors to the artifacts in the\nkspace data, then remove the transformed detected EMI from the MR data.\nPerformance of the EDITER algorithm was assessed in phantom and in vivo imaging\nexperiments in an 80mT portable brain MRI in a controlled EMI environment and\nwith an open 47.5mT MRI scanner in an uncontrolled EMI setting.\n  Results: In the controlled setting, the effectiveness of the EDITER technique\nwas demonstrated for specific types of introduced EMI sources with up to a 97%\nreduction of structured EMI and up to 76% reduction of broadband EMI. In the\nuncontrolled EMI experiments, we demonstrate EMI reductions of 37% with a\nsingle pickup coil and 89% with a single electrode and up to 99% with both.\n  Conclusion: The EDITER technique is a flexible and robust method to improve\nimage quality in portable MRI systems with minimal passive shielding. This\ncould reduce the reliance of MRI on shielded rooms and allow for truly portable\nMRI with specialized compact POC scanners\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 02:04:20 GMT"},{"version":"v2","created":"Thu, 29 Apr 2021 06:36:11 GMT"}],"update_date":"2021-04-30"}
{"id":"2104.08681","submitter":"Shaon Ghosh","authors":"Shaon Ghosh, Xiaoshu Liu, Jolien Creighton, Wolfgang Kastaun, Geraint\n  Pratten, Ignacio Magana Hernandez","title":"Rapid model comparison of equations of state from gravitational wave\n  observation of binary neutron star coalescences","comments":"Dataset produced in this study\n  https://zenodo.org/record/4679013#.YHuUnJNKhsA","journal-ref":"Phys. Rev. D 104, 083003 (2021)","doi":"10.1103/PhysRevD.104.083003","report-no":null,"categories":"gr-qc astro-ph.HE","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  The discovery of the coalescence of binary neutron star GW170817 was a\nwatershed moment in the field of gravitational wave astronomy. Among the rich\nvariety of information that we were able to uncover from this discovery was the\nfirst non-electromagnetic measurement of the neutron star radius, and the cold\nnuclear equation of state. It also led to a large equation of state\nmodel-selection study from gravitational-wave data. In those studies Bayesian\nnested sampling runs were conducted for each candidate equation of state model\nto compute their evidence in the gravitational-wave data. Such studies, though\ninvaluable, are computationally expensive and require repeated, redundant,\ncomputation for any new models. We present a novel technique to conduct\nmodel-selection of equation of state in an extremely rapid fashion (~minutes)\non any arbitrary model. We test this technique against the results of a\nnested-sampling model-selection technique published earlier by the LIGO/Virgo\ncollaboration, and show that the results are in good agreement with a median\nfractional error in Bayes factor of about 10%, where we assume that the true\nBayes factor is calculated in the aforementioned nested sampling runs. We found\nthat the highest fractional error occurs for equation of state models that have\nvery little support in the posterior distribution, thus resulting in large\nstatistical uncertainty. We then used this method to combine multiple binary\nneutron star mergers to compute a joint-Bayes factor between equation of state\nmodels. This is achieved by stacking the evidence of the individual events and\ncomputing the Bayes factor from these stacked evidences for each pairs of\nequation of state.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 02:20:19 GMT"},{"version":"v2","created":"Sat, 2 Oct 2021 15:33:15 GMT"}],"update_date":"2021-10-06"}
{"id":"2104.08682","submitter":"Dongkuan Xu","authors":"Dongkuan Xu, Ian E.H. Yen, Jinxi Zhao, Zhibin Xiao","title":"Rethinking Network Pruning -- under the Pre-train and Fine-tune Paradigm","comments":"7 pages, 6 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Transformer-based pre-trained language models have significantly improved the\nperformance of various natural language processing (NLP) tasks in the recent\nyears. While effective and prevalent, these models are usually prohibitively\nlarge for resource-limited deployment scenarios. A thread of research has thus\nbeen working on applying network pruning techniques under the\npretrain-then-finetune paradigm widely adopted in NLP. However, the existing\npruning results on benchmark transformers, such as BERT, are not as remarkable\nas the pruning results in the literature of convolutional neural networks\n(CNNs). In particular, common wisdom in pruning CNN states that sparse pruning\ntechnique compresses a model more than that obtained by reducing number of\nchannels and layers (Elsen et al., 2020; Zhu and Gupta, 2017), while existing\nworks on sparse pruning of BERT yields inferior results than its small-dense\ncounterparts such as TinyBERT (Jiao et al., 2020). In this work, we aim to fill\nthis gap by studying how knowledge are transferred and lost during the\npre-train, fine-tune, and pruning process, and proposing a knowledge-aware\nsparse pruning process that achieves significantly superior results than\nexisting literature. We show for the first time that sparse pruning compresses\na BERT model significantly more than reducing its number of channels and\nlayers. Experiments on multiple data sets of GLUE benchmark show that our\nmethod outperforms the leading competitors with a 20-times weight/FLOPs\ncompression and neglectable loss in prediction accuracy.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 02:20:37 GMT"},{"version":"v2","created":"Sun, 16 Jan 2022 18:28:21 GMT"}],"update_date":"2022-01-19"}
{"id":"2104.08683","submitter":"Chenxu Luo","authors":"Chenxu Luo, Xiaodong Yang, Alan Yuille","title":"Self-Supervised Pillar Motion Learning for Autonomous Driving","comments":"cvpr2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Autonomous driving can benefit from motion behavior comprehension when\ninteracting with diverse traffic participants in highly dynamic environments.\nRecently, there has been a growing interest in estimating class-agnostic motion\ndirectly from point clouds. Current motion estimation methods usually require\nvast amount of annotated training data from self-driving scenes. However,\nmanually labeling point clouds is notoriously difficult, error-prone and\ntime-consuming. In this paper, we seek to answer the research question of\nwhether the abundant unlabeled data collections can be utilized for accurate\nand efficient motion learning. To this end, we propose a learning framework\nthat leverages free supervisory signals from point clouds and paired camera\nimages to estimate motion purely via self-supervision. Our model involves a\npoint cloud based structural consistency augmented with probabilistic motion\nmasking as well as a cross-sensor motion regularization to realize the desired\nself-supervision. Experiments reveal that our approach performs competitively\nto supervised methods, and achieves the state-of-the-art result when combining\nour self-supervised model with supervised fine-tuning.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 02:32:08 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08684","submitter":"Daniel Zucker","authors":"Daniel B. Zucker, Jeffrey D. Simpson, Sarah L. Martell, Geraint F.\n  Lewis, Andrew R. Casey, Yuan-Sen Ting, Jonathan Horner, Thomas Nordlander,\n  Rosemary F. G. Wyse, Tomaz Zwitter, Joss Bland-Hawthorn, Sven Buder, Martin\n  Asplund, Gayandhi M. De Silva, Valentina D'Orazi, Ken C. Freeman, Michael R.\n  Hayden, Janez Kos, Jane Lin, Karin Lind, Katharine J. Schlesinger, Sanjib\n  Sharma and Dennis Stello","title":"The GALAH Survey: No chemical evidence of an extragalactic origin for\n  the Nyx stream","comments":"Accepted for publication by ApJ Letters","journal-ref":null,"doi":"10.3847/2041-8213/abf7cd","report-no":null,"categories":"astro-ph.GA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The results from the ESA Gaia astrometric mission and deep photometric\nsurveys have revolutionized our knowledge of the Milky Way. There are many\nongoing efforts to search these data for stellar substructure to find evidence\nof individual accretion events that built up the Milky Way and its halo. One of\nthese newly identified features, called Nyx, was announced as an accreted\nstellar stream traveling in the plane of the disk. Using a combination of\nelemental abundances and stellar parameters from the GALAH and APOGEE surveys,\nwe find that the abundances of the highest likelihood Nyx members are entirely\nconsistent with membership of the thick disk, and inconsistent with a dwarf\ngalaxy origin. We conclude that the postulated Nyx stream is most probably a\nhigh-velocity component of the Milky Way's thick disk. With the growing\navailability of large data sets including kinematics, stellar parameters, and\ndetailed abundances, the probability of detecting chance associations\nincreases, and hence new searches for substructure require confirmation across\nas many data dimensions as possible.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 02:34:49 GMT"}],"update_date":"2021-05-19"}
{"id":"2104.08685","submitter":"Jacob Louis Hoover","authors":"Jacob Louis Hoover, Alessandro Sordoni, Wenyu Du, Timothy J. O'Donnell","title":"Linguistic Dependencies and Statistical Dependence","comments":"EMNLP2021 camera-ready version. 9 pages, plus references and\n  appendices","journal-ref":"Proceedings EMNLP (2021), 2941--2963","doi":"10.18653/v1/2021.emnlp-main.234","report-no":"2021.emnlp-main.234","categories":"cs.CL cs.IT math.IT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Are pairs of words that tend to occur together also likely to stand in a\nlinguistic dependency? This empirical question is motivated by a long history\nof literature in cognitive science, psycholinguistics, and NLP. In this work we\ncontribute an extensive analysis of the relationship between linguistic\ndependencies and statistical dependence between words. Improving on previous\nwork, we introduce the use of large pretrained language models to compute\ncontextualized estimates of the pointwise mutual information between words\n(CPMI). For multiple models and languages, we extract dependency trees which\nmaximize CPMI, and compare to gold standard linguistic dependencies. Overall,\nwe find that CPMI dependencies achieve an unlabelled undirected attachment\nscore of at most $\\approx 0.5$. While far above chance, and consistently above\na non-contextualized PMI baseline, this score is generally comparable to a\nsimple baseline formed by connecting adjacent words. We analyze which kinds of\nlinguistic dependencies are best captured in CPMI dependencies, and also find\nmarked differences between the estimates of the large pretrained language\nmodels, illustrating how their different training schemes affect the type of\ndependencies they capture.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 02:43:37 GMT"},{"version":"v2","created":"Fri, 10 Sep 2021 14:32:15 GMT"},{"version":"v3","created":"Fri, 29 Apr 2022 16:00:27 GMT"}],"update_date":"2022-05-02"}
{"id":"2104.08686","submitter":"Jaehyuk Choi","authors":"Jaehyuk Choi, Minsuk Kwak, Chyng Wen Tee, Yumeng Wang","title":"A Black-Scholes user's guide to the Bachelier model","comments":null,"journal-ref":"Journal of Futures Markets, 42(5):959-980, 2022","doi":"10.1002/fut.22315","report-no":null,"categories":"q-fin.MF q-fin.PR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  To cope with the negative oil futures price caused by the COVID-19 recession,\nglobal commodity futures exchanges temporarily switched the option model from\nBlack--Scholes to Bachelier in 2020. This study reviews the literature on\nBachelier's pioneering option pricing model and summarizes the practical\nresults on volatility conversion, risk management, stochastic volatility, and\nbarrier options pricing to facilitate the model transition. In particular,\nusing the displaced Black-Scholes model as a model family with the\nBlack-Scholes and Bachelier models as special cases, we not only connect the\ntwo models but also present a continuous spectrum of model choices.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 02:51:25 GMT"},{"version":"v2","created":"Sun, 6 Feb 2022 14:48:35 GMT"}],"update_date":"2022-04-12"}
{"id":"2104.08687","submitter":"Dan M. Kluger","authors":"Dan M. Kluger and Art B. Owen","title":"A central limit theorem for the Benjamini-Hochberg false discovery\n  proportion under a factor model","comments":"Changes in version 4: Added two paragraphs of text at the end of the\n  discussion section, turned the appendix into a supplement, and throughout the\n  text made a few minor changes to improve clarity or fix typos","journal-ref":null,"doi":null,"report-no":null,"categories":"math.ST stat.ME stat.TH","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The Benjamini-Hochberg (BH) procedure remains widely popular despite having\nlimited theoretical guarantees in the commonly encountered scenario of\ncorrelated test statistics. Of particular concern is the possibility that the\nmethod could exhibit bursty behavior, meaning that it might typically yield no\nfalse discoveries while occasionally yielding both a large number of false\ndiscoveries and a false discovery proportion (FDP) that far exceeds its own\nwell controlled mean. In this paper, we investigate which test statistic\ncorrelation structures lead to bursty behavior and which ones lead to well\ncontrolled FDPs. To this end, we develop a central limit theorem for the FDP in\na multiple testing setup where the test statistic correlations can be either\nshort-range or long-range as well as either weak or strong. The theorem and our\nsimulations from a data-driven factor model suggest that the BH procedure\nexhibits severe burstiness when the test statistics have many strong,\nlong-range correlations, but does not otherwise.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 02:52:35 GMT"},{"version":"v2","created":"Mon, 21 Jun 2021 00:56:54 GMT"},{"version":"v3","created":"Sun, 10 Apr 2022 02:00:52 GMT"},{"version":"v4","created":"Mon, 24 Apr 2023 15:29:28 GMT"}],"update_date":"2023-04-25"}
{"id":"2104.08688","submitter":"Shixiang Zhu","authors":"Josh Kacher, Yao Xie, Sven P. Voigt, Shixiang Zhu, Henry Yuchi, Jordan\n  Key, Surya R. Kalidindi","title":"Signal Processing Challenges and Examples for {\\it in-situ} Transmission\n  Electron Microscopy","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Transmission Electron Microscopy (TEM) is a powerful tool for imaging\nmaterial structure and characterizing material chemistry. Recent advances in\ndata collection technology for TEM have enabled high-volume and high-resolution\ndata collection at a microsecond frame rate. Taking advantage of these advances\nin data collection rates requires the development and application of data\nprocessing tools, including image analysis, feature extraction, and streaming\ndata processing techniques. In this paper, we highlight a few areas in\nmaterials science that have benefited from combining signal processing and\nstatistical analysis with data collection capabilities in TEM and present a\nfuture outlook on opportunities of integrating signal processing with automated\nTEM data analysis.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 02:56:04 GMT"},{"version":"v2","created":"Fri, 20 Aug 2021 19:46:35 GMT"}],"update_date":"2021-08-24"}
{"id":"2104.08689","submitter":"Kai Li","authors":"Kai Li, Curtis Wigington, Chris Tensmeyer, Vlad I. Morariu, Handong\n  Zhao, Varun Manjunatha, Nikolaos Barmpalios, Yun Fu","title":"RPCL: A Framework for Improving Cross-Domain Detection with Auxiliary\n  Tasks","comments":"10 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Cross-Domain Detection (XDD) aims to train an object detector using labeled\nimage from a source domain but have good performance in the target domain with\nonly unlabeled images. Existing approaches achieve this either by aligning the\nfeature maps or the region proposals from the two domains, or by transferring\nthe style of source images to that of target image. Contrasted with prior work,\nthis paper provides a complementary solution to align domains by learning the\nsame auxiliary tasks in both domains simultaneously. These auxiliary tasks push\nimage from both domains towards shared spaces, which bridges the domain gap.\nSpecifically, this paper proposes Rotation Prediction and Consistency Learning\n(PRCL), a framework complementing existing XDD methods for domain alignment by\nleveraging the two auxiliary tasks. The first one encourages the model to\nextract region proposals from foreground regions by rotating an image and\npredicting the rotation angle from the extracted region proposals. The second\ntask encourages the model to be robust to changes in the image space by\noptimizing the model to make consistent class predictions for region proposals\nregardless of image perturbations. Experiments show the detection performance\ncan be consistently and significantly enhanced by applying the two proposed\ntasks to existing XDD methods.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 02:56:19 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08690","submitter":"Yue Gao","authors":"Yue Gao, Ilia Shumailov, Kassem Fawaz","title":"Rethinking Image-Scaling Attacks: The Interplay Between Vulnerabilities\n  in Machine Learning Systems","comments":"Accepted by Proceedings of the 39th International Conference on\n  Machine Learning","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CR","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  As real-world images come in varying sizes, the machine learning model is\npart of a larger system that includes an upstream image scaling algorithm. In\nthis paper, we investigate the interplay between vulnerabilities of the image\nscaling procedure and machine learning models in the decision-based black-box\nsetting. We propose a novel sampling strategy to make a black-box attack\nexploit vulnerabilities in scaling algorithms, scaling defenses, and the final\nmachine learning model in an end-to-end manner. Based on this scaling-aware\nattack, we reveal that most existing scaling defenses are ineffective under\nthreat from downstream models. Moreover, we empirically observe that standard\nblack-box attacks can significantly improve their performance by exploiting the\nvulnerable scaling procedure. We further demonstrate this problem on a\ncommercial Image Analysis API with decision-based black-box attacks.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 03:19:15 GMT"},{"version":"v2","created":"Tue, 19 Oct 2021 19:59:38 GMT"},{"version":"v3","created":"Sun, 19 Jun 2022 19:02:43 GMT"}],"update_date":"2022-06-22"}
{"id":"2104.08691","submitter":"Brian Lester","authors":"Brian Lester, Rami Al-Rfou, Noah Constant","title":"The Power of Scale for Parameter-Efficient Prompt Tuning","comments":"Accepted to EMNLP 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this work, we explore \"prompt tuning\", a simple yet effective mechanism\nfor learning \"soft prompts\" to condition frozen language models to perform\nspecific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft\nprompts are learned through backpropagation and can be tuned to incorporate\nsignal from any number of labeled examples. Our end-to-end learned approach\noutperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably,\nthrough ablations on model size using T5, we show that prompt tuning becomes\nmore competitive with scale: as models exceed billions of parameters, our\nmethod \"closes the gap\" and matches the strong performance of model tuning\n(where all model weights are tuned). This finding is especially relevant in\nthat large models are costly to share and serve, and the ability to reuse one\nfrozen model for multiple downstream tasks can ease this burden. Our method can\nbe seen as a simplification of the recently proposed \"prefix tuning\" of Li and\nLiang (2021), and we provide a comparison to this and other similar approaches.\nFinally, we show that conditioning a frozen model with soft prompts confers\nbenefits in robustness to domain transfer, as compared to full model tuning.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 03:19:26 GMT"},{"version":"v2","created":"Thu, 2 Sep 2021 17:34:41 GMT"}],"update_date":"2021-09-03"}
{"id":"2104.08692","submitter":"Li Dong","authors":"Zewen Chi, Li Dong, Shuming Ma, Shaohan Huang Xian-Ling Mao, Heyan\n  Huang, Furu Wei","title":"MT6: Multilingual Pretrained Text-to-Text Transformer with Translation\n  Pairs","comments":"EMNLP 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Multilingual T5 (mT5) pretrains a sequence-to-sequence model on massive\nmonolingual texts, which has shown promising results on many cross-lingual\ntasks. In this paper, we improve multilingual text-to-text transfer Transformer\nwith translation pairs (mT6). Specifically, we explore three cross-lingual\ntext-to-text pre-training tasks, namely, machine translation, translation pair\nspan corruption, and translation span corruption. In addition, we propose a\npartially non-autoregressive objective for text-to-text pre-training. We\nevaluate the methods on eight multilingual benchmark datasets, including\nsentence classification, named entity recognition, question answering, and\nabstractive summarization. Experimental results show that the proposed mT6\nimproves cross-lingual transferability over mT5.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 03:24:07 GMT"},{"version":"v2","created":"Mon, 13 Sep 2021 12:13:46 GMT"}],"update_date":"2021-09-14"}
{"id":"2104.08693","submitter":"A. V. Syromyatnikov","authors":"A. V. Syromyatnikov, F. D. Timkovskii","title":"Quantum transitions from superfluid to insulating phases in disordered\n  Bose systems","comments":"12 pages, 2 figures","journal-ref":"Journal of Magnetism and Magnetic Materials 570, 170540 (2023)","doi":"10.1016/j.jmmm.2023.170540","report-no":null,"categories":"cond-mat.str-el","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  By the example of Heisenberg $d$-dimensional disordered non-frustrated\nantiferromagnets, we discuss quantum transitions at $d\\ge2$ from magnetically\nordered (superfluid) to various disorder-induced insulating phases (Bose-glass,\nMott-glass, etc.) in Bose systems with quenched disorder. We perform a scaling\nconsideration as well as a discussion based on the hydrodynamic description of\nlong-wavelength excitations and on the assumption that the ordered part of the\nsystem shows fractal properties near the transition point. We propose that the\nscaling ansatz for the singular part of the free energy suggested before for\nthe transition to the Bose-glass phase is applicable also for other transitions\nif the quenched disorder does not produce a local imbalance in sublattices\nmagnetizations. We show using the scaling consideration that $\\eta=2-z$ and\n$\\beta=\\nu d/2$, where $\\eta$, $\\beta$, and $\\nu$, are critical exponents of\nthe correlation function, the order parameter, and the correlation length,\nrespectively, and $z$ is the dynamical critical index. These relations were\nmissed in previous analytical discussions of Bose-glass and Mott-glass phases.\nThey signify, in particular, that $z=d/2$ for the transition to the Mott-glass\nphase and that the density of states of localized excitations shows a\nsuperuniversal (i.e., independent of $d$) behavior near the transitions. Being\nderived solely from the scaling analysis, the above relations for $\\eta$ and\n$\\beta$ are valid also for the transition to the random-singlet phase.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 03:27:52 GMT"},{"version":"v2","created":"Thu, 17 Jun 2021 08:44:39 GMT"},{"version":"v3","created":"Thu, 28 Jul 2022 08:08:52 GMT"},{"version":"v4","created":"Fri, 24 Feb 2023 07:07:47 GMT"}],"update_date":"2023-02-27"}
{"id":"2104.08694","submitter":"Md. Mohi Uddin","authors":"N. Jahan, M.M. Uddin, M.N.I. Khan, F.-U.-Z. Chowdhury, M. R. Hasan, H.\n  N. Das and M.M. Hossain","title":"Impact of particle size on the magnetic properties of highly crystalline\n  Yb3+ substituted Ni-Zn nanoferrites","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mtrl-sci","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Yb-substituted Ni-Zn ferrites have been synthesized using sol-gel auto\ncombustion method. The structural characterization of the compositions has been\nperformed by X-ray diffraction analysis, field emission scanning electron\nmicroscopy (FESEM), quantum design physical properties measurement system\n(PPMS). That ensured the formation of single phase cubic spinel structure.\nCrystallite and average grain size are calculated and found to decrease with\nincreasing Yb3+ contents. Saturation magnetization and Bohr magnetic moment\ndecrease while the coercivity increases with the increase in Yb3+ contents\nsuccessfully explained by the Neels collinear two sub-lattice model and\ncritical size effect, respectively. Critical particle size has been estimated\nat 6.4 nm, the transition point between single domain regime (below the\ncritical size) and multi-domain regime (beyond the critical size). Curie\ntemperature reduces due to the weakening of A-O-B super exchange interaction\nand redistribution of cations, confirmed by the M-T graph. The compositions\nretain ferromagnetic ordered structured below Curie temperature and above Curie\ntemperature, it becomes paramagnetic, making them plausible candidates for high\ntemperature magnetic device applications. The relative quality factor peak is\nobtained at a very high frequency, indicating the compositions could also be\napplicable for high frequency magnetic device applications.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 03:30:49 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08695","submitter":"Glen Chou","authors":"Glen Chou, Necmiye Ozay, and Dmitry Berenson","title":"Model Error Propagation via Learned Contraction Metrics for Safe\n  Feedback Motion Planning of Unknown Systems","comments":"Extended paper; abridged version presented at the 60th IEEE\n  Conference on Decision and Control (CDC 2021)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.LG cs.SY eess.SY","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present a method for contraction-based feedback motion planning of locally\nincrementally exponentially stabilizable systems with unknown dynamics that\nprovides probabilistic safety and reachability guarantees. Given a dynamics\ndataset, our method learns a deep control-affine approximation of the dynamics.\nTo find a trusted domain where this model can be used for planning, we obtain\nan estimate of the Lipschitz constant of the model error, which is valid with a\ngiven probability, in a region around the training data, providing a local,\nspatially-varying model error bound. We derive a trajectory tracking error\nbound for a contraction-based controller that is subjected to this model error,\nand then learn a controller that optimizes this tracking bound. With a given\nprobability, we verify the correctness of the controller and tracking error\nbound in the trusted domain. We then use the trajectory error bound together\nwith the trusted domain to guide a sampling-based planner to return\ntrajectories that can be robustly tracked in execution. We show results on a 4D\ncar, a 6D quadrotor, and a 22D deformable object manipulation task, showing our\nmethod plans safely with learned models of high-dimensional underactuated\nsystems, while baselines that plan without considering the tracking error bound\nor the trusted domain can fail to stabilize the system and become unsafe.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 03:34:00 GMT"},{"version":"v2","created":"Tue, 1 Mar 2022 16:43:48 GMT"}],"update_date":"2022-03-02"}
{"id":"2104.08696","submitter":"Li Dong","authors":"Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, Furu Wei","title":"Knowledge Neurons in Pretrained Transformers","comments":"ACL 2022","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Large-scale pretrained language models are surprisingly good at recalling\nfactual knowledge presented in the training corpus. In this paper, we present\npreliminary studies on how factual knowledge is stored in pretrained\nTransformers by introducing the concept of knowledge neurons. Specifically, we\nexamine the fill-in-the-blank cloze task for BERT. Given a relational fact, we\npropose a knowledge attribution method to identify the neurons that express the\nfact. We find that the activation of such knowledge neurons is positively\ncorrelated to the expression of their corresponding facts. In our case studies,\nwe attempt to leverage knowledge neurons to edit (such as update, and erase)\nspecific factual knowledge without fine-tuning. Our results shed light on\nunderstanding the storage of knowledge within pretrained Transformers. The code\nis available at https://github.com/Hunter-DDM/knowledge-neurons.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 03:38:26 GMT"},{"version":"v2","created":"Thu, 10 Mar 2022 02:28:59 GMT"}],"update_date":"2022-03-11"}
{"id":"2104.08697","submitter":"Dongchen Lu","authors":"Dongchen Lu","title":"OSKDet: Towards Orientation-sensitive Keypoint Localization for Rotated\n  Object Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Rotated object detection is a challenging issue of computer vision field.\nLoss of spatial information and confusion of parametric order have been the\nbottleneck for rotated detection accuracy. In this paper, we propose an\norientation-sensitive keypoint based rotated detector OSKDet. We adopt a set of\nkeypoints to characterize the target and predict the keypoint heatmap on ROI to\nform a rotated target. By proposing the orientation-sensitive heatmap, OSKDet\ncould learn the shape and direction of rotated target implicitly and has\nstronger modeling capabilities for target representation, which improves the\nlocalization accuracy and acquires high quality detection results. To extract\nhighly effective features at border areas, we design a rotation-aware\ndeformable convolution module. Furthermore, we explore a new keypoint reorder\nalgorithm and feature fusion module based on the angle distribution to\neliminate the confusion of keypoint order. Experimental results on several\npublic benchmarks show the state-of-the-art performance of OSKDet.\nSpecifically, we achieve an AP of 77.81% on DOTA, 89.91% on HRSC2016, and\n97.18% on UCAS-AOD, respectively.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 03:40:52 GMT"},{"version":"v2","created":"Mon, 28 Jun 2021 14:57:11 GMT"}],"update_date":"2021-06-29"}
{"id":"2104.08698","submitter":"Pu-Chin Chen","authors":"Pu-Chin Chen, Henry Tsai, Srinadh Bhojanapalli, Hyung Won Chung,\n  Yin-Wen Chang, Chun-Sung Ferng","title":"A Simple and Effective Positional Encoding for Transformers","comments":"Accepted by EMNLP","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Transformer models are permutation equivariant. To supply the order and type\ninformation of the input tokens, position and segment embeddings are usually\nadded to the input. Recent works proposed variations of positional encodings\nwith relative position encodings achieving better performance. Our analysis\nshows that the gain actually comes from moving positional information to\nattention layer from the input. Motivated by this, we introduce Decoupled\nPositional Attention for Transformers (DIET), a simple yet effective mechanism\nto encode position and segment information into the Transformer models. The\nproposed method has faster training and inference time, while achieving\ncompetitive performance on GLUE, XTREME and WMT benchmarks. We further\ngeneralize our method to long-range transformers and show performance gain.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 03:44:57 GMT"},{"version":"v2","created":"Wed, 3 Nov 2021 04:36:53 GMT"}],"update_date":"2021-11-04"}
{"id":"2104.08699","submitter":"Mao Ye","authors":"Mao Ye","title":"FOX: Hardware-Assisted File Auditing for Direct Access NVM-Hosted\n  Filesystems","comments":"13 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  With emerging non-volatile memories entering the mainstream market, several\noperating systems start to incorporate new changes and optimizations. One major\nOS support is the direct-access for files, which enables efficient access for\nfiles hosted in byte-addressable NVM systems. With DAX-enabled filesystems,\nfiles can be accessed directly similar to memory with typical load/store\noperations. Despite its efficiency, the frequently used system call of direct\naccess is troublesome for system auditing. File system auditing is mandatory\nand widely used because auditing logs can help detect anomalies, suspicious\nfile accesses, or be used as an evidence in digital forensics. However, the\nfrequent and long-time usage of direct access call blinds the operating system\nor file system from tracking process operations to shared files after the\ninitial page faults. This might results in imprecise casualty analysis and\nleads to false conclusion for attack detection. To remedy the tension between\nenabling fine-grained file system auditing and leveraging the performance of\nNVM-hosted file systems, we propose a novel hardware-assisted auditing scheme,\nFOX. FOX enables file system auditing through lightweight hardware-software\nchanges which can monitor every read or write event for mapped files on NVM.\nAdditionally, we propose the optimized schemes, that enable auditing\nflexibility for selected files/memory range. By prototyping FOX on a full\nsystem simulator, Gem5, we observe a relatively small reduced throughput and an\nacceptable extra writes compared to our baseline. Compared to other\ninstrumentation-based software schemes, our scheme is low-overhead and secure.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 03:47:40 GMT"},{"version":"v2","created":"Wed, 21 Apr 2021 02:10:23 GMT"}],"update_date":"2021-04-22"}
{"id":"2104.08700","submitter":"Yuxin Zhang","authors":"Yuxin Zhang, Mingbao Lin, Yunshan Zhong, Fei Chao, Rongrong Ji","title":"Lottery Jackpots Exist in Pre-trained Models","comments":"14 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Network pruning is an effective approach to reduce network complexity with\nacceptable performance compromise. Existing studies achieve the sparsity of\nneural networks via time-consuming weight training or complex searching on\nnetworks with expanded width, which greatly limits the applications of network\npruning. In this paper, we show that high-performing and sparse sub-networks\nwithout the involvement of weight training, termed \"lottery jackpots\", exist in\npre-trained models with unexpanded width. Furthermore, we improve the\nefficiency for searching lottery jackpots from two perspectives. Firstly, we\nobserve that the sparse masks derived from many existing pruning criteria have\na high overlap with the searched mask of our lottery jackpot, among which, the\nmagnitude-based pruning results in the most similar mask with ours.\nConsequently, our searched lottery jackpot removes 90% weights in ResNet-50,\nwhile it easily obtains more than 70% top-1 accuracy using only 5 searching\nepochs on ImageNet. In compliance with this insight, we initialize our sparse\nmask using the magnitude-based pruning, resulting in at least 3x cost reduction\non the lottery jackpot searching while achieving comparable or even better\nperformance. Secondly, we conduct an in-depth analysis of the searching process\nfor lottery jackpots. Our theoretical result suggests that the decrease in\ntraining loss during weight searching can be disturbed by the dependency\nbetween weights in modern networks. To mitigate this, we propose a novel short\nrestriction method to restrict change of masks that may have potential negative\nimpacts on the training loss. Our code is available at\nhttps://github.com/zyxxmu/lottery-jackpots.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 03:50:28 GMT"},{"version":"v2","created":"Wed, 2 Jun 2021 06:21:53 GMT"},{"version":"v3","created":"Thu, 9 Sep 2021 13:14:28 GMT"},{"version":"v4","created":"Mon, 22 Nov 2021 03:05:24 GMT"},{"version":"v5","created":"Fri, 4 Feb 2022 14:10:00 GMT"},{"version":"v6","created":"Tue, 13 Dec 2022 03:08:41 GMT"}],"update_date":"2022-12-14"}
{"id":"2104.08701","submitter":"Brian Lester","authors":"Brian Lester, Sagnik Ray Choudhury, Rashmi Prasad, Srinivas Bangalore","title":"Intent Features for Rich Natural Language Understanding","comments":"Camera-ready for NAACL 2021 Industry Track","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Complex natural language understanding modules in dialog systems have a\nricher understanding of user utterances, and thus are critical in providing a\nbetter user experience. However, these models are often created from scratch,\nfor specific clients and use cases, and require the annotation of large\ndatasets. This encourages the sharing of annotated data across multiple\nclients. To facilitate this we introduce the idea of intent features: domain\nand topic agnostic properties of intents that can be learned from the syntactic\ncues only, and hence can be shared. We introduce a new neural network\narchitecture, the Global-Local model, that shows significant improvement over\nstrong baselines for identifying these features in a deployed, multi-intent\nnatural language understanding module, and, more generally, in a classification\nsetting where a part of an utterance has to be classified utilizing the whole\ncontext.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 03:57:02 GMT"},{"version":"v2","created":"Wed, 21 Apr 2021 16:08:55 GMT"}],"update_date":"2021-04-22"}
{"id":"2104.08702","submitter":"Mahdi S. Hosseini Dr.","authors":"Andre Fu and Mahdi S. Hosseini and Konstantinos N. Plataniotis","title":"Reconsidering CO2 emissions from Computer Vision","comments":"Accepted for publication in CVPR 2021 Workshop","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Climate change is a pressing issue that is currently affecting and will\naffect every part of our lives. It's becoming incredibly vital we, as a\nsociety, address the climate crisis as a universal effort, including those in\nthe Computer Vision (CV) community. In this work, we analyze the total cost of\nCO2 emissions by breaking it into (1) the architecture creation cost and (2)\nthe life-time evaluation cost. We show that over time, these costs are\nnon-negligible and are having a direct impact on our future. Importantly, we\nconduct an ethical analysis of how the CV-community is unintentionally\noverlooking its own ethical AI principles by emitting this level of CO2. To\naddress these concerns, we propose adding \"enforcement\" as a pillar of ethical\nAI and provide some recommendations for how architecture designers and broader\nCV community can curb the climate crisis.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:01:40 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08703","submitter":"Qingyu Gan","authors":"Qingyu Gan, Peng Wang, Houwen Wu, Haitang Yang","title":"Photon Spheres and Spherical Accretion Image of a Hairy Black Hole","comments":"v1: 13 pages, 6 figures; v2: 13 pages, 6 figures, references added","journal-ref":"Phys. Rev. D 104, 024003 (2021)","doi":"10.1103/PhysRevD.104.024003","report-no":null,"categories":"gr-qc","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we first consider null geodesics of a class of charged,\nspherical and asymptotically flat hairy black holes in an\nEinstein-Maxwell-scalar theory with a non-minimal coupling for the scalar and\nelectromagnetic fields. Remarkably, we show that there are two unstable\ncircular orbits for a photon in a certain parameter regime, corresponding to\ntwo unstable photon spheres of different sizes outside the event horizon. To\nillustrate the optical appearance of photon spheres, we then consider a simple\nspherical model of optically thin accretion on the hairy black hole, and obtain\nthe accretion image seen by a distant observer. In the single photon sphere\ncase, only one bright ring appears in the image, and is identified as the edge\nof the black hole shadow. Whereas in the case with two photon spheres, there\ncan be two concentric bright rings of different radii in the image, and the\nsmaller one serves as the boundary of the shadow, whose radius goes to zero at\nthe critical charge.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:02:16 GMT"},{"version":"v2","created":"Mon, 3 May 2021 14:38:52 GMT"}],"update_date":"2021-07-07"}
{"id":"2104.08704","submitter":"Tianyu Liu","authors":"Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu\n  Chen and Bill Dolan","title":"A Token-level Reference-free Hallucination Detection Benchmark for\n  Free-form Text Generation","comments":"Accepted by ACL2022 main conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Large pretrained generative models like GPT-3 often suffer from hallucinating\nnon-existent or incorrect content, which undermines their potential merits in\nreal applications. Existing work usually attempts to detect these\nhallucinations based on a corresponding oracle reference at a sentence or\ndocument level. However ground-truth references may not be readily available\nfor many free-form text generation applications, and sentence- or\ndocument-level detection may fail to provide the fine-grained signals that\nwould prevent fallacious content in real time. As a first step to addressing\nthese issues, we propose a novel token-level, reference-free hallucination\ndetection task and an associated annotated dataset named HaDes (HAllucination\nDEtection dataSet). To create this dataset, we first perturb a large number of\ntext segments extracted from English language Wikipedia, and then verify these\nwith crowd-sourced annotations. To mitigate label imbalance during annotation,\nwe utilize an iterative model-in-loop strategy. We conduct comprehensive data\nanalyses and create multiple baseline models.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:09:48 GMT"},{"version":"v2","created":"Sat, 2 Apr 2022 15:23:44 GMT"}],"update_date":"2022-04-05"}
{"id":"2104.08705","submitter":"Jonathan Keith","authors":"Jonathan M. Keith and Greg Markowsky","title":"A theory of integration for Ces\\`aro limits","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.CA math.PR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The Ces\\`aro limit - the asymptotic average of a sequence of real numbers -\nis an operator of fundamental importance in probability, statistics and\nanalysis. Surprisingly, spaces of sequences with Ces\\`aro limits have not\npreviously been studied. This paper introduces spaces of such sequences,\ndenoted $K_p(\\mathcal{A})$, with the Ces\\`aro limit acting as a kind of\nintegral. The space $\\mathcal{F}$ comprised of all binary sequences with a\nCes\\`aro limit is studied first, along with the associated functional $\\nu:\n\\mathcal{F} \\rightarrow [0,1]$ mapping each such sequence to its Ces\\`aro\nlimit. It is shown that $\\mathcal{F}$ can be factored to produce a monotone\nclass on which $\\nu$ induces a countably additive set function. The space\n$K_p(\\mathcal{A})$ is then defined, and a quotient denoted\n$\\mathcal{K}_p(\\mathcal{A})$ is shown to be isometrically isomorphic, under\ncertain conditions, to the function space\n$\\mathcal{L}_p(\\mathbb{N},\\mathcal{A},\\nu)$, where $\\mathcal{A}$ is a field of\nsets isomorphic to a subset of $\\mathcal{F}$, and $\\nu$ is a finitely additive\nmeasure induced by the functional mentioned above. The Ces\\`aro limit of an\nelement of $K_p(\\mathcal{A})$ is shown to be equal to its integral. The\ncomplete $\\mathcal{L}_p(\\mathbb{N},\\mathcal{A},\\nu)$ spaces (and by\nimplication, the $\\mathcal{K}_p(\\mathcal{A})$ spaces isomorphic to them) are\ncharacterised, and a sufficient condition for these spaces to be separable is\nidentified.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:21:44 GMT"},{"version":"v2","created":"Mon, 28 Jun 2021 07:44:38 GMT"},{"version":"v3","created":"Sun, 20 Feb 2022 11:37:27 GMT"},{"version":"v4","created":"Wed, 16 Mar 2022 05:25:06 GMT"}],"update_date":"2022-03-17"}
{"id":"2104.08706","submitter":"Ali Kazemi Jahromi","authors":"Ali K. Jahromi, Massimo L. Villinger, Ahmed El Halawany, Soroush\n  Shabahang, H. Esat Kondakci, Joshua D. Perlstein, and Ayman F. Abouraddy","title":"Broadband Omni-resonant Coherent Perfect Absorption in Graphene","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.optics","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Coherent perfect absorption (CPA) refers to interferometrically induced\ncomplete absorption of incident light by a partial absorber independently of\nits intrinsic absorption (which may be vanishingly small) or its thickness. CPA\nis typically realized in a resonant device, and thus cannot be achieved over a\nbroad continuous spectrum, which thwarts its applicability to photodetectors\nand solar cells, for example. Here, we demonstrate broadband omni-resonant CPA\nby placing a thin weak absorber in a planar cavity and pre-conditioning the\nincident optical field by introducing judicious angular dispersion. We make use\nof monolayer graphene embedded in silica as the absorber and boost its optical\nabsorption from ~1.6% to ~60% over a bandwidth of ~70 nm in the visible.\nCrucially, an analytical model demonstrates that placement of the graphene\nmonolayer at a peak in the cavity standing-wave field is not necessary to\nachieve CPA, contrary to conventional wisdom.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:23:44 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08707","submitter":"Sheng-Chieh Lin","authors":"Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin","title":"Contextualized Query Embeddings for Conversational Search","comments":"Published in EMNLP 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.IR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper describes a compact and effective model for low-latency passage\nretrieval in conversational search based on learned dense representations.\nPrior to our work, the state-of-the-art approach uses a multi-stage pipeline\ncomprising conversational query reformulation and information retrieval\nmodules. Despite its effectiveness, such a pipeline often includes multiple\nneural models that require long inference times. In addition, independently\noptimizing each module ignores dependencies among them. To address these\nshortcomings, we propose to integrate conversational query reformulation\ndirectly into a dense retrieval model. To aid in this goal, we create a dataset\nwith pseudo-relevance labels for conversational search to overcome the lack of\ntraining data and to explore different training strategies. We demonstrate that\nour model effectively rewrites conversational queries as dense representations\nin conversational search and open-domain question answering datasets. Finally,\nafter observing that our model learns to adjust the $L_2$ norm of query token\nembeddings, we leverage this property for hybrid retrieval and to support error\nanalysis.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:29:01 GMT"},{"version":"v2","created":"Fri, 26 Nov 2021 23:34:23 GMT"}],"update_date":"2021-11-30"}
{"id":"2104.08708","submitter":"Haochuan Li","authors":"Haochuan Li, Yi Tian, Jingzhao Zhang, Ali Jadbabaie","title":"Complexity Lower Bounds for Nonconvex-Strongly-Concave Min-Max\n  Optimization","comments":"20 pages, 1 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC cs.LG stat.ML","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We provide a first-order oracle complexity lower bound for finding stationary\npoints of min-max optimization problems where the objective function is smooth,\nnonconvex in the minimization variable, and strongly concave in the\nmaximization variable. We establish a lower bound of\n$\\Omega\\left(\\sqrt{\\kappa}\\epsilon^{-2}\\right)$ for deterministic oracles,\nwhere $\\epsilon$ defines the level of approximate stationarity and $\\kappa$ is\nthe condition number. Our analysis shows that the upper bound achieved in (Lin\net al., 2020b) is optimal in the $\\epsilon$ and $\\kappa$ dependence up to\nlogarithmic factors. For stochastic oracles, we provide a lower bound of\n$\\Omega\\left(\\sqrt{\\kappa}\\epsilon^{-2} + \\kappa^{1/3}\\epsilon^{-4}\\right)$. It\nsuggests that there is a significant gap between the upper bound\n$\\mathcal{O}(\\kappa^3 \\epsilon^{-4})$ in (Lin et al., 2020a) and our lower\nbound in the condition number dependence.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:30:01 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08709","submitter":"Lev Magarill Dr.","authors":"A.V.Chaplik and L.I.Magarill","title":"Size and Shape Effects in the Orbital Magnetization of TMDs Monolayers","comments":"7 pages, 1 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.mes-hall","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The intrinsic orbital magnetization of a TMD monolayer is usually calculated\nfor a plane unbounded system without mentioning the geometrical shape of\nsamples and boundary conditions (BCs) for electron wave functions. The method\nof calculations includes allowing for the Berry curvature contribution also in\nthe case when the system is described by the two-band minimal model [9]. In the\npresent paper, we show that the geometrical and topological properties of the\nspecimen, as well as the BCs, play an important role in the problem of\nmagnetization even for a macroscopic specimen.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:31:09 GMT"},{"version":"v2","created":"Sun, 27 Jun 2021 06:33:04 GMT"}],"update_date":"2021-06-29"}
{"id":"2104.08710","submitter":"Vidhisha Balachandran","authors":"Vidhisha Balachandran, Ashish Vaswani, Yulia Tsvetkov, Niki Parmar","title":"Simple and Efficient ways to Improve REALM","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Dense retrieval has been shown to be effective for retrieving relevant\ndocuments for Open Domain QA, surpassing popular sparse retrieval methods like\nBM25. REALM (Guu et al., 2020) is an end-to-end dense retrieval system that\nrelies on MLM based pretraining for improved downstream QA efficiency across\nmultiple datasets. We study the finetuning of REALM on various QA tasks and\nexplore the limits of various hyperparameter and supervision choices. We find\nthat REALM was significantly undertrained when finetuning and simple\nimprovements in the training, supervision, and inference setups can\nsignificantly benefit QA results and exceed the performance of other models\npublished post it. Our best model, REALM++, incorporates all the best working\nfindings and achieves significant QA accuracy improvements over baselines\n(~5.5% absolute accuracy) without any model design changes. Additionally,\nREALM++ matches the performance of large Open Domain QA models which have 3x\nmore parameters demonstrating the efficiency of the setup.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:32:33 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08711","submitter":"Zhan Sun","authors":"Zhan Sun and Hong Fei Zhang","title":"Comprehensive studies of $\\Upsilon$ inclusive production in $Z$ boson\n  decay","comments":"17 pages, 6 figures, and 4 tables; accepted for publication in JHEP","journal-ref":"JHEP 06 (2021) 152","doi":"10.1007/JHEP06(2021)152","report-no":null,"categories":"hep-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we present a comprehensive study of $\\Upsilon$ inclusive\nproduction in $Z$ boson decay, including the first complete\nnext-to-leading-order calculations of the color-octet (CO) contributions. With\nthe inclusion of the newly-calculated remarkable QCD corrections, the CO\nprocesses exhibit crucially phenomenological influence on the existing\npredictions built on the color-singlet mechanism. We also include the\nexhaustive evaluations of the feed-down contributions, which remained ignored\nin the literature, and find them to be considerable. Summing up all the\ncontributions, the $\\mathcal{B}_{Z \\to \\Upsilon(nS)+X}$ still notably\nundershoot the data released by the L3 Collaboration.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:36:11 GMT"},{"version":"v2","created":"Fri, 11 Jun 2021 09:03:45 GMT"}],"update_date":"2021-07-14"}
{"id":"2104.08712","submitter":"Ehsan Qasemi","authors":"Ehsan Qasemi, Filip Ilievski, Muhao Chen, Pedro Szekely","title":"PaCo: Preconditions Attributed to Commonsense Knowledge","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Humans can seamlessly reason with circumstantial preconditions of commonsense\nknowledge. We understand that a glass is used for drinking water, unless the\nglass is broken or the water is toxic. Despite state-of-the-art (SOTA) language\nmodels' (LMs) impressive performance on inferring commonsense knowledge, it is\nunclear whether they understand the circumstantial preconditions. To address\nthis gap, we propose a novel challenge of reasoning with circumstantial\npreconditions. We collect a dataset, called PaCo, consisting of 12.4 thousand\npreconditions of commonsense statements expressed in natural language. Based on\nthis dataset, we create three canonical evaluation tasks and use them to\nexamine the capability of existing LMs to understand situational preconditions.\nOur results reveal a 10-30% gap between machine and human performance on our\ntasks, which shows that reasoning with preconditions is an open challenge.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:37:54 GMT"},{"version":"v2","created":"Thu, 12 May 2022 19:09:38 GMT"}],"update_date":"2022-05-16"}
{"id":"2104.08713","submitter":"Jinglai Shen","authors":"Jinglai Shen, Eswar Kumar H. Kammara, Lili Du","title":"Nonconvex, Fully Distributed Optimization based CAV Platooning Control\n  under Nonlinear Vehicle Dynamics","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  CAV platooning technology has received considerable attention in the past few\nyears, driven by the next generation smart transportation systems. Unlike most\nof the existing platooning methods that focus on linear vehicle dynamics of\nCAVs, this paper considers nonlinear vehicle dynamics and develops fully\ndistributed optimization based CAV platooning control schemes via the model\npredictive control (MPC) approach for a possibly heterogeneous CAV platoon. The\nnonlinear vehicle dynamics leads to several major difficulties in distributed\nalgorithm development and control analysis and design. Specifically, the\nunderlying MPC optimization problem is nonconvex and densely coupled. Further,\nthe closed loop dynamics becomes a time-varying nonlinear system subject to\nexternal perturbations, making closed loop stability analysis rather\ncomplicated. To overcome these difficulties, we formulate the underlying MPC\noptimization problem as a locally coupled, albeit nonconvex, optimization\nproblem and develop a sequential convex programming based fully distributed\nscheme for a general MPC horizon. Such a scheme can be effectively implemented\nfor real-time computing using operator splitting methods. To analyze the closed\nloop stability, we apply various tools from global implicit function theorems,\nstability of linear time-varying systems, and Lyapunov theory for\ninput-to-state stability to show that the closed loop system is locally\ninput-to-state stable uniformly in all small coefficients pertaining to the\nnonlinear dynamics. Numerical tests on homogeneous and heterogeneous CAV\nplatoons demonstrate the effectiveness of the proposed fully distributed\nschemes and CAV platooning control.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:42:03 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08714","submitter":"Dongfang Gao","authors":"Dongfang Gao, Kaiming Zhao","title":"Tensor product weight modules for the mirror-twisted Heisenberg-Virasoro\n  algebra","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.RT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we study irreducible weight modules with infinite dimensional\nweight spaces over the mirror-twisted Heisenberg-Virasoro algebra\n$\\mathcal{D}$. More precisely, the necessary and sufficient conditions for the\ntensor products of irreducible highest weight modules and irreducible modules\nof intermediates series over $\\mathcal{D}$ to be irreducible are determined by\nusing \"shifting technique\". This leads to a family of new irreducible weight\nmodules over $\\mathcal{D}$. Then we obtain that any two such tensor products\nare isomorphic if and only if the corresponding highest weight modules and\nmodules of intermediate series are isomorphic respectively. Also we discuss\nsubmodules of the tensor product module when it is not irreducible.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:44:57 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08715","submitter":"Dongfang Gao","authors":"Dongfang Gao, Yao Ma, Kaiming Zhao","title":"Non-weight modules over the mirror Heisenberg-Virasoro algebra","comments":"to appear in Science China Mathematics. arXiv admin note: text\n  overlap with arXiv:2104.08714","journal-ref":null,"doi":null,"report-no":null,"categories":"math.RT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, we study irreducible non-weight modules over the mirror\nHeisenberg-Virasoro algebra $\\mathcal{D}$, including Whittaker modules,\n$\\mathcal{U}(\\mathbb{C} d_0)$-free modules, and their tensor products. More\nprecisely, we give the necessary and sufficient conditions for the Whittaker\nmodules to be irreducible. We determine all $\\mathcal{D}$-module structures on\n$\\mathcal{U}(\\mathbb{C} d_0)$, and find the necessary and sufficient conditions\nfor these modules to be irreducible. At last we determine the necessary and\nsufficient conditions for the tensor products of Whittaker modules and\n$\\mathcal{U}(\\mathbb{C} d_0)$-free modules to be irreducible, and obtain that\nany two such tensor products are isomorphic if and only if the corresponding\nWhittaker modules and $\\mathcal{U}(\\mathbb{C} d_0)$-free modules are\nisomorphic. These lead to many new irreducible non-weight modules over\n$\\mathcal{D}$.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:50:07 GMT"},{"version":"v2","created":"Fri, 24 Dec 2021 11:41:17 GMT"}],"update_date":"2021-12-28"}
{"id":"2104.08716","submitter":"Yu Zhang","authors":"Huangbin Zhang, Chong Zhao, Yu Zhang, Danlei Wang, Haichao Yang","title":"Deep Latent Emotion Network for Multi-Task Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.IR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Feed recommendation models are widely adopted by numerous feed platforms to\nencourage users to explore the contents they are interested in. However, most\nof the current research simply focus on targeting user's preference and lack\nin-depth study of avoiding objectionable contents to be frequently recommended,\nwhich is a common reason that let user detest. To address this issue, we\npropose a Deep Latent Emotion Network (DLEN) model to extract latent\nprobability of a user preferring a feed by modeling multiple targets with\nsemi-supervised learning. With this method, the conflicts of different targets\nare successfully reduced in the training phase, which improves the training\naccuracy of each target effectively. Besides, by adding this latent state of\nuser emotion to multi-target fusion, the model is capable of decreasing the\nprobability to recommend objectionable contents to improve user retention and\nstay time during online testing phase. DLEN is deployed on a real-world\nmulti-task feed recommendation scenario of Tencent QQ-Small-World with a\ndataset containing over a billion samples, and it exhibits a significant\nperformance advantage over the SOTA MTL model in offline evaluation, together\nwith a considerable increase by 3.02% in view-count and 2.63% in user stay-time\nin production. Complementary offline experiments of DLEN model on a public\ndataset also repeat improvements in various scenarios. At present, DLEN model\nhas been successfully deployed in Tencent's feed recommendation system.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:55:13 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08717","submitter":"Bingyuan Liu","authors":"Bingyuan Liu, Jose Dolz, Adrian Galdran, Riadh Kobbi, Ismail Ben Ayed","title":"The hidden label-marginal biases of segmentation losses","comments":"Code available at https://github.com/by-liu/SegLossBias","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Most segmentation losses are arguably variants of the Cross-Entropy (CE) or\nDice losses. In the abundant segmentation literature, there is no clear\nconsensus as to which of these losses is a better choice, with varying\nperformances for each across different benchmarks and applications. In this\nwork, we develop a theoretical analysis that links these two types of losses,\nexposing their advantages and weaknesses. First, we provide a\nconstrained-optimization perspective showing that CE and Dice share a much\ndeeper connection than previously thought: They both decompose into\nlabel-marginal penalties and closely related ground-truth matching penalties.\nThen, we provide bound relationships and an information-theoretic analysis,\nwhich uncover hidden label-marginal biases: Dice has an intrinsic bias towards\nspecific extremely imbalanced solutions, whereas CE implicitly encourages the\nground-truth region proportions. Our theoretical results explain the wide\nexperimental evidence in the medical-imaging literature, whereby Dice losses\nbring improvements for imbalanced segmentation. It also explains why CE\ndominates natural-image problems with diverse class proportions, in which case\nDice might have difficulty adapting to different label-marginal distributions.\nBased on our theoretical analysis, we propose a principled and simple solution,\nwhich enables to control explicitly the label-marginal bias. Our loss\nintegrates CE with explicit ${\\cal L}_1$ regularization, which encourages label\nmarginals to match target class proportions, thereby mitigating class imbalance\nbut without losing generality. Comprehensive experiments and ablation studies\nover different losses and applications validate our theoretical analysis, as\nwell as the effectiveness of our explicit label-marginal regularizers.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 04:59:39 GMT"},{"version":"v2","created":"Wed, 6 Oct 2021 00:02:13 GMT"},{"version":"v3","created":"Tue, 29 Mar 2022 20:36:17 GMT"}],"update_date":"2022-03-31"}
{"id":"2104.08718","submitter":"Jack Hessel","authors":"Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, Yejin Choi","title":"CLIPScore: A Reference-free Evaluation Metric for Image Captioning","comments":null,"journal-ref":"EMNLP 2021","doi":null,"report-no":null,"categories":"cs.CV cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Image captioning has conventionally relied on reference-based automatic\nevaluations, where machine captions are compared against captions written by\nhumans. This is in contrast to the reference-free manner in which humans assess\ncaption quality.\n  In this paper, we report the surprising empirical finding that CLIP (Radford\net al., 2021), a cross-modal model pretrained on 400M image+caption pairs from\nthe web, can be used for robust automatic evaluation of image captioning\nwithout the need for references. Experiments spanning several corpora\ndemonstrate that our new reference-free metric, CLIPScore, achieves the highest\ncorrelation with human judgements, outperforming existing reference-based\nmetrics like CIDEr and SPICE. Information gain experiments demonstrate that\nCLIPScore, with its tight focus on image-text compatibility, is complementary\nto existing reference-based metrics that emphasize text-text similarities.\nThus, we also present a reference-augmented version, RefCLIPScore, which\nachieves even higher correlation. Beyond literal description tasks, several\ncase studies reveal domains where CLIPScore performs well (clip-art images,\nalt-text rating), but also where it is relatively weaker in comparison to\nreference-based metrics, e.g., news captions that require richer contextual\nknowledge.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 05:00:29 GMT"},{"version":"v2","created":"Tue, 14 Sep 2021 21:25:18 GMT"},{"version":"v3","created":"Wed, 23 Mar 2022 19:47:21 GMT"}],"update_date":"2022-03-25"}
{"id":"2104.08719","submitter":"Yande Que","authors":"Yande Que, Bin Liu, Yuan Zhuang, Chaoqiang Xu, Kedong Wang, and Xudong\n  Xiao","title":"On-Surface Synthesis of Graphene Nanoribbons on Two-Dimensional Rare\n  Earth-Gold Intermetallic Compounds","comments":"25 pages including 5 figures in the main text and 6 figures in the\n  supporting information","journal-ref":"JPCL2020","doi":"10.1021/acs.jpclett.0c01398","report-no":null,"categories":"cond-mat.mtrl-sci cond-mat.mes-hall","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Here, we demonstrate two reliable routes for the fabrication of armchair-edge\ngraphene nanoribbons (GNRs) on TbAu2/Au(111), belonging to a class of\ntwo-dimensional ferromagnetic rare earth-gold intermetallic compounds.\nOn-surface synthesis directly on TbAu2 leads to the formation of GNRs, which\nare short and interconnected with each other. In contrast, the intercalation\napproach - on-surface synthesis of GNRs directly on Au(111) followed by rare\nearth intercalation - yields GNRs on TbAu2/Au(111), where both the ribbons and\nTbAu2 are of high quality comparable with those directly grown on clean\nAu(111). Besides, the as-grown ribbons retain the same band gap while changing\nfrom p-doping to weak n-doping mainly due to a change in the work function of\nthe substrate after the rare earth intercalation. The intercalation approach\nmight also be employed to fabricate other types of GNRs on various rare earth\nintermetallic compounds, providing platforms to tailor the electronic and\nmagnetic properties of GNRs on magnetic substrates.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 05:08:43 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08720","submitter":"Alberto  Escalante","authors":"Alberto Escalante (Puebla U., Inst. Fis.) and Victor Alberto\n  Zavala-Perez (Puebla U., Inst. Fis.)","title":"The Hamilton-Jacobi analysis for higher order Maxwell-Chern-Simons gauge\n  theory","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th math-ph math.MP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  By using the Hamilton-Jacobi [$HJ$] framework the higher-order\nMaxwell-Chern-Simons theory is analyzed. The complete set of $HJ$ Hamiltonians\nand a generalized $HJ$ differential are reported, from which all symmetries of\nthe theory are identified. In addition, we complete our study by performing the\nhigher order Gitman-Lyakhovich-Tyutin [$GLT$] framework and compare the results\nof both formalisms.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 05:09:22 GMT"},{"version":"v2","created":"Fri, 18 Jun 2021 14:54:07 GMT"}],"update_date":"2021-06-21"}
{"id":"2104.08721","submitter":"Kelly Marchisio","authors":"Kelly Marchisio, Conghao Xiong, and Philipp Koehn","title":"Embedding-Enhanced Giza++: Improving Alignment in Low- and High-\n  Resource Scenarios Using Embedding Space Geometry","comments":"AMTA2022 Camera Ready","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  A popular natural language processing task decades ago, word alignment has\nbeen dominated until recently by GIZA++, a statistical method based on the\n30-year-old IBM models. New methods that outperform GIZA++ primarily rely on\nlarge machine translation models, massively multilingual language models, or\nsupervision from GIZA++ alignments itself. We introduce Embedding-Enhanced\nGIZA++, and outperform GIZA++ without any of the aforementioned factors. Taking\nadvantage of monolingual embedding spaces of source and target language only,\nwe exceed GIZA++'s performance in every tested scenario for three languages\npairs. In the lowest-resource setting, we outperform GIZA++ by 8.5, 10.9, and\n12 AER for Ro-En, De-En, and En-Fr, respectively. We release our code at\nhttps://github.com/kellymarchisio/ee-giza.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 05:21:50 GMT"},{"version":"v2","created":"Tue, 11 Oct 2022 02:39:34 GMT"}],"update_date":"2022-10-12"}
{"id":"2104.08722","submitter":"Alberto  Escalante","authors":"A. Escalante (Puebla U., Inst. Fis.) and Jorge Hern\\'andez Aguilar\n  (FCFM, BUAP)","title":"New canonical analysis for higher order topologically massive gravity","comments":null,"journal-ref":null,"doi":"10.1140/epjc/s10052-021-09429-6","report-no":null,"categories":"gr-qc math-ph math.MP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  A detailed Gitman-Lyakhovich-Tyutin analysis for higher-order topologically\nmassive gravity is performed. The full structure of the constraints, the\ncounting of physical degrees of freedom, and the Dirac algebra among the\nconstraints are reported. Moreover, our analysis presents a new structure of\nthe constraints and we compare our results with those reported in the\nliterature where a standard Ostrogradski framework was developed.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 05:25:05 GMT"},{"version":"v2","created":"Sun, 30 May 2021 21:55:46 GMT"},{"version":"v3","created":"Fri, 18 Jun 2021 16:42:59 GMT"}],"update_date":"2021-08-18"}
{"id":"2104.08723","submitter":"Xiuwen Zheng","authors":"Xiuwen Zheng, Dheeraj Mekala, Amarnath Gupta, Jingbo Shang","title":"News Meets Microblog: Hashtag Annotation via Retriever-Generator","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Hashtag annotation for microblog posts has been recently formulated as a\nsequence generation problem to handle emerging hashtags that are unseen in the\ntraining set. The state-of-the-art method leverages conversations initiated by\nposts to enrich contextual information for the short posts. However, it is\nunrealistic to assume the existence of conversations before the hashtag\nannotation itself. Therefore, we propose to leverage news articles published\nbefore the microblog post to generate hashtags following a Retriever-Generator\nframework. Extensive experiments on English Twitter datasets demonstrate\nsuperior performance and significant advantages of leveraging news articles to\ngenerate hashtags.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 05:28:13 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08724","submitter":"Yuning Mao","authors":"Yuning Mao, Wenchang Ma, Deren Lei, Jiawei Han, Xiang Ren","title":"Extract, Denoise and Enforce: Evaluating and Improving Concept\n  Preservation for Text-to-Text Generation","comments":"EMNLP 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Prior studies on text-to-text generation typically assume that the model\ncould figure out what to attend to in the input and what to include in the\noutput via seq2seq learning, with only the parallel training data and no\nadditional guidance. However, it remains unclear whether current models can\npreserve important concepts in the source input, as seq2seq learning does not\nhave explicit focus on the concepts and commonly used evaluation metrics also\ntreat concepts equally important as other tokens. In this paper, we present a\nsystematic analysis that studies whether current seq2seq models, especially\npre-trained language models, are good enough for preserving important input\nconcepts and to what extent explicitly guiding generation with the concepts as\nlexical constraints is beneficial. We answer the above questions by conducting\nextensive analytical experiments on four representative text-to-text generation\ntasks. Based on the observations, we then propose a simple yet effective\nframework to automatically extract, denoise, and enforce important input\nconcepts as lexical constraints. This new method performs comparably or better\nthan its unconstrained counterpart on automatic metrics, demonstrates higher\ncoverage for concept preservation, and receives better ratings in the human\nevaluation. Our code is available at https://github.com/morningmoni/EDE.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 05:29:02 GMT"},{"version":"v2","created":"Fri, 3 Sep 2021 00:24:15 GMT"}],"update_date":"2021-09-06"}
{"id":"2104.08725","submitter":"Vardarajan Suneeta","authors":"Sreejith Nair, Vardarajan Suneeta","title":"The Black hole Black string phase transition in Einstein-Gauss-Bonnet\n  gravity","comments":"23 pages, modifications, subsection added","journal-ref":"Phys. Rev. D 104, 044042 (2021)","doi":"10.1103/PhysRevD.104.044042","report-no":null,"categories":"gr-qc hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We investigate the presence of a black hole black string phase transition in\nEinstein Gauss Bonnet (EGB) gravity in the large dimension limit. The merger\npoint is the static spacetime connecting the black string phase with the black\nhole phase. We consider several ranges of the Gauss-Bonnet parameter. We find\nthat there is a range when the Gauss-Bonnet corrections are subordinate to the\nEinstein gravity terms in the large dimension limit, and yet the merger point\ngeometry does not approach a black hole away from the neck. We cannot rule out\na topology changing phase transition as argued by Kol. However as the merger\npoint geometry does not approach the black hole geometry asymptotically it is\nnot obvious that the transition is directly to a black hole phase. We also\ndemonstrate that for another range of the Gauss-Bonnet parameter, the merger\npoint geometry approaches the black hole geometry asymptotically when a certain\nparameter depending on the Gauss-Bonnet parameter $\\alpha$ and on the\nparameters in the Einstein-Gauss-Bonnet black hole metric is small enough.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 05:30:29 GMT"},{"version":"v2","created":"Wed, 30 Jun 2021 05:25:56 GMT"}],"update_date":"2021-08-25"}
{"id":"2104.08726","submitter":"Abteen Ebrahimi","authors":"Abteen Ebrahimi, Manuel Mager, Arturo Oncevay, Vishrav Chaudhary, Luis\n  Chiruzzo, Angela Fan, John Ortega, Ricardo Ramos, Annette Rios, Ivan\n  Meza-Ruiz, Gustavo A. Gim\\'enez-Lugo, Elisabeth Mager, Graham Neubig, Alexis\n  Palmer, Rolando Coto-Solano, Ngoc Thang Vu and Katharina Kann","title":"AmericasNLI: Evaluating Zero-shot Natural Language Understanding of\n  Pretrained Multilingual Models in Truly Low-resource Languages","comments":"Accepted to ACL 2022","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Pretrained multilingual models are able to perform cross-lingual transfer in\na zero-shot setting, even for languages unseen during pretraining. However,\nprior work evaluating performance on unseen languages has largely been limited\nto low-level, syntactic tasks, and it remains unclear if zero-shot learning of\nhigh-level, semantic tasks is possible for unseen languages. To explore this\nquestion, we present AmericasNLI, an extension of XNLI (Conneau et al., 2018)\nto 10 indigenous languages of the Americas. We conduct experiments with XLM-R,\ntesting multiple zero-shot and translation-based approaches. Additionally, we\nexplore model adaptation via continued pretraining and provide an analysis of\nthe dataset by considering hypothesis-only models. We find that XLM-R's\nzero-shot performance is poor for all 10 languages, with an average performance\nof 38.62%. Continued pretraining offers improvements, with an average accuracy\nof 44.05%. Surprisingly, training on poorly translated data by far outperforms\nall other methods with an accuracy of 48.72%.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 05:32:28 GMT"},{"version":"v2","created":"Wed, 16 Mar 2022 21:40:42 GMT"}],"update_date":"2022-03-18"}
{"id":"2104.08727","submitter":"Daniel Khashabi Mr.","authors":"Daniel Khashabi, Amos Ng, Tushar Khot, Ashish Sabharwal, Hannaneh\n  Hajishirzi, Chris Callison-Burch","title":"GooAQ: Open Question Answering with Diverse Answer Types","comments":"EMNLP-Findings 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  While day-to-day questions come with a variety of answer types, the current\nquestion-answering (QA) literature has failed to adequately address the answer\ndiversity of questions. To this end, we present GooAQ, a large-scale dataset\nwith a variety of answer types. This dataset contains over 5 million questions\nand 3 million answers collected from Google. GooAQ questions are collected\nsemi-automatically from the Google search engine using its autocomplete\nfeature. This results in naturalistic questions of practical interest that are\nnonetheless short and expressed using simple language. GooAQ answers are mined\nfrom Google's responses to our collected questions, specifically from the\nanswer boxes in the search results. This yields a rich space of answer types,\ncontaining both textual answers (short and long) as well as more structured\nones such as collections. We benchmarkT5 models on GooAQ and observe that: (a)\nin line with recent work, LM's strong performance on GooAQ's short-answer\nquestions heavily benefit from annotated data; however, (b) their quality in\ngenerating coherent and accurate responses for questions requiring long\nresponses (such as 'how' and 'why' questions) is less reliant on observing\nannotated data and mainly supported by their pre-training. We release GooAQ to\nfacilitate further research on improving QA with diverse response types.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 05:40:39 GMT"},{"version":"v2","created":"Fri, 10 Sep 2021 22:00:30 GMT"}],"update_date":"2021-09-14"}
{"id":"2104.08728","submitter":"Emily Sheng","authors":"Emily Sheng, Josh Arnold, Zhou Yu, Kai-Wei Chang, Nanyun Peng","title":"Revealing Persona Biases in Dialogue Systems","comments":"8 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Dialogue systems in the form of chatbots and personal assistants are being\nincreasingly integrated into people's lives. Modern dialogue systems may\nconsider adopting anthropomorphic personas, mimicking societal demographic\ngroups to appear more approachable and trustworthy to users. However, the\nadoption of a persona can result in the adoption of biases. In this paper, we\npresent the first large-scale study on persona biases in dialogue systems and\nconduct analyses on personas of different social classes, sexual orientations,\nraces, and genders. We define persona biases as harmful differences in\nresponses (e.g., varying levels of offensiveness, agreement with harmful\nstatements) generated from adopting different demographic personas.\nFurthermore, we introduce an open-source framework, UnitPersonaBias, to explore\nand aggregate persona biases in dialogue systems. By analyzing the Blender and\nDialoGPT dialogue systems, we observe that adopting personas can actually\ndecrease harmful responses, compared to not using any personas. Additionally,\nwe find that persona choices can affect the degree of harms in generated\nresponses and thus should be systematically evaluated before deployment. We\nalso analyze how personas can result in different amounts of harm towards\nspecific demographics.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 05:44:41 GMT"},{"version":"v2","created":"Wed, 15 Dec 2021 07:38:56 GMT"}],"update_date":"2021-12-16"}
{"id":"2104.08729","submitter":"Xianjie Shen","authors":"Xianjie Shen, Yinghan Wang, Rui Meng, Jingbo Shang","title":"Unsupervised Deep Keyphrase Generation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Keyphrase generation aims to summarize long documents with a collection of\nsalient phrases. Deep neural models have demonstrated a remarkable success in\nthis task, capable of predicting keyphrases that are even absent from a\ndocument. However, such abstractiveness is acquired at the expense of a\nsubstantial amount of annotated data. In this paper, we present a novel method\nfor keyphrase generation, AutoKeyGen, without the supervision of any human\nannotation. Motivated by the observation that an absent keyphrase in one\ndocument can appear in other places, in whole or in part, we first construct a\nphrase bank by pooling all phrases in a corpus. With this phrase bank, we then\ndraw candidate absent keyphrases for each document through a partial matching\nprocess. To rank both types of candidates, we combine their lexical- and\nsemantic-level similarities to the input document. Moreover, we utilize these\ntop-ranked candidates as to train a deep generative model for more absent\nkeyphrases. Extensive experiments demonstrate that AutoKeyGen outperforms all\nunsupervised baselines and can even beat strong supervised methods in certain\ncases.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 05:53:19 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08730","submitter":"Apurba Biswas","authors":"Apurba Biswas, V. V. Prasad and R. Rajesh","title":"Mpemba effect in an anisotropically driven granular gas","comments":"5 pages, 4 figures and supplemental material","journal-ref":null,"doi":"10.1209/0295-5075/ac2d54","report-no":null,"categories":"cond-mat.stat-mech","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We demonstrate the existence, as well as determine the conditions, of a\nMpemba effect - a counterintuitive phenomenon where a hotter system\nequilibrates faster than a cooler system when quenched to a cold temperature -\nin anisotropically driven granular gases. In contrast to earlier studies of\nMpemba effect in granular systems, the initial states are stationary, making it\na suitable system to experimentally study the effect. Our theoretical\npredictions for the regular, inverse and strong Mpemba effects agree well with\nresults of event-driven molecular dynamics simulations of hard discs.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 05:59:41 GMT"}],"update_date":"2022-03-14"}
{"id":"2104.08731","submitter":"Jifan Chen","authors":"Jifan Chen, Eunsol Choi, Greg Durrett","title":"Can NLI Models Verify QA Systems' Predictions?","comments":"EMNLP2021 Findings","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  To build robust question answering systems, we need the ability to verify\nwhether answers to questions are truly correct, not just \"good enough\" in the\ncontext of imperfect QA datasets. We explore the use of natural language\ninference (NLI) as a way to achieve this goal, as NLI inherently requires the\npremise (document context) to contain all necessary information to support the\nhypothesis (proposed answer to the question). We leverage large pre-trained\nmodels and recent prior datasets to construct powerful question converter and\ndecontextualization modules, which can reformulate QA instances as\npremise-hypothesis pairs with very high reliability. Then, by combining\nstandard NLI datasets with NLI examples automatically derived from QA training\ndata, we can train NLI models to judge the correctness of QA models' proposed\nanswers. We show that our NLI approach can generally improve the confidence\nestimation of a QA model across different domains, evaluated in a selective QA\nsetting. Careful manual analysis over the predictions of our NLI model shows\nthat it can further identify cases where the QA model produces the right answer\nfor the wrong reason, or where the answer cannot be verified as addressing all\naspects of the question.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:03:07 GMT"},{"version":"v2","created":"Sat, 11 Sep 2021 06:32:41 GMT"}],"update_date":"2021-09-14"}
{"id":"2104.08732","submitter":"Burhan Rashid Hussein","authors":"Burhan Rashid Hussein, Owais Ahmed Malik, Wee-Hong Ong, Johan Willem\n  Frederik Slik","title":"Application of Computer Vision and Machine Learning for Digitized\n  Herbarium Specimens: A Systematic Literature Review","comments":"42 pages, 9 figures, journal","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Herbarium contains treasures of millions of specimens which have been\npreserved for several years for scientific studies. To speed up more scientific\ndiscoveries, a digitization of these specimens is currently on going to\nfacilitate easy access and sharing of its data to a wider scientific community.\nOnline digital repositories such as IDigBio and GBIF have already accumulated\nmillions of specimen images yet to be explored. This presents a perfect time to\nautomate and speed up more novel discoveries using machine learning and\ncomputer vision. In this study, a thorough analysis and comparison of more than\n50 peer-reviewed studies which focus on application of computer vision and\nmachine learning techniques to digitized herbarium specimen have been examined.\nThe study categorizes different techniques and applications which have been\ncommonly used and it also highlights existing challenges together with their\npossible solutions. It is our hope that the outcome of this study will serve as\na strong foundation for beginners of the relevant field and will also shed more\nlight for both computer science and ecology experts.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:08:51 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08733","submitter":"Tao Li","authors":"Tao Li and Jianhua Yang","title":"Can Hubbard model resist electric current?","comments":"3 pages, 0 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cond-mat.str-el cond-mat.supr-con","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  It is claimed by a recent quantum Monte Carlo simulation that the\nlinear-in-temperature DC resistivity observed in the high-$T_{c}$ cuprate\nsuperconductors can be reproduced in the pure two dimensional Hubbard\nmodel\\cite{Huang}. Here we show perturbatively that such a translational\ninvariant electronic model can not support a steady state current in the\npresence of a uniform electric field at any finite temperature. Instead, the\nHubbard model is perfectly conducting in the linear response regime and will\nundergo Bloch oscillation at finite electric field for any finite temperature.\nNevertheless, the quantum Monte Carlo simulation can provide us the key\ninformation on the temperature dependence of the Drude weight, a quantity of\ncentral importance in the holographic description of the transport properties\nof the strange metal phase.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:10:31 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08734","submitter":"Ashish Gondimalla","authors":"Ashish Gondimalla, Sree Charan Gundabolu, T.N. Vijaykumar, and Mithuna\n  Thottethodi","title":"Barrier-Free Large-Scale Sparse Tensor Accelerator (BARISTA) For\n  Convolutional Neural Networks","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Convolutional neural networks (CNNs) are emerging as powerful tools for\nvisual recognition. Recent architecture proposals for sparse CNNs exploit zeros\nin the feature maps and filters for performance and energy without losing\naccuracy. Sparse architectures that exploit two-sided sparsity in both feature\nmaps and filters have been studied only at small scales (e.g., 1K\nmultiply-accumulate(MAC) units). However, to realize their advantages in full,\nthe sparse architectures have to be scaled up to levels of the dense\narchitectures (e.g., 32K MACs in the TPU). Such scaling is challenging since\nachieving reuse through broadcasts incurs implicit barrier cost raises the\ninter-related issues of load imbalance, buffering, and on-chip bandwidth\ndemand. SparTen, a previous scheme, addresses one aspect of load balancing but\nnot other aspects, nor the other issues of buffering and bandwidth. To that\nend, we propose the barrier-free large-scale sparse tensor accelerator\n(BARISTA). BARISTA (1) is the first architecture for scaling up sparse CNN\naccelerators; (2) reduces on-chip bandwidth demand by telescoping\nrequest-combining the input map requests and snarfing the filter requests; (3)\nreduces buffering via basic buffer sharing and avoids the ensuing barriers\nbetween consecutive input maps by coloring the output buffers; (4) load\nbalances intra-filter work via dynamic round-robin work assignment; and (5)\nemploys hierarchical buffering which achieves high cache bandwidth via a few,\nwide, shared buffers and low buffering via narrower, private buffers at the\ncompute. Our simulations show that, on average, barista performs 5.4x, 2.2x,\n1.7x, 2.5x better than a dense, a one-sided, a naively-scaled two-sided, and an\niso-area two-sided architecture, respectively. Using 45-nm technology, ASIC\nsynthesis of our RTL design for four clusters of 8K MACs at 1 GHz clock speed,\nreports 213 mm$^2$ area and 170 W power.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:13:31 GMT"},{"version":"v2","created":"Sat, 8 May 2021 22:19:46 GMT"}],"update_date":"2021-05-11"}
{"id":"2104.08735","submitter":"Dheeru Dua","authors":"Dheeru Dua, Pradeep Dasigi, Sameer Singh, Matt Gardner","title":"Learning with Instance Bundles for Reading Comprehension","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  When training most modern reading comprehension models, all the questions\nassociated with a context are treated as being independent from each other.\nHowever, closely related questions and their corresponding answers are not\nindependent, and leveraging these relationships could provide a strong\nsupervision signal to a model. Drawing on ideas from contrastive estimation, we\nintroduce several new supervision techniques that compare question-answer\nscores across multiple related instances. Specifically, we normalize these\nscores across various neighborhoods of closely contrasting questions and/or\nanswers, adding another cross entropy loss term that is used in addition to\ntraditional maximum likelihood estimation. Our techniques require bundles of\nrelated question-answer pairs, which we can either mine from within existing\ndata or create using various automated heuristics. We empirically demonstrate\nthe effectiveness of training with instance bundles on two datasets -- HotpotQA\nand ROPES -- showing up to 11% absolute gains in accuracy.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:17:54 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08736","submitter":"Qi Qi","authors":"Qi Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, Tianbao Yang","title":"Stochastic Optimization of Areas Under Precision-Recall Curves with\n  Provable Convergence","comments":"Published on NeurIPS 2021, 24 pages, 10 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI cs.CV math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common\nmetrics for evaluating classification performance for imbalanced problems.\nCompared with AUROC, AUPRC is a more appropriate metric for highly imbalanced\ndatasets. While stochastic optimization of AUROC has been studied extensively,\nprincipled stochastic optimization of AUPRC has been rarely explored. In this\nwork, we propose a principled technical method to optimize AUPRC for deep\nlearning. Our approach is based on maximizing the averaged precision (AP),\nwhich is an unbiased point estimator of AUPRC. We cast the objective into a sum\nof {\\it dependent compositional functions} with inner functions dependent on\nrandom variables of the outer level. We propose efficient adaptive and\nnon-adaptive stochastic algorithms named SOAP with {\\it provable convergence\nguarantee under mild conditions} by leveraging recent advances in stochastic\ncompositional optimization. Extensive experimental results on image and graph\ndatasets demonstrate that our proposed method outperforms prior methods on\nimbalanced problems in terms of AUPRC. To the best of our knowledge, our work\nrepresents the first attempt to optimize AUPRC with provable convergence. The\nSOAP has been implemented in the libAUC library at~\\url{https://libauc.org/}.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:22:21 GMT"},{"version":"v2","created":"Wed, 2 Jun 2021 05:06:40 GMT"},{"version":"v3","created":"Sun, 6 Jun 2021 15:20:25 GMT"},{"version":"v4","created":"Thu, 11 Nov 2021 03:56:46 GMT"},{"version":"v5","created":"Wed, 12 Apr 2023 22:42:58 GMT"}],"update_date":"2023-04-14"}
{"id":"2104.08737","submitter":"Akhil Arora","authors":"Akhil Arora, Alberto Garc\\'ia-Dur\\'an, Robert West","title":"Low-Rank Subspaces for Unsupervised Entity Linking","comments":"EMNLP 2021, 18 pages, 22 figures","journal-ref":null,"doi":"10.18653/v1/2021.emnlp-main.634","report-no":null,"categories":"cs.CL cs.IR cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Entity linking is an important problem with many applications. Most previous\nsolutions were designed for settings where annotated training data is\navailable, which is, however, not the case in numerous domains. We propose a\nlight-weight and scalable entity linking method, Eigenthemes, that relies\nsolely on the availability of entity names and a referent knowledge base.\nEigenthemes exploits the fact that the entities that are truly mentioned in a\ndocument (the \"gold entities\") tend to form a semantically dense subset of the\nset of all candidate entities in the document. Geometrically speaking, when\nrepresenting entities as vectors via some given embedding, the gold entities\ntend to lie in a low-rank subspace of the full embedding space. Eigenthemes\nidentifies this subspace using the singular value decomposition and scores\ncandidate entities according to their proximity to the subspace. On the\nempirical front, we introduce multiple strong baselines that compare favorably\nto (and sometimes even outperform) the existing state of the art. Extensive\nexperiments on benchmark datasets from a variety of real-world domains showcase\nthe effectiveness of our approach.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:24:48 GMT"},{"version":"v2","created":"Thu, 14 Oct 2021 22:09:44 GMT"}],"update_date":"2022-07-07"}
{"id":"2104.08738","submitter":"In-Jee Jeong","authors":"In-Jee Jeong and Kyungkeun Kang","title":"Well-posedness and singularity formation for inviscid Keller-Segel-fluid\n  system of consumption type","comments":"39 pages, Comm. Math Phys., to appear","journal-ref":null,"doi":"10.1007/s00220-021-04292-8","report-no":null,"categories":"math.AP","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We consider the Keller-Segel system of consumption type coupled with an\nincompressible fluid equation. The system describes the dynamics of oxygen and\nbacteria densities evolving within a fluid. We establish local well-posedness\nof the system in Sobolev spaces for partially inviscid and fully inviscid\ncases. In the latter, additional assumptions on the initial data are required\nwhen either the oxygen or bacteria density touches zero. Even though the oxygen\ndensity satisfies a maximum principle due to consumption, we prove finite time\nblow-up of its $C^{2}$--norm with certain initial data.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:33:30 GMT"},{"version":"v2","created":"Fri, 23 Apr 2021 01:42:55 GMT"},{"version":"v3","created":"Mon, 29 Nov 2021 08:27:38 GMT"}],"update_date":"2022-02-16"}
{"id":"2104.08739","submitter":"Shen Li","authors":"Shen Li, Bingpeng Ma, Hong Chang, Shiguang Shan, Xilin Chen","title":"Continuity-Discrimination Convolutional Neural Network for Visual Object\n  Tracking","comments":"Accepted to ICME2018","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  This paper proposes a novel model, named Continuity-Discrimination\nConvolutional Neural Network (CD-CNN), for visual object tracking. Existing\nstate-of-the-art tracking methods do not deal with temporal relationship in\nvideo sequences, which leads to imperfect feature representations. To address\nthis problem, CD-CNN models temporal appearance continuity based on the idea of\ntemporal slowness. Mathematically, we prove that, by introducing temporal\nappearance continuity into tracking, the upper bound of target appearance\nrepresentation error can be sufficiently small with high probability. Further,\nin order to alleviate inaccurate target localization and drifting, we propose a\nnovel notion, object-centroid, to characterize not only objectness but also the\nrelative position of the target within a given patch. Both temporal appearance\ncontinuity and object-centroid are jointly learned during offline training and\nthen transferred for online tracking. We evaluate our tracker through extensive\nexperiments on two challenging benchmarks and show its competitive tracking\nperformance compared with state-of-the-art trackers.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:35:03 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08740","submitter":"Lei Yu","authors":"Lei Yu","title":"On the $\\Phi$-Stability and Related Conjectures","comments":"41 pages, 2 figure","journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR cs.IT math.CO math.IT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Given a convex function $\\Phi:[0,1]\\to\\mathbb{R}$ and the mean\n$\\mathbb{E}f(\\mathbf{X})=a\\in[0,1]$, which Boolean function $f$ maximizes the\n$\\Phi$-stability $\\mathbb{E}[\\Phi(T_{\\rho}f(\\mathbf{X}))]$ of $f$? Here\n$\\mathbf{X}$ is a random vector uniformly distributed on the discrete cube\n$\\{-1,1\\}^{n}$ and $T_{\\rho}$ is the Bonami-Beckner operator. Special cases of\nthis problem include the (symmetric and asymmetric) $\\alpha$-stability problems\nand the ``Most Informative Boolean Function'' problem. In this paper, we\nprovide several upper bounds for the maximal $\\Phi$-stability. When\nspecializing $\\Phi$ to some particular forms, by these upper bounds, we\npartially resolve Mossel and O'Donnell's conjecture on $\\alpha$-stability with\n$\\alpha>2$, Li and M\\'edard's conjecture on $\\alpha$-stability with\n$1<\\alpha<2$, and Courtade and Kumar's conjecture on the ``Most Informative\nBoolean Function'' which corresponds to a conjecture on $\\alpha$-stability with\n$\\alpha=1$. Our proofs are based on discrete Fourier analysis, optimization\ntheory, and improvements of the Friedgut--Kalai--Naor (FKN) theorem. Our\nimprovements of the FKN theorem are sharp or asymptotically sharp for certain\ncases.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:46:16 GMT"},{"version":"v2","created":"Thu, 29 Apr 2021 09:09:43 GMT"},{"version":"v3","created":"Wed, 9 Feb 2022 04:05:19 GMT"},{"version":"v4","created":"Thu, 27 Apr 2023 08:14:44 GMT"}],"update_date":"2023-04-28"}
{"id":"2104.08741","submitter":"Keshav Kolluru","authors":"Keshav Kolluru, Mayank Singh Chauhan, Yatin Nandwani, Parag Singla and\n  Mausam","title":"CEAR: Cross-Entity Aware Reranker for Knowledge Base Completion","comments":"We found a bug in the code that invalidates the reported results for\n  FB15k-237 and WN18RR. The results for OLPBench hold the same. We are in\n  process of updating the paper","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Pre-trained language models (LMs) like BERT have shown to store factual\nknowledge about the world. This knowledge can be used to augment the\ninformation present in Knowledge Bases, which tend to be incomplete. However,\nprior attempts at using BERT for task of Knowledge Base Completion (KBC)\nresulted in performance worse than embedding based techniques that rely only on\nthe graph structure. In this work we develop a novel model, Cross-Entity Aware\nReranker (CEAR), that uses BERT to re-rank the output of existing KBC models\nwith cross-entity attention. Unlike prior work that scores each entity\nindependently, CEAR uses BERT to score the entities together, which is\neffective for exploiting its factual knowledge. CEAR achieves a new state of\nart for the OLPBench dataset.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:56:00 GMT"},{"version":"v2","created":"Fri, 28 Jan 2022 04:34:37 GMT"}],"update_date":"2022-01-31"}
{"id":"2104.08742","submitter":"Rik Koncel-Kedziorski","authors":"Rik Koncel-Kedziorski and Noah A. Smith","title":"Go Forth and Prosper: Language Modeling with Ancient Textual History","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We introduce a technique for improving document-level language models (LM) by\nleveraging \"ancient history\": text that is outside the LM's current context\nwindow. We learn an auxiliary function to select spans from the ancient history\nwhich can help the LM to predict future text. The selected text spans are then\ncopied directly into the LM's context window, replacing less predictive spans.\nThis method can improve perplexity of pretrained LMs with no updates to the\nLM's own parameters. We further observe that an auxiliary function trained in a\nspecific textual domain like Wikipedia will also work in a substantially\ndifferent domain such as scientific publications. With this technique we see a\n7 percent perplexity reduction on Wikipedia articles, and a 12 percent\nperplexity reduction on scientific texts.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:57:30 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08743","submitter":"Imran Javaid Dr.","authors":"Imran Javaid, Shahroz Ali, Shahid Ur Rehman, Aqsa Shah","title":"Rough Sets in Graphs Using Similarity Relations","comments":"15 pages, 2 figures, 1 table","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO cs.IT math.IT","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we use theory of rough set to study graphs using the concept\nof orbits. We investigate the indiscernibility partitions and approximations of\ngraphs induced by orbits of graphs. We also study rough membership functions,\nessential sets, discernibility matrix and their relationships for graphs.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:59:18 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08744","submitter":"Dheeru Dua","authors":"Dheeru Dua, Cicero Nogueira dos Santos, Patrick Ng, Ben Athiwaratkun,\n  Bing Xiang, Matt Gardner, Sameer Singh","title":"Generative Context Pair Selection for Multi-hop Question Answering","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Compositional reasoning tasks like multi-hop question answering, require\nmaking latent decisions to get the final answer, given a question. However,\ncrowdsourced datasets often capture only a slice of the underlying task\ndistribution, which can induce unanticipated biases in models performing\ncompositional reasoning. Furthermore, discriminatively trained models exploit\nsuch biases to get a better held-out performance, without learning the right\nway to reason, as they do not necessitate paying attention to the question\nrepresentation (conditioning variable) in its entirety, to estimate the answer\nlikelihood. In this work, we propose a generative context selection model for\nmulti-hop question answering that reasons about how the given question could\nhave been generated given a context pair. While being comparable to the\nstate-of-the-art answering performance, our proposed generative passage\nselection model has a better performance (4.9% higher than baseline) on\nadversarial held-out set which tests robustness of model's multi-hop reasoning\ncapabilities.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:00:48 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08745","submitter":"Xingni Jiang","authors":"Marcel de Jeu and Xingni Jiang","title":"Order Integrals","comments":"Current version contains 39 pages. Several minor improvements in\n  presentation have been made. Final version, to appear in Positivity","journal-ref":"Positivity 26 (2022), no. 2, Paper No. 32, 39 pp","doi":"10.1007/s11117-022-00880-7","report-no":null,"categories":"math.FA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We define an integral of real-valued functions with respect to a measure that\ntakes its values in the extended positive cone of a partially ordered vector\nspace $E$. The monotone convergence theorem, Fatou's lemma, and the dominated\nconvergence theorem are established; the analogues of the classical ${\\mathcal\nL}^1$- and ${\\mathrm L}^1$-spaces are investigated. The results extend earlier\nwork by Wright and specialise to those for the Lebesgue integral when $E$\nequals the real numbers.\n  The hypothesis on $E$ that is needed for the definition of the integral and\nfor the monotone convergence theorem to hold ($\\sigma$-monotone completeness)\nis a rather mild one. It is satisfied, for example, by the space of regular\noperators between a directed partially ordered vector space and a\n$\\sigma$-monotone complete partially ordered vector space, and by every\nJBW-algebra. Fatou's lemma and the dominated convergence theorem hold for every\n$\\sigma$-Dedekind complete space.\n  When $E$ consists of the regular operators on a Banach lattice with an order\ncontinuous norm, or when it consists of the self-adjoint elements of a strongly\nclosed complex linear subspace of the bounded operators on a complex Hilbert\nspace, then the finite measures as in the current paper are precisely the\nstrongly $\\sigma$-additive positive operator-valued measures. When $E$ is a\npartially ordered Banach space with a closed positive cone, then every positive\nvector measure is a measure in our sense, but not conversely. Even when a\nmeasure falls into both categories, the domain of the integral as defined in\nthis paper can properly contain that of any reasonably defined integral with\nrespect to the vector measure using Banach space methods.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:02:57 GMT"},{"version":"v2","created":"Tue, 9 Nov 2021 05:26:06 GMT"}],"update_date":"2023-05-31"}
{"id":"2104.08746","submitter":"Rangeet Bhattacharyya","authors":"Nilanjana Chanda and Rangeet Bhattacharyya","title":"Emergence of the Born rule in strongly-driven dissipative systems","comments":null,"journal-ref":"Phys. Rev. A 104, 022436 (2021)","doi":"10.1103/PhysRevA.104.022436","report-no":null,"categories":"quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  To understand the dynamical origin of the measurement in quantum mechanics,\nseveral models have been put forward which have a quantum system coupled to an\napparatus. The system and the apparatus evolve in time and the Born rule for\nthe system to be in various eigenstates of the observable is naturally\nobtained. In this work, we show that the effect of the drive-induced\ndissipation in such a system can lead to the Born rule, even if there is no\nseparate apparatus. The applied drive needs to be much stronger than the\nsystem-environment coupling. In this condition, we show that the dynamics of a\ndriven-dissipative system could be reduced to a Milburn-like form, using a\nrecently-proposed fluctuation-regulated quantum master equation [A. Chakrabarti\nand R. Bhattacharyya, Phys. Rev. A 97, 063837 (2018)]. The system evolves\nirreversibly under the action of the first-order effect of the drive and the\ndrive-induced dissipation. The resulting mixed state is identical to that\nobtained by using the Born rule.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:09:33 GMT"}],"update_date":"2021-09-08"}
{"id":"2104.08747","submitter":"Xue Yu","authors":"Yu Xue, Yihang Tang, Xin Xu, Jiayu Liang, Ferrante Neri","title":"Multi-objective Feature Selection with Missing Data in Classification","comments":"1","journal-ref":null,"doi":"10.1109/TETCI.2021.3074147","report-no":null,"categories":"cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Feature selection (FS) is an important research topic in machine learning.\nUsually, FS is modelled as a+ bi-objective optimization problem whose\nobjectives are: 1) classification accuracy; 2) number of features. One of the\nmain issues in real-world applications is missing data. Databases with missing\ndata are likely to be unreliable. Thus, FS performed on a data set missing some\ndata is also unreliable. In order to directly control this issue plaguing the\nfield, we propose in this study a novel modelling of FS: we include reliability\nas the third objective of the problem. In order to address the modified\nproblem, we propose the application of the non-dominated sorting genetic\nalgorithm-III (NSGA-III). We selected six incomplete data sets from the\nUniversity of California Irvine (UCI) machine learning repository. We used the\nmean imputation method to deal with the missing data. In the experiments,\nk-nearest neighbors (K-NN) is used as the classifier to evaluate the feature\nsubsets. Experimental results show that the proposed three-objective model\ncoupled with NSGA-III efficiently addresses the FS problem for the six data\nsets included in this study.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:12:39 GMT"}],"update_date":"2021-04-21"}
{"id":"2104.08748","submitter":"Mohamed Boucetta","authors":"Abdelhak Abouqateb, Mohamed Boucetta, Charif Bourzik","title":"Submanifolds in Koszul-Vinberg geometry","comments":"27 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.DG math.SG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  A Koszul-Vinberg manifold is a manifold $M$ endowed with a pair $(\\nabla,h)$\nwhere $\\nabla$ is a flat connection and $h$ is a symmetric bivector field\nsatisfying a generalized Codazzi equation. The geometry of such manifolds could\nbe seen as a type of bridge between Poisson geometry and pseudo-Riemannian\ngeometry, as has been highlighted in our previous article\n[\\textit{Contravariant Pseudo-Hessian manifolds and their associated Poisson\nstructures}. \\rm{Differential Geometry and its Applications} (2020)]. Our\nobjective here will be to pursue our study by focusing in this setting on\nsubmanifolds by taking into account some developments in the theory of Poisson\nsubmanifolds.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:15:55 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08749","submitter":"Eldad Bettelheim","authors":"Eldad Bettelheim","title":"Exact Matrix Elements of the Field Operator in the Thermodynamic Limit\n  of the Lieb-Liniger Model","comments":"References added","journal-ref":null,"doi":"10.1088/1751-8121/ac1d8c","report-no":null,"categories":"cond-mat.stat-mech hep-th math-ph math.MP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We study a matrix element of the field operator in the Lieb-Liniger model\nusing the Bethe ansatz technique coupled with a functional approach to compute\nSlavnov determinants. We obtain the matrix element exactly in the thermodynamic\nlimit for any coupling constant $c$, and compare our results to known\nsemiclassics at the limit $c\\to0.$\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:21:57 GMT"},{"version":"v2","created":"Thu, 20 May 2021 09:05:26 GMT"}],"update_date":"2021-09-22"}
{"id":"2104.08750","submitter":"Alexander S. Sakharov","authors":"Alexander S. Sakharov, Yury N. Eroshenko, Sergey G. Rubin","title":"Looking at the NANOGrav Signal Through the Anthropic Window of\n  Axion-Like Particles","comments":"39 pages, 3 figures, the version accepted for publication in Physical\n  Review D","journal-ref":"Phys. Rev. D 104, 043005 (2021)","doi":"10.1103/PhysRevD.104.043005","report-no":null,"categories":"hep-ph astro-ph.CO gr-qc hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We explore the inflationary dynamics leading to formation of closed domain\nwalls in course of evolution of an axion like particle (ALP) field whose\nPeccei-Quinn-like phase transition occurred well before inflationary epoch.\nEvolving after inflation, the domain walls may leave their imprint in\nstochastic gravitational waves background, in the frequency range accessible\nfor the pulsar timing array measurements. We derive the characteristic strain\npower spectrum produced by the distribution of the closed domain walls and\nrelate it with the recently reported NANOGrav signal excess. We found that the\nslope of the frequency dependence of the characteristic strain spectrum\ngenerated by the domain walls is very well centered inside the range of the\nslopes in the signal reported by the NANOGrav. Analyzing the inflationary\ndynamics of the ALP field, in consistency with the isocurvature constraint, we\nrevealed those combinations of the parameters where the signal from the\ninflationary induced ALPs domain walls could saturate the amplitude of the\nNANOGrav excess. The evolution of big enough closed domain walls may incur in\nformation of wormholes with the walls escaping into baby universes. We studied\nthe conditions, when closed walls escaped into baby universes could leave a\ndetectable imprint in the stochastic gravitational waves background.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:22:06 GMT"},{"version":"v2","created":"Sat, 26 Jun 2021 21:35:12 GMT"}],"update_date":"2021-08-11"}
{"id":"2104.08751","submitter":"Dominik K\\\"oppl","authors":"Tomohiro I and Dominik K\\\"oppl","title":"Load-Balancing Succinct B Trees","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We propose a B tree representation storing $n$ keys, each of $k$ bits, in\neither (a) $nk + O(nk / \\lg n)$ bits or (b) $nk + O(nk \\lg \\lg n/ \\lg n)$ bits\nof space supporting all B tree operations in either (a) $O(\\lg n )$ time or (b)\n$O(\\lg n / \\lg \\lg n)$ time, respectively. We can augment each node with an\naggregate value such as the minimum value within its subtree, and maintain\nthese aggregate values within the same space and time complexities. Finally, we\ngive the sparse suffix tree as an application, and present a linear-time\nalgorithm computing the sparse longest common prefix array from the suffix AVL\ntree of Irving et al. [JDA'2003].\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:22:29 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08752","submitter":"Jos\\'e Luis Gonz\\'alez","authors":"Patricio Gallardo, Jos\\'e Luis Gonz\\'alez and Evangelos Routis","title":"The Fulton-MacPherson compactification is not a Mori dream space","comments":"17 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AG","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  We show that the Fulton-MacPherson compactification of the configuration\nspace of $n$ distinct labeled points in certain varieties of arbitrary\ndimension $d$, including projective space, is not a Mori dream space for $n$\nlarger than $d+8$.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:27:18 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08753","submitter":"Shima Bab Hadiashar","authors":"Anurag Anshu, Shima Bab Hadiashar, Rahul Jain, Ashwin Nayak, Dave\n  Touchette","title":"One-shot quantum state redistribution and quantum Markov chains","comments":"26 pages, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph cs.IT math.IT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We revisit the task of quantum state redistribution in the one-shot setting,\nand design a protocol for this task with communication cost in terms of a\nmeasure of distance from quantum Markov chains. More precisely, the distance is\ndefined in terms of quantum max-relative entropy and quantum hypothesis testing\nentropy.\n  Our result is the first to operationally connect quantum state redistribution\nand quantum Markov chains, and can be interpreted as an operational\ninterpretation for a possible one-shot analogue of quantum conditional mutual\ninformation. The communication cost of our protocol is lower than all\npreviously known ones and asymptotically achieves the well-known rate of\nquantum conditional mutual information. Thus, our work takes a step towards the\nimportant open question of near-optimal characterization of the one-shot\nquantum state redistribution.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:34:22 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08754","submitter":"Xiao Zhang","authors":"M. Ablikim, M. N. Achasov, P. Adlarson, S. Ahmed, M. Albrecht, R.\n  Aliberti, A. Amoroso, M. R. An, Q. An, X. H. Bai, Y. Bai, O. Bakina, R.\n  Baldini Ferroli, I. Balossino, Y. Ban, K. Begzsuren, N. Berger, M. Bertani,\n  D. Bettoni, F. Bianchi, J. Bloms, A. Bortone, I. Boyko, R. A. Briere, H. Cai,\n  X. Cai, A. Calcaterra, G. F. Cao, N. Cao, S. A. Cetin, J. F. Chang, W. L.\n  Chang, G. Chelkov, D. Y. Chen, G. Chen, H. S. Chen, M. L. Chen, S. J. Chen,\n  X. R. Chen, Y. B. Chen, Z. J Chen, W. S. Cheng, G. Cibinetto, F. Cossio, X.\n  F. Cui, H. L. Dai, X. C. Dai, A. Dbeyssi, R. E. de Boer, D. Dedovich, Z. Y.\n  Deng, A. Denig, I. Denysenko, M. Destefanis, F. De Mori, Y. Ding, C. Dong, J.\n  Dong, L. Y. Dong, M. Y. Dong, X. Dong, S. X. Du, Y. L. Fan, J. Fang, S. S.\n  Fang, Y. Fang, R. Farinelli, L. Fava, F. Feldbauer, G. Felici, C. Q. Feng, J.\n  H. Feng, M. Fritsch, C. D. Fu, Y. Gao, Y. Gao, Y. Gao, Y. G. Gao, I. Garzia,\n  P. T. Ge, C. Geng, E. M. Gersabeck, A Gilman, K. Goetzen, L. Gong, W. X.\n  Gong, W. Gradl, M. Greco, L. M. Gu, M. H. Gu, Y. T. Gu, C. Y Guan, A. Q. Guo,\n  L. B. Guo, R. P. Guo, Y. P. Guo, A. Guskov, T. T. Han, W. Y. Han, X. Q. Hao,\n  F. A. Harris, K. L. He, F. H. Heinsius, C. H. Heinz, T. Held, Y. K. Heng, C.\n  Herold, M. Himmelreich, T. Holtmann, G. Y. Hou, Y. R. Hou, Z. L. Hou, H. M.\n  Hu, J. F. Hu, T. Hu, Y. Hu, G. S. Huang, L. Q. Huang, X. T. Huang, Y. P.\n  Huang, Z. Huang, T. Hussain, N H\\\"usken, W. Ikegami Andersson, W. Imoehl, M.\n  Irshad, S. Jaeger, S. Janchiv, Q. Ji, Q. P. Ji, X. B. Ji, X. L. Ji, Y. Y. Ji,\n  H. B. Jiang, X. S. Jiang, J. B. Jiao, Z. Jiao, S. Jin, Y. Jin, M. Q. Jing, T.\n  Johansson, N. Kalantar-Nayestanaki, X. S. Kang, R. Kappert, M. Kavatsyuk, B.\n  C. Ke, I. K. Keshk, A. Khoukaz, P. Kiese, R. Kiuchi, R. Kliemt, L. Koch, O.\n  B. Kolcu, B. Kopf, M. Kuemmel, M. Kuessner, A. Kupsc, M. G. Kurth, W. K\\\"uhn,\n  J. J. Lane, J. S. Lange, P. Larin, A. Lavania, L. Lavezzi, Z. H. Lei, H.\n  Leithoff, M. Lellmann, T. Lenz, C. Li, C. H. Li, Cheng Li, D. M. Li, F. Li,\n  G. Li, H. Li, H. Li, H. B. Li, H. J. Li, J. L. Li, J. Q. Li, J. S. Li, Ke Li,\n  L. K. Li, Lei Li, P. R. Li, S. Y. Li, W. D. Li, W. G. Li, X. H. Li, X. L. Li,\n  Xiaoyu Li, Z. Y. Li, H. Liang, H. Liang, H. Liang, Y. F. Liang, Y. T. Liang,\n  G. R. Liao, L. Z. Liao, J. Libby, C. X. Lin, B. J. Liu, C. X. Liu, D. Liu, F.\n  H. Liu, Fang Liu, Feng Liu, H. B. Liu, H. M. Liu, Huanhuan Liu, Huihui Liu,\n  J. B. Liu, J. L. Liu, J. Y. Liu, K. Liu, K. Y. Liu, L. Liu, M. H. Liu, P. L.\n  Liu, Q. Liu, Q. Liu, S. B. Liu, Shuai Liu, T. Liu, W. M. Liu, X. Liu, Y. Liu,\n  Y. B. Liu, Z. A. Liu, Z. Q. Liu, X. C. Lou, F. X. Lu, H. J. Lu, J. D. Lu, J.\n  G. Lu, X. L. Lu, Y. Lu, Y. P. Lu, C. L. Luo, M. X. Luo, P. W. Luo, T. Luo, X.\n  L. Luo, X. R. Lyu, F. C. Ma, H. L. Ma, L. L. Ma, M. M. Ma, Q. M. Ma, R. Q.\n  Ma, R. T. Ma, X. X. Ma, X. Y. Ma, F. E. Maas, M. Maggiora, S. Maldaner, S.\n  Malde, Q. A. Malik, A. Mangoni, Y. J. Mao, Z. P. Mao, S. Marcello, Z. X.\n  Meng, J. G. Messchendorp, G. Mezzadri, T. J. Min, R. E. Mitchell, X. H. Mo,\n  Y. J. Mo, N. Yu. Muchnoi, H. Muramatsu, S. Nakhoul, Y. Nefedov, F. Nerling,\n  I. B. Nikolaev, Z. Ning, S. Nisar, Q. Ouyang, S. Pacetti, X. Pan, Y. Pan, A.\n  Pathak, A. Pathak, P. Patteri, M. Pelizaeus, H. P. Peng, K. Peters, J.\n  Pettersson, J. L. Ping, R. G. Ping, S. Pogodin, R. Poling, V. Prasad, H. Qi,\n  H. R. Qi, K. H. Qi, M. Qi, T. Y. Qi, S. Qian, W. B. Qian, Z. Qian, C. F.\n  Qiao, L. Q. Qin, X. P. Qin, X. S. Qin, Z. H. Qin, J. F. Qiu, S. Q. Qu, K. H.\n  Rashid, K. Ravindran, C. F. Redmer, A. Rivetti, V. Rodin, M. Rolo, G. Rong,\n  Ch. Rosner, M. Rump, H. S. Sang, A. Sarantsev, Y. Schelhaas, C. Schnier, K.\n  Schoenning, M. Scodeggio, D. C. Shan, W. Shan, X. Y. Shan, J. F. Shangguan,\n  M. Shao, C. P. Shen, H. F. Shen, P. X. Shen, X. Y. Shen, H. C. Shi, R. S.\n  Shi, X. Shi, X. D Shi, J. J. Song, W. M. Song, Y. X. Song, S. Sosio, S.\n  Spataro, K. X. Su, P. P. Su, F. F. Sui, G. X. Sun, H. K. Sun, J. F. Sun, L.\n  Sun, S. S. Sun, T. Sun, W. Y. Sun, W. Y. Sun, X Sun, Y. J. Sun, Y. K. Sun, Y.\n  Z. Sun, Z. T. Sun, Y. H. Tan, Y. X. Tan, C. J. Tang, G. Y. Tang, J. Tang, J.\n  X. Teng, V. Thoren, W. H. Tian, Y. T. Tian, I. Uman, B. Wang, C. W. Wang, D.\n  Y. Wang, H. J. Wang, H. P. Wang, K. Wang, L. L. Wang, M. Wang, M. Z. Wang,\n  Meng Wang, W. Wang, W. H. Wang, W. P. Wang, X. Wang, X. F. Wang, X. L. Wang,\n  Y. Wang, Y. Wang, Y. D. Wang, Y. F. Wang, Y. Q. Wang, Y. Y. Wang, Z. Wang, Z.\n  Y. Wang, Ziyi Wang, Zongyuan Wang, D. H. Wei, F. Weidner, S. P. Wen, D. J.\n  White, U. Wiedner, G. Wilkinson, M. Wolke, L. Wollenberg, J. F. Wu, L. H. Wu,\n  L. J. Wu, X. Wu, Z. Wu, L. Xia, H. Xiao, S. Y. Xiao, Z. J. Xiao, X. H. Xie,\n  Y. G. Xie, Y. H. Xie, T. Y. Xing, G. F. Xu, Q. J. Xu, W. Xu, X. P. Xu, Y. C.\n  Xu, F. Yan, L. Yan, W. B. Yan, W. C. Yan, Xu Yan, H. J. Yang, H. X. Yang, L.\n  Yang, S. L. Yang, Y. X. Yang, Yifan Yang, Zhi Yang, M. Ye, M. H. Ye, J. H.\n  Yin, Z. Y. You, B. X. Yu, C. X. Yu, G. Yu, J. S. Yu, T. Yu, C. Z. Yuan, L.\n  Yuan, X. Q. Yuan, Y. Yuan, Z. Y. Yuan, C. X. Yue, A. A. Zafar, X. Zeng Zeng,\n  Y. Zeng, A. Q. Zhang, B. X. Zhang, Guangyi Zhang, H. Zhang, H. H. Zhang, H.\n  H. Zhang, H. Y. Zhang, J. J. Zhang, J. L. Zhang, J. Q. Zhang, J. W. Zhang, J.\n  Y. Zhang, J. Z. Zhang, Jianyu Zhang, Jiawei Zhang, L. M. Zhang, L. Q. Zhang,\n  Lei Zhang, S. Zhang, S. F. Zhang, Shulei Zhang, X. Zhang, X. D. Zhang, X. Y.\n  Zhang, Y. Zhang, Y. T. Zhang, Y. H. Zhang, Yan Zhang, Yao Zhang, Z. H. Zhang,\n  Z. Y. Zhang, G. Zhao, J. Zhao, J. Y. Zhao, J. Z. Zhao, Lei Zhao, Ling Zhao,\n  M. G. Zhao, Q. Zhao, S. J. Zhao, Y. B. Zhao, Y. X. Zhao, Z. G. Zhao, A.\n  Zhemchugov, B. Zheng, J. P. Zheng, Y. Zheng, Y. H. Zheng, B. Zhong, C. Zhong,\n  L. P. Zhou, Q. Zhou, X. Zhou, X. K. Zhou, X. R. Zhou, X. Y. Zhou, A. N. Zhu,\n  J. Zhu, K. Zhu, K. J. Zhu, S. H. Zhu, T. J. Zhu, W. J. Zhu, W. J. Zhu, Y. C.\n  Zhu, Z. A. Zhu, B. S. Zou, J. H. Zou","title":"Observation of a near-threshold enhancement in the\n  $\\Lambda\\bar{\\Lambda}$ mass spectrum from $e^+e^-\\to\\phi\\Lambda\\bar{\\Lambda}$\n  at $\\sqrt{s}$ from 3.51 to 4.60 GeV","comments":null,"journal-ref":"Phys. Rev. D 104, 052006 (2021)","doi":"10.1103/PhysRevD.104.052006","report-no":null,"categories":"hep-ex","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The process $e^+ e^- \\to \\phi \\Lambda \\bar{\\Lambda}$ is studied using data\nsamples collected with the BESIII detector at the BEPCII collider at\ncenter-of-mass energies $\\sqrt{s}$ ranging from $3.51$ to $4.60~{\\rm GeV}$ . An\nintermediate resonance structure is observed near the threshold of $\\Lambda\n\\bar{\\Lambda}$. It has a mass of $(2262 \\pm 4 \\pm 28)~{\\rm{MeV}}/c^{2}$ and a\nwidth of $(72 \\pm 5 \\pm 43)~\\rm{MeV}$, where the quoted uncertainties are\nstatistical and systematic, respectively. The $J^{PC}$ quantum numbers of\n$0^{-+}$ and $0^{++}$ are rejected, while other $J^{PC}$ hypotheses are\npossible, according to the helicity angle study. The energy-dependent cross\nsection of the $e^+ e^- \\to \\phi \\Lambda \\bar{\\Lambda}$ process is measured for\nthe first time in this energy region, and contributions from excited $\\psi$\nstates and vector charmonium-like $Y$-states are investigated.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:35:05 GMT"},{"version":"v2","created":"Sat, 4 Sep 2021 00:34:52 GMT"}],"update_date":"2021-09-22"}
{"id":"2104.08755","submitter":"Zhaohao Zeng","authors":"Zhaohao Zeng and Tetsuya Sakai","title":"DCH-2: A Parallel Customer-Helpdesk Dialogue Corpus with Distributions\n  of Annotators' Labels","comments":"6 pages, 3 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.IR","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  We introduce a data set called DCH-2, which contains 4,390 real\ncustomer-helpdesk dialogues in Chinese and their English translations. DCH-2\nalso contains dialogue-level annotations and turn-level annotations obtained\nindependently from either 19 or 20 annotators. The data set was built through\nour effort as organisers of the NTCIR-14 Short Text Conversation and NTCIR-15\nDialogue Evaluation tasks, to help researchers understand what constitutes an\neffective customer-helpdesk dialogue, and thereby build efficient and helpful\nhelpdesk systems that are available to customers at all times. In addition,\nDCH-2 may be utilised for other purposes, for example, as a repository for\nretrieval-based dialogue systems, or as a parallel corpus for machine\ntranslation in the helpdesk domain.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:35:15 GMT"},{"version":"v2","created":"Sun, 30 May 2021 15:32:47 GMT"}],"update_date":"2021-06-01"}
{"id":"2104.08756","submitter":"Jinbang Yang","authors":"Xin Lu and Jinbang Yang and Kang Zuo","title":"Strict Arakelov inequality for a family of varieties of general type","comments":"17 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Let $f:\\, X\\to Y$ be a semistable non-isotrivial family of $n$-folds over a\nsmooth projective curve with discriminant locus $S \\subseteq Y$ and with\ngeneral fibre $F$ of general type. We show the strict Arakelov inequality\n\\[\\frac{\\mathrm{deg}\\, f_*\\omega_{X/Y}^\\nu}{\\mathrm{rank}\\,\nf_*\\omega_{X/Y}^\\nu} < {n\\nu\\over 2}\\cdot\\mathrm{deg}\\,\\Omega^1_Y(\\log S),\\]\nfor all $\\nu\\in \\mathbb N$ such that the $\\nu$-th pluricanonical linear system\n$|\\omega^\\nu_F|$ is birational. This answers a question asked by M\\\"oller,\nViehweg and the third named author.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:37:31 GMT"},{"version":"v2","created":"Tue, 26 Oct 2021 08:58:18 GMT"},{"version":"v3","created":"Thu, 6 Jan 2022 04:18:29 GMT"}],"update_date":"2022-01-07"}
{"id":"2104.08757","submitter":"Guanhua Chen","authors":"Guanhua Chen, Shuming Ma, Yun Chen, Li Dong, Dongdong Zhang, Jia Pan,\n  Wenping Wang, Furu Wei","title":"Zero-shot Cross-lingual Transfer of Neural Machine Translation with\n  Multilingual Pretrained Encoders","comments":"Accepted to EMNLP 2021. Code is available\n  [here](https://github.com/ghchen18/emnlp2021-sixt)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Previous work mainly focuses on improving cross-lingual transfer for NLU\ntasks with a multilingual pretrained encoder (MPE), or improving the\nperformance on supervised machine translation with BERT. However, it is\nunder-explored that whether the MPE can help to facilitate the cross-lingual\ntransferability of NMT model. In this paper, we focus on a zero-shot\ncross-lingual transfer task in NMT. In this task, the NMT model is trained with\nparallel dataset of only one language pair and an off-the-shelf MPE, then it is\ndirectly tested on zero-shot language pairs. We propose SixT, a simple yet\neffective model for this task. SixT leverages the MPE with a two-stage training\nschedule and gets further improvement with a position disentangled encoder and\na capacity-enhanced decoder. Using this method, SixT significantly outperforms\nmBART, a pretrained multilingual encoder-decoder model explicitly designed for\nNMT, with an average improvement of 7.1 BLEU on zero-shot any-to-English test\nsets across 14 source languages. Furthermore, with much less training\ncomputation cost and training data, our model achieves better performance on 15\nany-to-English test sets than CRISS and m2m-100, two strong multilingual NMT\nbaselines.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:42:45 GMT"},{"version":"v2","created":"Fri, 5 Nov 2021 07:33:11 GMT"}],"update_date":"2021-11-08"}
{"id":"2104.08758","submitter":"Ana Marasovi\\'c","authors":"Jesse Dodge, Maarten Sap, Ana Marasovi\\'c, William Agnew, Gabriel\n  Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner","title":"Documenting Large Webtext Corpora: A Case Study on the Colossal Clean\n  Crawled Corpus","comments":"EMNLP 2021 accepted paper camera ready version","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large language models have led to remarkable progress on many NLP tasks, and\nresearchers are turning to ever-larger text corpora to train them. Some of the\nlargest corpora available are made by scraping significant portions of the\ninternet, and are frequently introduced with only minimal documentation. In\nthis work we provide some of the first documentation for the Colossal Clean\nCrawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set\nof filters to a single snapshot of Common Crawl. We begin by investigating\nwhere the data came from, and find a significant amount of text from unexpected\nsources like patents and US military websites. Then we explore the content of\nthe text itself, and find machine-generated text (e.g., from machine\ntranslation systems) and evaluation examples from other benchmark NLP datasets.\nTo understand the impact of the filters applied to create this dataset, we\nevaluate the text that was removed, and show that blocklist filtering\ndisproportionately removes text from and about minority individuals. Finally,\nwe conclude with some recommendations for how to created and document web-scale\ndatasets from a scrape of the internet.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:42:52 GMT"},{"version":"v2","created":"Thu, 30 Sep 2021 17:20:01 GMT"}],"update_date":"2021-10-01"}
{"id":"2104.08759","submitter":"Ofir Gordon","authors":"Ofir Gordon, Yuval Filmus, Oren Salzman","title":"Revisiting the Complexity Analysis of Conflict-Based Search: New\n  Computational Techniques and Improved Bounds","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.MA cs.AI cs.CC cs.RO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The problem of Multi-Agent Path Finding (MAPF) calls for finding a set of\nconflict-free paths for a fleet of agents operating in a given environment.\nArguably, the state-of-the-art approach to computing optimal solutions is\nConflict-Based Search (CBS). In this work we revisit the complexity analysis of\nCBS to provide tighter bounds on the algorithm's run-time in the worst-case.\nOur analysis paves the way to better pinpoint the parameters that govern (in\nthe worst case) the algorithm's computational complexity.\n  Our analysis is based on two complementary approaches: In the first approach\nwe bound the run-time using the size of a Multi-valued Decision Diagram (MDD)\n-- a layered graph which compactly contains all possible single-agent paths\nbetween two given vertices for a specific path length.\n  In the second approach we express the running time by a novel recurrence\nrelation which bounds the algorithm's complexity. We use generating\nfunctions-based analysis in order to tightly bound the recurrence.\n  Using these technique we provide several new upper-bounds on CBS's\ncomplexity. The results allow us to improve the existing bound on the running\ntime of CBS for many cases. For example, on a set of common benchmarks we\nimprove the upper-bound by a factor of at least $2^{10^{7}}$.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:46:28 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08760","submitter":"Guangrun Wang","authors":"Guangrun Wang, Keze Wang, Guangcong Wang, Philip H.S. Torr, Liang Lin","title":"Solving Inefficiency of Self-supervised Representation Learning","comments":"ICCV 2021 paper, oral presentation","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Self-supervised learning (especially contrastive learning) has attracted\ngreat interest due to its huge potential in learning discriminative\nrepresentations in an unsupervised manner. Despite the acknowledged successes,\nexisting contrastive learning methods suffer from very low learning efficiency,\ne.g., taking about ten times more training epochs than supervised learning for\ncomparable recognition accuracy. In this paper, we reveal two contradictory\nphenomena in contrastive learning that we call under-clustering and\nover-clustering problems, which are major obstacles to learning efficiency.\nUnder-clustering means that the model cannot efficiently learn to discover the\ndissimilarity between inter-class samples when the negative sample pairs for\ncontrastive learning are insufficient to differentiate all the actual object\nclasses. Over-clustering implies that the model cannot efficiently learn\nfeatures from excessive negative sample pairs, forcing the model to\nover-cluster samples of the same actual classes into different clusters. To\nsimultaneously overcome these two problems, we propose a novel self-supervised\nlearning framework using a truncated triplet loss. Precisely, we employ a\ntriplet loss tending to maximize the relative distance between the positive\npair and negative pairs to address the under-clustering problem; and we\nconstruct the negative pair by selecting a negative sample deputy from all\nnegative samples to avoid the over-clustering problem, guaranteed by the\nBernoulli Distribution model. We extensively evaluate our framework in several\nlarge-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results\ndemonstrate our model's superiority (e.g., the learning efficiency) over the\nlatest state-of-the-art methods by a clear margin. Codes available at:\nhttps://github.com/wanggrun/triplet .\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:47:10 GMT"},{"version":"v2","created":"Tue, 22 Jun 2021 10:30:49 GMT"},{"version":"v3","created":"Thu, 21 Oct 2021 10:19:10 GMT"}],"update_date":"2021-10-22"}
{"id":"2104.08761","submitter":"Shaoning Li","authors":"Yipeng Ji, Jingyi Wang, Shaoning Li, Yangyang Li, Shenwen Lin, Xiong\n  Li","title":"An Anomaly Event Detection Method Based on GNN Algorithm for Multi-data\n  Sources","comments":"Accepted by BSCI 2021","journal-ref":null,"doi":"10.1145/3457337.3457846","report-no":null,"categories":"cs.SI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Anomaly event detection is crucial for critical infrastructure\nsecurity(transportation system, social-ecological sector, insurance service,\ngovernment sector etc.) due to its ability to reveal and address the potential\ncyber-threats in advance by analysing the data(messages, microblogs, logs etc.)\nfrom digital systems and networks. However, the convenience and applicability\nof smart devices and the maturity of connected technology make the social\nanomaly events data multi-source and dynamic, which result in the\ninadaptability for multi-source data detection and thus affect the critical\ninfrastructure security. To effectively address the proposed problems, in this\npaper, we design a novel anomaly detection method based on multi-source data.\nFirst, we leverage spectral clustering algorithm for feature extraction and\nfusion of multiple data sources. Second, by harnessing the power of deep graph\nneural network(Deep-GNN), we perform a fine-gained anomaly social event\ndetection, revealing the threatening events and guarantee the critical\ninfrastructure security. Experimental results demonstrate that our framework\noutperforms other baseline anomaly event detection methods and shows high\ntracking accuracy, strong robustness and stability.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:50:30 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08762","submitter":"Rajarshi Das","authors":"Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez,\n  Jay-Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew McCallum","title":"Case-based Reasoning for Natural Language Queries over Knowledge Bases","comments":"EMNLP 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  It is often challenging to solve a complex problem from scratch, but much\neasier if we can access other similar problems with their solutions -- a\nparadigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR\napproach (CBR-KBQA) for question answering over large knowledge bases. CBR-KBQA\nconsists of a nonparametric memory that stores cases (question and logical\nforms) and a parametric model that can generate a logical form for a new\nquestion by retrieving cases that are relevant to it. On several KBQA datasets\nthat contain complex questions, CBR-KBQA achieves competitive performance. For\nexample, on the ComplexWebQuestions dataset, CBR-KBQA outperforms the current\nstate of the art by 11\\% on accuracy. Furthermore, we show that CBR-KBQA is\ncapable of using new cases \\emph{without} any further training: by\nincorporating a few human-labeled examples in the case memory, CBR-KBQA is able\nto successfully generate logical forms containing unseen KB entities as well as\nrelations.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:50:31 GMT"},{"version":"v2","created":"Sun, 7 Nov 2021 23:46:12 GMT"}],"update_date":"2021-11-09"}
{"id":"2104.08763","submitter":"Shunsuke Kitada","authors":"Shunsuke Kitada, Hitoshi Iyatomi","title":"Making Attention Mechanisms More Robust and Interpretable with Virtual\n  Adversarial Training","comments":"18 pages, 3 figures. Accepted for publication in Springer Applied\n  Intelligence (APIN)","journal-ref":"Applied Intelligence, Springer, 2022","doi":"10.1007/s10489-022-04301-w","report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Although attention mechanisms have become fundamental components of deep\nlearning models, they are vulnerable to perturbations, which may degrade the\nprediction performance and model interpretability. Adversarial training (AT)\nfor attention mechanisms has successfully reduced such drawbacks by considering\nadversarial perturbations. However, this technique requires label information,\nand thus, its use is limited to supervised settings. In this study, we explore\nthe concept of incorporating virtual AT (VAT) into the attention mechanisms, by\nwhich adversarial perturbations can be computed even from unlabeled data. To\nrealize this approach, we propose two general training techniques, namely VAT\nfor attention mechanisms (Attention VAT) and \"interpretable\" VAT for attention\nmechanisms (Attention iVAT), which extend AT for attention mechanisms to a\nsemi-supervised setting. In particular, Attention iVAT focuses on the\ndifferences in attention; thus, it can efficiently learn clearer attention and\nimprove model interpretability, even with unlabeled data. Empirical experiments\nbased on six public datasets revealed that our techniques provide better\nprediction performance than conventional AT-based as well as VAT-based\ntechniques, and stronger agreement with evidence that is provided by humans in\ndetecting important words in sentences. Moreover, our proposal offers these\nadvantages without needing to add the careful selection of unlabeled data. That\nis, even if the model using our VAT-based technique is trained on unlabeled\ndata from a source other than the target task, both the prediction performance\nand model interpretability can be improved.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:51:45 GMT"},{"version":"v2","created":"Fri, 28 Oct 2022 04:20:35 GMT"},{"version":"v3","created":"Mon, 26 Dec 2022 01:47:10 GMT"}],"update_date":"2022-12-27"}
{"id":"2104.08764","submitter":"Dachao Lin","authors":"Dachao Lin, Haishan Ye, Zhihua Zhang","title":"Explicit Convergence Rates of Greedy and Random Quasi-Newton Methods","comments":"Fix some typos, 40 pages, 3 figures, final version","journal-ref":null,"doi":null,"report-no":null,"categories":"math.OC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Optimization is important in machine learning problems, and quasi-Newton\nmethods have a reputation as the most efficient numerical schemes for smooth\nunconstrained optimization. In this paper, we consider the explicit superlinear\nconvergence rates of quasi-Newton methods and address two open problems\nmentioned by Rodomanov and Nesterov. First, we extend Rodomanov and Nesterov's\nresults to random quasi-Newton methods, which include common DFP, BFGS, SR1\nmethods. Such random methods adopt a random direction for updating the\napproximate Hessian matrix in each iteration. Second, we focus on the specific\nquasi-Newton methods: SR1 and BFGS methods. We provide improved versions of\ngreedy and random methods with provable better explicit (local) superlinear\nconvergence rates. Our analysis is closely related to the approximation of a\ngiven Hessian matrix, unconstrained quadratic objective, as well as the general\nstrongly convex, smooth, and strongly self-concordant functions.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 07:54:23 GMT"},{"version":"v2","created":"Sat, 23 Oct 2021 12:51:15 GMT"},{"version":"v3","created":"Mon, 1 Nov 2021 13:42:35 GMT"},{"version":"v4","created":"Sun, 20 Mar 2022 13:16:29 GMT"},{"version":"v5","created":"Sun, 11 Sep 2022 02:48:46 GMT"}],"update_date":"2022-09-13"}
{"id":"2104.08765","submitter":"Aman Madaan","authors":"Aman Madaan, Niket Tandon, Dheeraj Rajagopal, Yiming Yang, Peter\n  Clark, Keisuke Sakaguchi, Ed Hovy","title":"Improving Neural Model Performance through Natural Language Feedback on\n  Their Explanations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  A class of explainable NLP models for reasoning tasks support their decisions\nby generating free-form or structured explanations, but what happens when these\nsupporting structures contain errors? Our goal is to allow users to\ninteractively correct explanation structures through natural language feedback.\nWe introduce MERCURIE - an interactive system that refines its explanations for\na given reasoning task by getting human feedback in natural language. Our\napproach generates graphs that have 40% fewer inconsistencies as compared with\nthe off-the-shelf system. Further, simply appending the corrected explanation\nstructures to the output leads to a gain of 1.2 points on accuracy on\ndefeasible reasoning across all three domains. We release a dataset of over\n450k graphs for defeasible reasoning generated by our system at\nhttps://tinyurl.com/mercurie .\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 08:10:01 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08766","submitter":"Koki Ono","authors":"Koki Ono, Toshiya Higomoto, Yugo Saito, Shun Uchino, Yusuke Nishida,\n  Yoshiro Takahashi","title":"Observation of spin-space quantum transport induced by an atomic quantum\n  point contact","comments":"16 pages, 9 figures","journal-ref":"Nature Communications 12, 6724 (2021)","doi":"10.1038/s41467-021-27011-2","report-no":null,"categories":"cond-mat.quant-gas","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Quantum transport is ubiquitous in physics. So far, quantum transport between\nterminals has been extensively studied in solid state systems from the\nfundamental point of views such as the quantized conductance to the\napplications to quantum devices. Recent works have demonstrated a cold-atom\nanalog of a mesoscopic conductor by engineering a narrow conducting channel\nwith optical potentials, which opens the door for a wealth of research of\natomtronics emulating mesoscopic electronic devices and beyond. Here we realize\nan alternative scheme of the quantum transport experiment with ytterbium atoms\nin a two-orbital optical lattice system. Our system consists of a\nmulti-component Fermi gas and a localized impurity, where the current can be\ncreated in the spin space by introducing the spin-dependent interaction with\nthe impurity. We demonstrate a rich variety of localized-impurity-induced\nquantum transports, which paves the way for atomtronics exploiting spin degrees\nof freedom.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 08:11:19 GMT"},{"version":"v2","created":"Tue, 14 Dec 2021 07:41:13 GMT"}],"update_date":"2021-12-15"}
{"id":"2104.08767","submitter":"Jinhuan Wang","authors":"Jinhuan Wang and Pengtao Chen and Shanqing Yu and Qi Xuan","title":"TSGN: Transaction Subgraph Networks for Identifying Ethereum Phishing\n  Accounts","comments":"14 pages, 2 fihures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CR cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Blockchain technology and, in particular, blockchain-based transaction offers\nus information that has never been seen before in the financial world. In\ncontrast to fiat currencies, transactions through virtual currencies like\nBitcoin are completely public. And these transactions of cryptocurrencies are\npermanently recorded on Blockchain and are available at any time. Therefore,\nthis allows us to build transaction networks (TN) to analyze illegal\nphenomenons such as phishing scams in blockchain from a network perspective. In\nthis paper, we propose a Transaction SubGraph Network (TSGN) based\nclassification model to identify phishing accounts in Ethereum. Firstly we\nextract transaction subgraphs for each address and then expand these subgraphs\ninto corresponding TSGNs based on the different mapping mechanisms. We find\nthat TSGNs can provide more potential information to benefit the identification\nof phishing accounts. Moreover, Directed-TSGNs, by introducing direction\nattributes, can retain the transaction flow information that captures the\nsignificant topological pattern of phishing scams. By comparing with the TSGN,\nDirected-TSGN indeed has much lower time complexity, benefiting the graph\nrepresentation learning. Experimental results demonstrate that, combined with\nnetwork representation algorithms, the TSGN model can capture more features to\nenhance the classification algorithm and improve phishing nodes' identification\naccuracy in the Ethereum networks.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 08:12:51 GMT"},{"version":"v2","created":"Tue, 20 Apr 2021 13:48:30 GMT"}],"update_date":"2021-04-21"}
{"id":"2104.08768","submitter":"Richard Shin","authors":"Richard Shin, Christopher H. Lin, Sam Thomson, Charles Chen, Subhro\n  Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner,\n  Benjamin Van Durme","title":"Constrained Language Models Yield Few-Shot Semantic Parsers","comments":"EMNLP 2021. Code is available at\n  https://github.com/microsoft/semantic_parsing_with_constrained_lm","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We explore the use of large pretrained language models as few-shot semantic\nparsers. The goal in semantic parsing is to generate a structured meaning\nrepresentation given a natural language input. However, language models are\ntrained to generate natural language. To bridge the gap, we use language models\nto paraphrase inputs into a controlled sublanguage resembling English that can\nbe automatically mapped to a target meaning representation. Our results\ndemonstrate that with only a small amount of data and very little code to\nconvert into English-like representations, our blueprint for rapidly\nbootstrapping semantic parsers leads to surprisingly effective performance on\nmultiple community tasks, greatly exceeding baseline methods also trained on\nthe same limited data.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 08:13:06 GMT"},{"version":"v2","created":"Tue, 16 Nov 2021 23:33:56 GMT"}],"update_date":"2021-11-18"}
{"id":"2104.08769","submitter":"Ziqian Zeng","authors":"Ziqian Zeng, Rashidul Islam, Kamrun Naher Keya, James Foulds, Yangqiu\n  Song, Shimei Pan","title":"Fair Representation Learning for Heterogeneous Information Networks","comments":"Accepted at ICWSM 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Recently, much attention has been paid to the societal impact of AI,\nespecially concerns regarding its fairness. A growing body of research has\nidentified unfair AI systems and proposed methods to debias them, yet many\nchallenges remain. Representation learning for Heterogeneous Information\nNetworks (HINs), a fundamental building block used in complex network mining,\nhas socially consequential applications such as automated career counseling,\nbut there have been few attempts to ensure that it will not encode or amplify\nharmful biases, e.g. sexism in the job market. To address this gap, in this\npaper we propose a comprehensive set of de-biasing methods for fair HINs\nrepresentation learning, including sampling-based, projection-based, and graph\nneural networks (GNNs)-based techniques. We systematically study the behavior\nof these algorithms, especially their capability in balancing the trade-off\nbetween fairness and prediction accuracy. We evaluate the performance of the\nproposed methods in an automated career counseling application where we\nmitigate gender bias in career recommendation. Based on the evaluation results\non two datasets, we identify the most effective fair HINs representation\nlearning techniques under different conditions.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 08:28:18 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08770","submitter":"Daniel Cizma","authors":"Daniel Cizma and Nati Linial","title":"Irreducible Non-Metrizable Path Systems in Graphs","comments":"7 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  A path system $\\mathcal{P}$ in a graph $G=(V,E)$ is said to be irreducible if\nthere does not exist a partition $V= A\\sqcup B$ such that $\\mathcal{P}$\nrestricts to a path system on both $G[A]$ and $G[B]$. In this paper, we\nconstruct an infinite family of non-metrizable irreducible path systems defined\non certain Paley graphs.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 08:39:44 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08771","submitter":"Mozhdeh Gheini","authors":"Mozhdeh Gheini, Xiang Ren, Jonathan May","title":"Cross-Attention is All You Need: Adapting Pretrained Transformers for\n  Machine Translation","comments":"Accepted to EMNLP 2021 Main Conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We study the power of cross-attention in the Transformer architecture within\nthe context of transfer learning for machine translation, and extend the\nfindings of studies into cross-attention when training from scratch. We conduct\na series of experiments through fine-tuning a translation model on data where\neither the source or target language has changed. These experiments reveal that\nfine-tuning only the cross-attention parameters is nearly as effective as\nfine-tuning all parameters (i.e., the entire translation model). We provide\ninsights into why this is the case and observe that limiting fine-tuning in\nthis manner yields cross-lingually aligned embeddings. The implications of this\nfinding for researchers and practitioners include a mitigation of catastrophic\nforgetting, the potential for zero-shot translation, and the ability to extend\nmachine translation models to several new language pairs with reduced parameter\nstorage overhead.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 08:41:01 GMT"},{"version":"v2","created":"Tue, 14 Sep 2021 16:07:09 GMT"}],"update_date":"2021-09-15"}
{"id":"2104.08772","submitter":"Xi Tong","authors":"Chon Man Sou, Xi Tong, and Yi Wang","title":"Chemical-Potential-Assisted Particle Production in FRW Spacetimes","comments":"48 pages, 27 figures; v2: 49 pages, 27 figures, with minor changes","journal-ref":null,"doi":"10.1007/JHEP06(2021)129","report-no":null,"categories":"hep-th astro-ph.CO gr-qc hep-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We analyze gravitational particle production assisted by chemical potential.\nBy utilizing the uniformly smoothed Stokes-line method and Borel summation, we\ngain insight into the fine-grained history of enhanced particle production.\nAnalytic/semi-analytic formulae describing the production amount, time and\nwidth are obtained for both spin-1 and spin-1/2 particles in various FRW\nspacetimes. Our work also serves as a concrete demonstration of the uniformly\nsmoothed Stokes-line method applied to cosmology.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 08:42:10 GMT"},{"version":"v2","created":"Wed, 14 Jul 2021 09:50:15 GMT"}],"update_date":"2021-07-15"}
{"id":"2104.08773","submitter":"Swaroop Mishra","authors":"Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi","title":"Cross-Task Generalization via Natural Language Crowdsourcing\n  Instructions","comments":"ACL 2022","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.CV cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. Despite the success of the conventional supervised learning on\nindividual datasets, such models often struggle with generalization across\ntasks (e.g., a question-answering system cannot solve classification tasks). A\nlong-standing challenge in AI is to build a model that learns a new task by\nunderstanding the human-readable instructions that define it. To study this, we\nintroduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their\nhuman-authored instructions, and 193k task instances (input-output pairs). The\ninstructions are obtained from crowdsourcing instructions used to create\nexisting NLP datasets and mapped to a unified schema. Using this meta-dataset,\nwe measure cross-task generalization by training models on seen tasks and\nmeasuring generalization to the remaining unseen ones. We adopt generative\npre-trained language models to encode task-specific instructions along with\ninput and generate task output. Our results indicate that models benefit from\ninstructions when evaluated in terms of generalization to unseen tasks (19%\nbetter for models utilizing instructions). These models, however, are far\nbehind an estimated performance upperbound indicating significant room for more\nprogress in this direction.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 08:44:56 GMT"},{"version":"v2","created":"Fri, 3 Sep 2021 21:58:23 GMT"},{"version":"v3","created":"Sat, 16 Oct 2021 05:12:48 GMT"},{"version":"v4","created":"Mon, 14 Mar 2022 09:15:08 GMT"}],"update_date":"2022-03-15"}
{"id":"2104.08774","submitter":"Antonios Liapis","authors":"Theodoros Galanos and Antonios Liapis and Georgios N. Yannakakis and\n  Reinhard Koenig","title":"ARCH-Elites: Quality-Diversity for Urban Design","comments":"Published at Genetic and Evolutionary Computation Conference\n  Companion, 2021, 2 pages, 1 figure","journal-ref":null,"doi":"10.1145/3449726.3459490","report-no":null,"categories":"cs.NE cs.HC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper introduces ARCH-Elites, a MAP-Elites implementation that can\nreconfigure large-scale urban layouts at real-world locations via a pre-trained\nsurrogate model instead of costly simulations. In a series of experiments, we\ngenerate novel urban designs for two real-world locations in Boston,\nMassachusetts. Combining the exploration of a possibility space with real-time\nperformance evaluation creates a powerful new paradigm for architectural\ngenerative design that can extract and articulate design intelligence.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 08:46:38 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08775","submitter":"Nayeon Lee","authors":"Nayeon Lee, Andrea Madotto, Yejin Bang, Pascale Fung","title":"Dynamically Addressing Unseen Rumor via Continual Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.SI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Rumors are often associated with newly emerging events, thus, an ability to\ndeal with unseen rumors is crucial for a rumor veracity classification model.\nPrevious works address this issue by improving the model's generalizability,\nwith an assumption that the model will stay unchanged even after the new\noutbreak of an event. In this work, we propose an alternative solution to\ncontinuously update the model in accordance with the dynamics of rumor domain\ncreations. The biggest technical challenge associated with this new approach is\nthe catastrophic forgetting of previous learnings due to new learnings. We\nadopt continual learning strategies that control the new learnings to avoid\ncatastrophic forgetting and propose an additional strategy that can jointly be\nused to strengthen the forgetting alleviation.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 08:50:10 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08776","submitter":"Hossein Hosseini","authors":"Hossein Hosseini, Hyunsin Park, Sungrack Yun, Christos Louizos, Joseph\n  Soriaga, Max Welling","title":"Federated Learning of User Verification Models Without Sharing\n  Embeddings","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.CR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We consider the problem of training User Verification (UV) models in\nfederated setting, where each user has access to the data of only one class and\nuser embeddings cannot be shared with the server or other users. To address\nthis problem, we propose Federated User Verification (FedUV), a framework in\nwhich users jointly learn a set of vectors and maximize the correlation of\ntheir instance embeddings with a secret linear combination of those vectors. We\nshow that choosing the linear combinations from the codewords of an\nerror-correcting code allows users to collaboratively train the model without\nrevealing their embedding vectors. We present the experimental results for user\nverification with voice, face, and handwriting data and show that FedUV is on\npar with existing approaches, while not sharing the embeddings with other users\nor the server.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 08:51:39 GMT"},{"version":"v2","created":"Mon, 7 Jun 2021 17:32:41 GMT"}],"update_date":"2021-06-08"}
{"id":"2104.08777","submitter":"Nidhi Gupta","authors":"Nidhi Gupta, Wenju Liu","title":"Line Segmentation from Unconstrained Handwritten Text Images using\n  Adaptive Approach","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Line segmentation from handwritten text images is one of the challenging task\ndue to diversity and unknown variations as undefined spaces, styles,\norientations, stroke heights, overlapping, and alignments. Though abundant\nresearches, there is a need of improvement to achieve robustness and higher\nsegmentation rates. In the present work, an adaptive approach is used for the\nline segmentation from handwritten text images merging the alignment of\nconnected component coordinates and text height. The mathematical justification\nis provided for measuring the text height respective to the image size. The\nnovelty of the work lies in the text height calculation dynamically. The\nexperiments are tested on the dataset provided by the Chinese company for the\nproject. The proposed scheme is tested on two different type of datasets;\ndocument pages having base lines and plain pages. Dataset is highly complex and\nconsists of abundant and uncommon variations in handwriting patterns. The\nperformance of the proposed method is tested on our datasets as well as\nbenchmark datasets, namely IAM and ICDAR09 to achieve 98.01% detection rate on\naverage. The performance is examined on the above said datasets to observe\n91.99% and 96% detection rates, respectively.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 08:52:52 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08778","submitter":"Ignacio Negueruela","authors":"Ignacio Negueruela (Alicante), Andr\\'e-Nicolas Chen\\'e (Valpara\\'iso\n  et Gemini-Hilo), Hugo M. Tabernero (Oporto), Ricardo Dorda (IAC), Jura\n  Borissova (Valpara\\'iso), Amparo Marco (Alicante), Radostin Kurtev\n  (Valpara\\'iso)","title":"A massive open cluster hiding in full sight","comments":"12 pages, 10 figures, accepted for publication in MNRAS","journal-ref":"MNRAS 505, 1618 (2021)","doi":"10.1093/mnras/stab1117","report-no":null,"categories":"astro-ph.GA astro-ph.SR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Obscuration and confusion conspire to limit our knowledge of the inner Milky\nWay. Even at moderate distances, the identification of stellar systems becomes\ncompounded by the extremely high density of background sources. Here we provide\na very revealing example of these complications by unveiling a large, massive,\nyoung cluster in the Sagittarius arm that has escaped detection until now\ndespite containing more than 30 stars brighter than $G=13$. By combining Gaia\nDR2 astrometry, Gaia and 2MASS photometry and optical spectroscopy, we find\nthat the new cluster, which we name Valparaiso~1, located at $\\sim2.3\\:$kpc, is\nabout 75~Ma old and includes a large complement of evolved stars, among which\nwe highlight the 4~d classical Cepheid CM~Sct and an M-type giant that probably\nrepresents the first detection of an AGB star in a Galactic young open cluster.\nAlthough strong differential reddening renders accurate parameter determination\nunfeasible with the current dataset, direct comparison to clusters of similar\nage suggests that Valparaiso~1 was born as one of the most massive clusters in\nthe Solar Neighbourhood, with an initial mass close to $10^{4}\\:$M$_{\\odot}$.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:02:52 GMT"}],"update_date":"2022-04-01"}
{"id":"2104.08779","submitter":"Ziqian Zeng","authors":"Ziqian Zeng, Yangqiu Song","title":"Variational Weakly Supervised Sentiment Analysis with Posterior\n  Regularization","comments":"Accepted at EACL 2021. arXiv admin note: text overlap with\n  arXiv:2008.09394","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Sentiment analysis is an important task in natural language processing (NLP).\nMost of existing state-of-the-art methods are under the supervised learning\nparadigm. However, human annotations can be scarce. Thus, we should leverage\nmore weak supervision for sentiment analysis. In this paper, we propose a\nposterior regularization framework for the variational approach to the weakly\nsupervised sentiment analysis to better control the posterior distribution of\nthe label assignment. The intuition behind the posterior regularization is that\nif extracted opinion words from two documents are semantically similar, the\nposterior distributions of two documents should be similar. Our experimental\nresults show that the posterior regularization can improve the original\nvariational approach to the weakly supervised sentiment analysis and the\nperformance is more stable with smaller prediction variance.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:05:31 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08780","submitter":"Yijie Lin","authors":"Yijie Lin","title":"Relative orbifold Pandharipande-Thomas theory and the degeneration\n  formula","comments":null,"journal-ref":null,"doi":"10.1093/imrn/rnab347","report-no":null,"categories":"math.AG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We construct relative moduli spaces of semistable pairs on a family of\nprojective Deligne-Mumford stacks. We define moduli stacks of stable orbifold\nPandharipande-Thomas pairs on stacks of expanded degenerations and pairs, and\nthen show they are separated and proper Deligne-Mumford stacks of finite type.\nAs an application, we present the degeneration formula for the absolute and\nrelative orbifold Pandharipande-Thomas invariants.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:08:41 GMT"},{"version":"v2","created":"Wed, 19 May 2021 09:13:28 GMT"},{"version":"v3","created":"Fri, 28 Jan 2022 15:08:45 GMT"}],"update_date":"2022-01-31"}
{"id":"2104.08781","submitter":"Antonios Liapis","authors":"Konstantinos Sfikas and Antonios Liapis and Georgios N. Yannakakis","title":"Monte Carlo Elites: Quality-Diversity Selection as a Multi-Armed Bandit\n  Problem","comments":"Proceedings of the Genetic and Evolutionary Computation Conference, 9\n  pages, 6 figures, 3 tables","journal-ref":null,"doi":"10.1145/3449639.3459321","report-no":null,"categories":"cs.NE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  A core challenge of evolutionary search is the need to balance between\nexploration of the search space and exploitation of highly fit regions.\nQuality-diversity search has explicitly walked this tightrope between a\npopulation's diversity and its quality. This paper extends a popular\nquality-diversity search algorithm, MAP-Elites, by treating the selection of\nparents as a multi-armed bandit problem. Using variations of the\nupper-confidence bound to select parents from under-explored but potentially\nrewarding areas of the search space can accelerate the discovery of new regions\nas well as improve its archive's total quality. The paper tests an indirect\nmeasure of quality for parent selection: the survival rate of a parent's\noffspring. Results show that maintaining a balance between exploration and\nexploitation leads to the most diverse and high-quality set of solutions in\nthree different testbeds.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:11:48 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08782","submitter":"Fan Yin","authors":"Fan Yin, Zhouxing Shi, Cho-Jui Hsieh, Kai-Wei Chang","title":"On the Sensitivity and Stability of Model Interpretations in NLP","comments":"ACL 2022, long paper, main conference","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Recent years have witnessed the emergence of a variety of post-hoc\ninterpretations that aim to uncover how natural language processing (NLP)\nmodels make predictions. Despite the surge of new interpretation methods, it\nremains an open problem how to define and quantitatively measure the\nfaithfulness of interpretations, i.e., to what extent interpretations reflect\nthe reasoning process by a model. We propose two new criteria, sensitivity and\nstability, that provide complementary notions of faithfulness to the existed\nremoval-based criteria. Our results show that the conclusion for how faithful\ninterpretations are could vary substantially based on different notions.\nMotivated by the desiderata of sensitivity and stability, we introduce a new\nclass of interpretation methods that adopt techniques from adversarial\nrobustness. Empirical results show that our proposed methods are effective\nunder the new criteria and overcome limitations of gradient-based methods on\nremoval-based criteria. Besides text classification, we also apply\ninterpretation methods and metrics to dependency parsing. Our results shed\nlight on understanding the diverse set of interpretations.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:19:44 GMT"},{"version":"v2","created":"Thu, 31 Mar 2022 19:43:18 GMT"}],"update_date":"2022-04-04"}
{"id":"2104.08783","submitter":"Xin Sun","authors":"Xin Sun, Changrui Chen, Xiaorui Wang, Junyu Dong, Huiyu Zhou, Sheng\n  Chen","title":"Gaussian Dynamic Convolution for Efficient Single-Image Segmentation","comments":null,"journal-ref":"IEEE Transactions on Circuits and Systems for Video Technology,\n  2021","doi":"10.1109/TCSVT.2021.3096814","report-no":"TCSVT-06243-2021","categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Interactive single-image segmentation is ubiquitous in the scientific and\ncommercial imaging software. In this work, we focus on the single-image\nsegmentation problem only with some seeds such as scribbles. Inspired by the\ndynamic receptive field in the human being's visual system, we propose the\nGaussian dynamic convolution (GDC) to fast and efficiently aggregate the\ncontextual information for neural networks. The core idea is randomly selecting\nthe spatial sampling area according to the Gaussian distribution offsets. Our\nGDC can be easily used as a module to build lightweight or complex segmentation\nnetworks. We adopt the proposed GDC to address the typical single-image\nsegmentation tasks. Furthermore, we also build a Gaussian dynamic pyramid\nPooling to show its potential and generality in common semantic segmentation.\nExperiments demonstrate that the GDC outperforms other existing convolutions on\nthree benchmark segmentation datasets including Pascal-Context, Pascal-VOC\n2012, and Cityscapes. Additional experiments are also conducted to illustrate\nthat the GDC can produce richer and more vivid features compared with other\nconvolutions. In general, our GDC is conducive to the convolutional neural\nnetworks to form an overall impression of the image.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:20:55 GMT"},{"version":"v2","created":"Sun, 23 May 2021 11:28:04 GMT"}],"update_date":"2021-07-13"}
{"id":"2104.08784","submitter":"Seong-Hee Kim","authors":"A.B. Dieker, Seong-Hee Kim","title":"Efficient Fully Sequential Indifference-Zone Procedures Using Properties\n  of Multidimensional Brownian Motion Exiting a Sphere","comments":"37 pages, 11 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"stat.ME","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We consider a ranking and selection (R&S) problem with the goal to select a\nsystem with the largest or smallest expected performance measure among a number\nof simulated systems with a pre-specified probability of correct selection.\nFully sequential procedures take one observation from each survived system and\neliminate inferior systems when there is clear statistical evidence that they\nare inferior. Most fully sequential procedures make elimination decisions based\non sample performances of each possible pair of survived systems and exploit\nthe bound crossing properties of a univariate Brownian motion. In this paper,\nwe present new fully sequential procedures with elimination decisions that are\nbased on sample performances of all competing systems. Using properties of a\nmultidimensional Brownian motion exiting a sphere, we derive heuristics that\naim to achieve a given target probability of correct selection. We show that in\npractice the new procedures significantly outperform a widely used fully\nsequential procedure. Compared to BIZ, a recent fully-sequential procedure that\nuses statistics inspired by Bayes posterior probabilities, our procedures have\nbetter performance under difficult mean or variance configurations but similar\nperformance under easy mean configurations.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:25:01 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08785","submitter":"Jean-Loup Ville","authors":"Jean-Loup Ville, Alexis Morvan, Akel Hashim, Ravi K. Naik, Marie Lu,\n  Bradley Mitchell, John-Mark Kreikebaum, Kevin P. O'Brien, Joel J. Wallman,\n  Ian Hincks, Joseph Emerson, Ethan Smith, Ed Younis, Costin Iancu, David I.\n  Santiago, Irfan Siddiqi","title":"Leveraging Randomized Compiling for the QITE Algorithm","comments":"Data has been updated","journal-ref":null,"doi":"10.1103/PhysRevResearch.4.033140","report-no":null,"categories":"quant-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The success of the current generation of Noisy Intermediate-Scale Quantum\n(NISQ) hardware shows that quantum hardware may be able to tackle complex\nproblems even without error correction. One outstanding issue is that of\ncoherent errors arising from the increased complexity of these devices. These\nerrors can accumulate through a circuit, making their impact on algorithms hard\nto predict and mitigate. Iterative algorithms like Quantum Imaginary Time\nEvolution are susceptible to these errors. This article presents the\ncombination of both noise tailoring using Randomized Compiling and error\nmitigation with a purification. We also show that Cycle Benchmarking gives an\nestimate of the reliability of the purification. We apply this method to the\nQuantum Imaginary Time Evolution of a Transverse Field Ising Model and report\nan energy estimation and a ground state infidelity both below 1\\%. Our\nmethodology is general and can be used for other algorithms and platforms. We\nshow how combining noise tailoring and error mitigation will push forward the\nperformance of NISQ devices.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:26:25 GMT"},{"version":"v2","created":"Tue, 26 Oct 2021 08:53:47 GMT"}],"update_date":"2023-03-03"}
{"id":"2104.08786","submitter":"Yao Lu","authors":"Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus\n  Stenetorp","title":"Fantastically Ordered Prompts and Where to Find Them: Overcoming\n  Few-Shot Prompt Order Sensitivity","comments":"ACL 2022","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  When primed with only a handful of training samples, very large, pretrained\nlanguage models such as GPT-3 have shown competitive results when compared to\nfully-supervised, fine-tuned, large, pretrained language models. We demonstrate\nthat the order in which the samples are provided can make the difference\nbetween near state-of-the-art and random guess performance: essentially some\npermutations are \"fantastic\" and some not. We analyse this phenomenon in\ndetail, establishing that: it is present across model sizes (even for the\nlargest current models), it is not related to a specific subset of samples, and\nthat a given good permutation for one model is not transferable to another.\nWhile one could use a development set to determine which permutations are\nperformant, this would deviate from the true few-shot setting as it requires\nadditional annotated data. Instead, we use the generative nature of language\nmodels to construct an artificial development set and based on entropy\nstatistics of the candidate permutations on this set, we identify performant\nprompts. Our method yields a 13% relative improvement for GPT-family models\nacross eleven different established text classification tasks.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:29:16 GMT"},{"version":"v2","created":"Thu, 3 Mar 2022 12:10:58 GMT"}],"update_date":"2022-03-04"}
{"id":"2104.08787","submitter":"Zhen Wang","authors":"Zhen Wang, Xiangxie Zhang, Yicong Tan","title":"Chinese Sentences Similarity via Cross-Attention Based Siamese Network","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Measuring sentence similarity is a key research area nowadays as it allows\nmachines to better understand human languages. In this paper, we proposed a\nCross-Attention Siamese Network (CATsNet) to carry out the task of learning the\nsemantic meanings of Chinese sentences and comparing the similarity between two\nsentences. This novel model is capable of catching non-local features.\nAdditionally, we also tried to apply the long short-term memory (LSTM) network\nin the model to improve its performance. The experiments were conducted on the\nLCQMC dataset and the results showed that our model could achieve a higher\naccuracy than previous work.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:35:58 GMT"},{"version":"v2","created":"Thu, 6 May 2021 18:35:07 GMT"}],"update_date":"2021-05-10"}
{"id":"2104.08788","submitter":"Chi Zhang","authors":"Zhenfeng Wu, Chi Zhang","title":"On finite groups factorized by $\\sigma$-nilpotent subgroups","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.GR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Let $G$ be a finite group and $\\sigma=\\{\\sigma_{i}|i\\in I\\}$ be a partition\nof the set of all primes $\\mathbb{P}$, that is, $\\mathbb{P}=\\bigcup_{i\\in\nI}\\sigma_{i}$ and $\\sigma_{i}\\cap \\sigma_{j}=\\emptyset$ for all $i\\neq j$. A\nchief factor $H/K$ of $G$ is said to be $\\sigma$-central in $G$, if the\nsemidirect product $(H/K)\\rtimes(G/C_G(H/K))$ is a $\\sigma_i$-group for some\n$i\\in I$. The group $G$ is said to be $\\sigma$-nilpotent if either $G=1$ or\nevery chief factor of $G$ is $\\sigma$-central. In this paper, we study the\nproperties of a finite group $G=AB$, factorized by two $\\sigma$-nilpotent\nsubgroups $A$ and $B$, and also generalize some known results.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:39:22 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08789","submitter":"Xavier Rafael-Palou","authors":"Xavier Rafael-Palou, Anton Aubanell, Mario Ceresa, Vicent Ribas, Gemma\n  Piella, Miguel A. Gonz\\'alez Ballester","title":"An Uncertainty-aware Hierarchical Probabilistic Network for Early\n  Prediction, Quantification and Segmentation of Pulmonary Tumour Growth","comments":"24 pages, 9 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.NE","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Early detection and quantification of tumour growth would help clinicians to\nprescribe more accurate treatments and provide better surgical planning.\nHowever, the multifactorial and heterogeneous nature of lung tumour progression\nhampers identification of growth patterns. In this study, we present a novel\nmethod based on a deep hierarchical generative and probabilistic framework\nthat, according to radiological guidelines, predicts tumour growth, quantifies\nits size and provides a semantic appearance of the future nodule. Unlike\nprevious deterministic solutions, the generative characteristic of our approach\nalso allows us to estimate the uncertainty in the predictions, especially\nimportant for complex and doubtful cases. Results of evaluating this method on\nan independent test set reported a tumour growth balanced accuracy of 74%, a\ntumour growth size MAE of 1.77 mm and a tumour segmentation Dice score of 78%.\nThese surpassed the performances of equivalent deterministic and alternative\ngenerative solutions (i.e. probabilistic U-Net, Bayesian test dropout and\nPix2Pix GAN) confirming the suitability of our approach.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:48:58 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08790","submitter":"Saadia Gabriel","authors":"Saadia Gabriel, Skyler Hallinan, Maarten Sap, Pemi Nguyen, Franziska\n  Roesner, Eunsol Choi, Yejin Choi","title":"Misinfo Reaction Frames: Reasoning about Readers' Reactions to News\n  Headlines","comments":"ACL 2022 camera-ready","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Even to a simple and short news headline, readers react in a multitude of\nways: cognitively (e.g. inferring the writer's intent), emotionally (e.g.\nfeeling distrust), and behaviorally (e.g. sharing the news with their friends).\nSuch reactions are instantaneous and yet complex, as they rely on factors that\ngo beyond interpreting factual content of news. We propose Misinfo Reaction\nFrames (MRF), a pragmatic formalism for modeling how readers might react to a\nnews headline. In contrast to categorical schema, our free-text dimensions\nprovide a more nuanced way of understanding intent beyond being benign or\nmalicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced\ndataset of reactions to over 25k news headlines focusing on global crises: the\nCovid-19 pandemic, climate change, and cancer. Empirical results confirm that\nit is indeed possible for neural models to predict the prominent patterns of\nreaders' reactions to previously unseen news headlines. Additionally, our user\nstudy shows that displaying machine-generated MRF implications alongside news\nheadlines to readers can increase their trust in real news while decreasing\ntheir trust in misinformation. Our work demonstrates the feasibility and\nimportance of pragmatic inferences on news headlines to help enhance AI-guided\nmisinformation detection and mitigation.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:50:11 GMT"},{"version":"v2","created":"Thu, 14 Oct 2021 05:41:42 GMT"},{"version":"v3","created":"Fri, 15 Oct 2021 21:09:53 GMT"},{"version":"v4","created":"Tue, 22 Mar 2022 04:42:13 GMT"}],"update_date":"2022-03-23"}
{"id":"2104.08791","submitter":"Tong Pan","authors":"Tong Pan, Heng Yu, Reinout J. van Weeren, Shumei Jia, Chengkui Li,\n  Yipeng Lyu","title":"Catalog of One-side Head-Tail Galaxies in the FIRST Survey","comments":"17 pages, 84 figures(69 figures in appendix). Published by ApJS,\n  comments welcome!","journal-ref":null,"doi":"10.3847/1538-4365/abf8bf","report-no":null,"categories":"astro-ph.GA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  One-side head-tail (OHT) galaxies are radio galaxies with a peculiar shape.\nThey usually appear in galaxy clusters, but they have never been cataloged\nsystematically. We design an automatic procedure to search for them in the\nFaint Images of the Radio Sky at Twenty-Centimeters source catalog and compile\na sample with 115 HT candidates. After cross-checking with the Sloan Digital\nSky Survey photometric data and catalogs of galaxy clusters, we find that 69 of\nthem are possible OHT galaxies. Most of them are close to the center of galaxy\nclusters. The lengths of their tails do not correlate with the projection\ndistance to the center of the nearest galaxy clusters, but show weak\nanticorrelation with the cluster richness, and are inversely proportional to\nthe radial velocity differences between clusters and host galaxies. Our catalog\nprovides a unique sample to study this special type of radio galaxies.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:54:41 GMT"},{"version":"v2","created":"Sun, 6 Jun 2021 16:46:19 GMT"}],"update_date":"2021-06-08"}
{"id":"2104.08792","submitter":"Mimansa Jaiswal","authors":"Mimansa Jaiswal, Emily Mower Provost","title":"Human-Imitating Metrics for Training and Evaluating Privacy Preserving\n  Emotion Recognition Models Using Sociolinguistic Knowledge","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.HC","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Privacy preservation is a crucial component of any real-world application.\nBut, in applications relying on machine learning backends, privacy is\nchallenging because models often capture more than what the model was initially\ntrained for, resulting in the potential leakage of sensitive information. In\nthis paper, we propose an automatic and quantifiable metric that allows us to\nevaluate humans' perception of a model's ability to preserve privacy with\nrespect to sensitive variables. In this paper, we focus on saliency-based\nexplanations, explanations that highlight regions of the input text, to infer\ninternal workings of a black box model. We use the degree with which\ndifferences in interpretation of general vs privacy preserving models correlate\nwith sociolinguistic biases to inform metric design. We show how certain\ncommonly-used methods that seek to preserve privacy do not align with human\nperception of privacy preservation leading to distrust about model's claims. We\ndemonstrate the versatility of our proposed metric by validating its utility\nfor measuring cross corpus generalization for both privacy and emotion.\nFinally, we conduct crowdsourcing experiments to evaluate the inclination of\nthe evaluators to choose a particular model for a given purpose when model\nexplanations are provided, and show a positive relationship with the proposed\nmetric. To the best of our knowledge, we take the first step in proposing\nautomatic and quantifiable metrics that best align with human perception of\nmodel's ability for privacy preservation, allowing for cost-effective model\ndevelopment.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:56:41 GMT"},{"version":"v2","created":"Mon, 4 Oct 2021 12:36:18 GMT"}],"update_date":"2021-10-05"}
{"id":"2104.08793","submitter":"Aaron Chan","authors":"Aaron Chan, Jiashu Xu, Boyuan Long, Soumya Sanyal, Tanishq Gupta,\n  Xiang Ren","title":"SalKG: Learning From Knowledge Graph Explanations for Commonsense\n  Reasoning","comments":"NeurIPS 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Augmenting pre-trained language models with knowledge graphs (KGs) has\nachieved success on various commonsense reasoning tasks. However, for a given\ntask instance, the KG, or certain parts of the KG, may not be useful. Although\nKG-augmented models often use attention to focus on specific KG components, the\nKG is still always used, and the attention mechanism is never explicitly taught\nwhich KG components should be used. Meanwhile, saliency methods can measure how\nmuch a KG feature (e.g., graph, node, path) influences the model to make the\ncorrect prediction, thus explaining which KG features are useful. This paper\nexplores how saliency explanations can be used to improve KG-augmented models'\nperformance. First, we propose to create coarse (Is the KG useful?) and fine\n(Which nodes/paths in the KG are useful?) saliency explanations. Second, to\nmotivate saliency-based supervision, we analyze oracle KG-augmented models\nwhich directly use saliency explanations as extra inputs for guiding their\nattention. Third, we propose SalKG, a framework for KG-augmented models to\nlearn from coarse and/or fine saliency explanations. Given saliency\nexplanations created from a task's training set, SalKG jointly trains the model\nto predict the explanations, then solve the task by attending to KG features\nhighlighted by the predicted explanations. On three commonsense QA benchmarks\n(CSQA, OBQA, CODAH) and a range of KG-augmented models, we show that SalKG can\nyield considerable performance gains -- up to 2.76% absolute improvement on\nCSQA.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 09:59:46 GMT"},{"version":"v2","created":"Mon, 12 Jul 2021 18:53:44 GMT"},{"version":"v3","created":"Tue, 7 Dec 2021 20:00:29 GMT"},{"version":"v4","created":"Sat, 15 Jan 2022 06:04:57 GMT"},{"version":"v5","created":"Sun, 20 Mar 2022 04:02:52 GMT"}],"update_date":"2022-12-20"}
{"id":"2104.08794","submitter":"Xiaopeng Li","authors":"Hongmian Shui, Shengjie Jin, Zhihan Li, Fansu Wei, Xuzong Chen,\n  Xiaopeng Li, Xiaoji Zhou","title":"Atom-Orbital Qubits under Holonomic Quantum Control","comments":"14 pages, 9 figures, published version","journal-ref":"Phys. Rev. A 104, L060601 (2021)","doi":"10.1103/PhysRevA.104.L060601","report-no":null,"categories":"quant-ph cond-mat.mtrl-sci cond-mat.quant-gas","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Quantum computing has been attracting tremendous efforts in recent years. One\nprominent application is to perform quantum simulations of electron\ncorrelations in large molecules and solid-state materials, where orbital\ndegrees of freedom are crucial to quantitatively model electronic properties.\nElectron orbitals unlike quantum spins obey crystal symmetries, making the\natomic orbital in optical lattices a natural candidate to emulate electron\norbitals. Here, we construct atom-orbital qubits by manipulating $s$- and\n$d$-orbitals of atomic Bose-Einstein condensation in an optical lattice.\nNoise-resilient quantum gate operations are achieved by performing holonomic\nquantum control, which admits geometrical protection. We find it is critical to\neliminate the orbital leakage error in the system. The gate robustness is\ntested by varying the intensity of the laser forming the lattice. Our work\nopens up wide opportunities for atom-orbital based quantum information\nprocessing, of vital importance to programmable quantum simulations of\nmulti-orbital physics in molecules and quantum materials.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:03:11 GMT"},{"version":"v2","created":"Thu, 9 Dec 2021 10:53:11 GMT"}],"update_date":"2021-12-10"}
{"id":"2104.08795","submitter":"Buddhika Semage","authors":"Buddhika Laknath Semage, Thommen George Karimpanal, Santu Rana, Svetha\n  Venkatesh","title":"Intuitive Physics Guided Exploration for Sample Efficient Sim2real\n  Transfer","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.RO cs.SY eess.SY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Physics-based reinforcement learning tasks can benefit from simplified\nphysics simulators as they potentially allow near-optimal policies to be\nlearned in simulation. However, such simulators require the latent factors\n(e.g. mass, friction coefficient etc.) of the associated objects and other\nenvironment-specific factors (e.g. wind speed, air density etc.) to be\naccurately specified, without which, it could take considerable additional\nlearning effort to adapt the learned simulation policy to the real environment.\nAs such a complete specification can be impractical, in this paper, we instead,\nfocus on learning task-specific estimates of latent factors which allow the\napproximation of real world trajectories in an ideal simulation environment.\nSpecifically, we propose two new concepts: a) action grouping - the idea that\ncertain types of actions are closely associated with the estimation of certain\nlatent factors, and; b) partial grounding - the idea that simulation of\ntask-specific dynamics may not need precise estimation of all the latent\nfactors. We first introduce intuitive action groupings based on human physics\nknowledge and experience, which is then used to design novel strategies for\ninteracting with the real environment. Next, we describe how prior knowledge of\na task in a given environment can be used to extract the relative importance of\ndifferent latent factors, and how this can be used to inform partial grounding,\nwhich enables efficient learning of the task in any arbitrary environment. We\ndemonstrate our approach in a range of physics based tasks, and show that it\nachieves superior performance relative to other baselines, using only a limited\nnumber of real-world interactions.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:03:26 GMT"},{"version":"v2","created":"Fri, 11 Feb 2022 18:41:19 GMT"}],"update_date":"2022-02-14"}
{"id":"2104.08796","submitter":"Sai Krishna Chada","authors":"Sai Krishna Chada, Jitin Mathew Thomas, Daniel G\\\"orges, Achim Ebert,\n  Roman Teutsch","title":"Ecological Adaptive Cruise Control for City Buses based on Hybrid Model\n  Predictive Control using PnG and Traffic Light Information","comments":"Submitted to IEEE Vehicular Power and Propulsion 2021","journal-ref":"2021 IEEE Vehicle Power and Propulsion Conference (VPPC)","doi":"10.1109/VPPC53923.2021.9699343","report-no":null,"categories":"eess.SY cs.SY","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This paper proposes an ecological adaptive cruise control (EACC) concept with\nthe primary goal to minimize the fuel consumption in a city bus with an\ninternal combustion engine (ICE). A hybrid model predictive control (HMPC) is\nimplemented in this work to control both continuous and discrete-time\nvariables. Moreover, a multi-objective optimization problem for EACC is\nformulated in time-domain as a mixed-integer quadratically constrained\nquadratic programming (MIQCQP) problem. The proposed HMPC-EACC performs robust\nvehicle-following while tracking a leading vehicle and plans fuel-efficient\nacceleration and deceleration maneuvers for the host vehicle. Additionally, it\nuses the signal phase and timing (SPaT) information to compute a green wave\nreference speed for the host vehicle to cross the signalized intersections at a\ngreen phase. Moreover, the proposed controller performs pulse and glide (PnG)\nto optimally control the engine ON and OFF states and save additional fuel.\nFurthermore, the performance of the proposed strategy is evaluated on a\nreal-world driving profile and compared against a baseline controller from the\nliterature. Finally, the influence of different prediction horizons on the fuel\nsavings and computation times are studied. The results reveal significant\nreduction in fuel consumption with HMPC-EACC and demonstrate that the proposed\ncontroller is real-time capable.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:04:22 GMT"}],"update_date":"2022-09-01"}
{"id":"2104.08797","submitter":"Zengyi Qin","authors":"Zengyi Qin, Jinglu Wang, Yan Lu","title":"MonoGRNet: A General Framework for Monocular 3D Object Detection","comments":"The IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Detecting and localizing objects in the real 3D space, which plays a crucial\nrole in scene understanding, is particularly challenging given only a monocular\nimage due to the geometric information loss during imagery projection. We\npropose MonoGRNet for the amodal 3D object detection from a monocular image via\ngeometric reasoning in both the observed 2D projection and the unobserved depth\ndimension. MonoGRNet decomposes the monocular 3D object detection task into\nfour sub-tasks including 2D object detection, instance-level depth estimation,\nprojected 3D center estimation and local corner regression. The task\ndecomposition significantly facilitates the monocular 3D object detection,\nallowing the target 3D bounding boxes to be efficiently predicted in a single\nforward pass, without using object proposals, post-processing or the\ncomputationally expensive pixel-level depth estimation utilized by previous\nmethods. In addition, MonoGRNet flexibly adapts to both fully and weakly\nsupervised learning, which improves the feasibility of our framework in diverse\nsettings. Experiments are conducted on KITTI, Cityscapes and MS COCO datasets.\nResults demonstrate the promising performance of our framework in various\nscenarios.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:07:52 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08798","submitter":"Kush Singhal","authors":"Kush Singhal (The University of Hong Kong)","title":"Near-miss Identities and Spinor Genus Classification of Ternary\n  Quadratic Forms with Congruence Conditions","comments":"33 pages, no figures, 9 tables (3 unnumbered) For associated Python\n  and C++ programme files, see\n  https://github.com/kush1729/ternary-quadratic-forms/blob/master/README.md","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this paper, near-miss identities for the number of representations of some\nintegral ternary quadratic forms with congruence conditions are found and\nproven. The genus and spinor genus of the corresponding lattice cosets are then\nclassified. Finally, a complete genus and spinor genus classification for all\nconductor 2 lattice cosets of 2-adically unimodular lattices is given.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:09:42 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08799","submitter":"Yige Xu","authors":"Yichao Luo, Yige Xu, Jiacheng Ye, Xipeng Qiu, Qi Zhang","title":"Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement\n  Learning","comments":"11 pages, Findings of EMNLP 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Aiming to generate a set of keyphrases, Keyphrase Generation (KG) is a\nclassical task for capturing the central idea from a given document. Based on\nSeq2Seq models, the previous reinforcement learning framework on KG tasks\nutilizes the evaluation metrics to further improve the well-trained neural\nmodels. However, these KG evaluation metrics such as $F_1@5$ and $F_1@M$ are\nonly aware of the exact correctness of predictions on phrase-level and ignore\nthe semantic similarities between similar predictions and targets, which\ninhibits the model from learning deep linguistic patterns. In response to this\nproblem, we propose a new fine-grained evaluation metric to improve the RL\nframework, which considers different granularities: token-level $F_1$ score,\nedit distance, duplication, and prediction quantities. On the whole, the new\nframework includes two reward functions: the fine-grained evaluation score and\nthe vanilla $F_1$ score. This framework helps the model identifying some\npartial match phrases which can be further optimized as the exact match ones.\nExperiments on KG benchmarks show that our proposed training framework\noutperforms the previous RL training frameworks among all evaluation scores. In\naddition, our method can effectively ease the synonym problem and generate a\nhigher quality prediction. The source code is available at\n\\url{https://github.com/xuyige/FGRL4KG}.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:13:46 GMT"},{"version":"v2","created":"Fri, 10 Sep 2021 13:22:05 GMT"}],"update_date":"2021-09-13"}
{"id":"2104.08800","submitter":"David Ziemkiewicz PhD","authors":"David Ziemkiewicz, Gerard Czajkowski, Karol Karpi\\'nski, Sylwia\n  Zieli\\'nska - Raczy\\'nska","title":"Electro-optical properties of excitons in Cu$_2$O quantum wells: I\n  discrete states","comments":null,"journal-ref":"Phys. Rev. B 104, 075303 (2021)","doi":"10.1103/PhysRevB.104.075303","report-no":null,"categories":"cond-mat.mes-hall","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present theoretical results of the calculations of optical functions for\nCu$_2$O quantum well (QW) with Rydberg excitons in an external homogeneous\nelectric field of an arbitrary field strength. Two configurations of an\nexternal electric field perpendicular and parallel to the QW planes are\nconsidered in the energetic region for discrete excitonic states and continuum\nstates. With the help of the real density matrix approach, which enables the\nderivation of the analytical expressions for the QW electro-optical functions,\nabsorption spectra are calculated for the case of the excitation energy below\nthe gap energy.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:18:38 GMT"}],"update_date":"2021-09-01"}
{"id":"2104.08801","submitter":"Devang Kulshreshtha","authors":"Devang Kulshreshtha, Robert Belfer, Iulian Vlad Serban, Siva Reddy","title":"Back-Training excels Self-Training at Unsupervised Domain Adaptation of\n  Question Generation and Passage Retrieval","comments":"EMNLP 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this work, we introduce back-training, an alternative to self-training for\nunsupervised domain adaptation (UDA) from source to target domain. While\nself-training generates synthetic training data where natural inputs are\naligned with noisy outputs, back-training results in natural outputs aligned\nwith noisy inputs. This significantly reduces the gap between the target domain\nand synthetic data distribution, and reduces model overfitting to the source\ndomain. We run UDA experiments on question generation and passage retrieval\nfrom the \\textit{Natural Questions} domain to machine learning and biomedical\ndomains. We find that back-training vastly outperforms self-training by a mean\nimprovement of 7.8 BLEU-4 points on generation, and 17.6\\% top-20 retrieval\naccuracy across both domains. We further propose consistency filters to remove\nlow-quality synthetic data before training. We also release a new\ndomain-adaptation dataset- \\textit{MLQuestions} containing 35K unaligned\nquestions, 50K unaligned passages, and 3K aligned question-passage pairs.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:20:07 GMT"},{"version":"v2","created":"Thu, 9 Sep 2021 00:20:28 GMT"}],"update_date":"2021-09-10"}
{"id":"2104.08802","submitter":"David Ziemkiewicz PhD","authors":"David Ziemkiewicz, Gerard Czajkowski, Karol Karpi\\'nski, Sylwia\n  Zieli\\'nska - Raczy\\'nska","title":"Electro-optical properties of excitons in Cu$_2$O quantum wells: II\n  continuum states","comments":null,"journal-ref":"Phys. Rev. B 104, 075304 (2021)","doi":"10.1103/PhysRevB.104.075304","report-no":null,"categories":"cond-mat.mes-hall","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present theoretically calculated optical functions for Cu$_2$O quantum\nwell (QW) with Rydberg excitons in an external, homogeneous electric field\nparallel to the QW planes for the energy region above the gap, suitable to\nobserve the Franz-Keldysh (FK) oscillations. We quantitatively describe the\namplitudes and periodicity of FK modulations and the influence of both Rydberg\nexcitons and confinement effect on this phenomenon.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:22:09 GMT"}],"update_date":"2021-09-01"}
{"id":"2104.08803","submitter":"Tal Schuster","authors":"Tal Schuster, Adam Fisch, Tommi Jaakkola, Regina Barzilay","title":"Consistent Accelerated Inference via Confident Adaptive Transformers","comments":"EMNLP 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We develop a novel approach for confidently accelerating inference in the\nlarge and expensive multilayer Transformers that are now ubiquitous in natural\nlanguage processing (NLP). Amortized or approximate computational methods\nincrease efficiency, but can come with unpredictable performance costs. In this\nwork, we present CATs -- Confident Adaptive Transformers -- in which we\nsimultaneously increase computational efficiency, while guaranteeing a\nspecifiable degree of consistency with the original model with high confidence.\nOur method trains additional prediction heads on top of intermediate layers,\nand dynamically decides when to stop allocating computational effort to each\ninput using a meta consistency classifier. To calibrate our early prediction\nstopping rule, we formulate a unique extension of conformal prediction. We\ndemonstrate the effectiveness of this approach on four classification and\nregression tasks.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:22:28 GMT"},{"version":"v2","created":"Thu, 9 Sep 2021 17:57:04 GMT"}],"update_date":"2021-09-10"}
{"id":"2104.08804","submitter":"Harkanwar Singh","authors":"Harkanwar Singh, Prachi Jain, Mausam, Soumen Chakrabarti","title":"Multilingual Knowledge Graph Completion with Joint Relation and Entity\n  Alignment","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI cs.CL cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Knowledge Graph Completion (KGC) predicts missing facts in an incomplete\nKnowledge Graph. Almost all of existing KGC research is applicable to only one\nKG at a time, and in one language only. However, different language speakers\nmay maintain separate KGs in their language and no individual KG is expected to\nbe complete. Moreover, common entities or relations in these KGs have different\nsurface forms and IDs, leading to ID proliferation. Entity alignment (EA) and\nrelation alignment (RA) tasks resolve this by recognizing pairs of entity\n(relation) IDs in different KGs that represent the same entity (relation). This\ncan further help prediction of missing facts, since knowledge from one KG is\nlikely to benefit completion of another. High confidence predictions may also\nadd valuable information for the alignment tasks. In response, we study the\nnovel task of jointly training multilingual KGC, relation alignment and entity\nalignment models. We present ALIGNKGC, which uses some seed alignments to\njointly optimize all three of KGC, EA and RA losses. A key component of\nALIGNKGC is an embedding based soft notion of asymmetric overlap defined on the\n(subject, object) set signatures of relations this aids in better predicting\nrelations that are equivalent to or implied by other relations. Extensive\nexperiments with DBPedia in five languages establish the benefits of joint\ntraining for all tasks, achieving 10-32 MRR improvements of ALIGNKGC over a\nstrong state-of-the-art single-KGC system completion model over each\nmonolingual KG . Further, ALIGNKGC achieves reasonable gains in EA and RA tasks\nover a vanilla completion model over a KG that combines all facts without\nalignment, underscoring the value of joint training for these tasks.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:27:44 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08805","submitter":"Sergio Rozada","authors":"Sergio Rozada, Victor Tenorio, and Antonio G. Marques","title":"Low-rank State-action Value-function Approximation","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AI","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  Value functions are central to Dynamic Programming and Reinforcement Learning\nbut their exact estimation suffers from the curse of dimensionality,\nchallenging the development of practical value-function (VF) estimation\nalgorithms. Several approaches have been proposed to overcome this issue, from\nnon-parametric schemes that aggregate states or actions to parametric\napproximations of state and action VFs via, e.g., linear estimators or deep\nneural networks. Relevantly, several high-dimensional state problems can be\nwell-approximated by an intrinsic low-rank structure. Motivated by this and\nleveraging results from low-rank optimization, this paper proposes different\nstochastic algorithms to estimate a low-rank factorization of the $Q(s, a)$\nmatrix. This is a non-parametric alternative to VF approximation that\ndramatically reduces the computational and sample complexities relative to\nclassical $Q$-learning methods that estimate $Q(s,a)$ separately for each\nstate-action pair.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:31:39 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08806","submitter":"Mimansa Jaiswal","authors":"Mimansa Jaiswal, Emily Mower Provost","title":"Best Practices for Noise-Based Augmentation to Improve the Performance\n  of Emotion Recognition \"In the Wild\"","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD cs.LG eess.AS","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Emotion recognition as a key component of high-stake downstream applications\nhas been shown to be effective, such as classroom engagement or mental health\nassessments. These systems are generally trained on small datasets collected in\nsingle laboratory environments, and hence falter when tested on data that has\ndifferent noise characteristics. Multiple noise-based data augmentation\napproaches have been proposed to counteract this challenge in other speech\ndomains. But, unlike speech recognition and speaker verification, in emotion\nrecognition, noise-based data augmentation may change the underlying label of\nthe original emotional sample. In this work, we generate realistic noisy\nsamples of a well known emotion dataset (IEMOCAP) using multiple categories of\nenvironmental and synthetic noise. We evaluate how both human and machine\nemotion perception changes when noise is introduced. We find that some commonly\nused augmentation techniques for emotion recognition significantly change human\nperception, which may lead to unreliable evaluation metrics such as evaluating\nefficiency of adversarial attack. We also find that the trained\nstate-of-the-art emotion recognition models fail to classify unseen\nnoise-augmented samples, even when trained on noise augmented datasets. This\nfinding demonstrates the brittleness of these systems in real-world conditions.\nWe propose a set of recommendations for noise-based augmentation of emotion\ndatasets and for how to deploy these emotion recognition systems \"in the wild\".\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:33:38 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08807","submitter":"Lars Mueller","authors":"Lars Mueller (1), Andreas Wetscherek (1), Tristan Anselm Kuder (1),\n  Frederik Bernd Laun (1) ((1) Medical Physics in Radiology, German Cancer\n  Research Center (DKFZ), Heidelberg, Germany)","title":"Eddy current compensated double diffusion encoded (DDE) MRI","comments":"23 pages, 4 figures, 1 table, 2 supplemental figures","journal-ref":"Magnetic Resonance for Medicine 77: 328-335 (2017)","doi":"10.1002/mrm.26092","report-no":null,"categories":"physics.med-ph","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Purpose: Eddy currents might lead to image distortions in diffusion weighted\necho planar imaging. A method is proposed to reduce their effects on double\ndiffusion encoding (DDE) MRI experiments and the thereby derived microscopic\nfractional anisotropy (\\mu FA) . Methods: The twice refocused spin echo scheme\nwas adapted for DDE measurements. To assess the effect of individual diffusion\nencodings on the image distortions, measurements of a grid of plastic rods in\nwater were performed. The effect of eddy current compensation on \\mu FA\nmeasurements was evaluated in the brains of six healthy volunteers. Results:\nThe use of an eddy current compensation reduced the signal variation. As\nexpected, the distortions caused by the second encoding were larger than those\nof the first encoding entailing a stronger need to compensate for them. For an\noptimal result, however, both encodings had to be compensated. The artifact\nreduction strongly improved the measurement of the \\mu FA in ventricles and\ngrey matter by reducing the overestimation. An effect of the compensation on\nabsolute \\mu FA values in white matter was not observed. Conclusion: It is\nadvisable to compensate both encodings in DDE measurements for eddy currents.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:41:54 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08808","submitter":"Xisen Jin","authors":"Xisen Jin, Bill Yuchen Lin, Mohammad Rostami, Xiang Ren","title":"Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation\n  for Few-shot Learning","comments":"Accepted at Findings of EMNLP 2021; Fixed an error in Table 3 (see\n  footnote 4); Updated Q3 in Sec. 4.2","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The ability to continuously expand knowledge over time and utilize it to\nrapidly generalize to new tasks is a key feature of human linguistic\nintelligence. Existing models that pursue rapid generalization to new tasks\n(e.g., few-shot learning methods), however, are mostly trained in a single shot\non fixed datasets, unable to dynamically expand their knowledge; while\ncontinual learning algorithms are not specifically designed for rapid\ngeneralization. We present a new learning setup, Continual Learning of Few-Shot\nLearners (CLIF), to address the challenges of both learning settings in a\nunified setup. CLIF assumes a model learns from a sequence of diverse NLP tasks\narriving sequentially, accumulating knowledge for improved generalization to\nnew tasks, while also retaining performance on the tasks learned earlier. We\nexamine how the generalization ability is affected in the continual learning\nsetup, evaluate a number of continual learning algorithms, and propose a novel\nregularized adapter generation approach. We find that catastrophic forgetting\naffects generalization ability to a less degree than performance on seen tasks;\nwhile continual learning algorithms can still bring considerable benefit to the\ngeneralization ability.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:41:56 GMT"},{"version":"v2","created":"Fri, 27 Aug 2021 04:34:42 GMT"},{"version":"v3","created":"Sun, 10 Oct 2021 00:16:29 GMT"},{"version":"v4","created":"Sun, 21 Aug 2022 03:21:12 GMT"}],"update_date":"2022-08-23"}
{"id":"2104.08809","submitter":"Arie Cattan","authors":"Arie Cattan, Sophie Johnson, Daniel Weld, Ido Dagan, Iz Beltagy, Doug\n  Downey, Tom Hope","title":"SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts","comments":"Accepted to AKBC 2021. Data and code available at\n  https://scico.apps.allenai.org/","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.IR cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Determining coreference of concept mentions across multiple documents is a\nfundamental task in natural language understanding. Previous work on\ncross-document coreference resolution (CDCR) typically considers mentions of\nevents in the news, which seldom involve abstract technical concepts that are\nprevalent in science and technology. These complex concepts take diverse or\nambiguous forms and have many hierarchical levels of granularity (e.g., tasks\nand subtasks), posing challenges for CDCR. We present a new task of\nHierarchical CDCR (H-CDCR) with the goal of jointly inferring coreference\nclusters and hierarchy between them. We create SciCo, an expert-annotated\ndataset for H-CDCR in scientific papers, 3X larger than the prominent ECB+\nresource. We study strong baseline models that we customize for H-CDCR, and\nhighlight challenges for future work.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:42:20 GMT"},{"version":"v2","created":"Fri, 27 Aug 2021 14:17:48 GMT"},{"version":"v3","created":"Wed, 1 Sep 2021 10:09:15 GMT"}],"update_date":"2021-09-02"}
{"id":"2104.08810","submitter":"Yajun Yan","authors":"Han-Shu Xu, Ya-Jun Yan, Ruotong Yin, Wei Xia, Shijie Fang, Ziyuan\n  Chen, Yuanji Li, Wenqi Yang, Yanfeng Guo, Dong-Lai Feng","title":"Multiband superconductivity with sign-preserving order parameter in\n  kagome superconductor CsV3Sb5","comments":"15 pages, 4 figures","journal-ref":null,"doi":"10.1103/PhysRevLett.127.187004","report-no":null,"categories":"cond-mat.supr-con","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The superconductivity of a kagome superconductor CsV3Sb5 is studied by\nscanning tunneling microscopy / spectroscopy at an ultralow temperature with\nhigh resolution. Two kinds of superconducting gaps with multiple sets of\ncoherent peaks and residual zero-energy density of states are observed on both\nhalf-Cs and Sb surfaces, implying multiband superconductivity with gap nodes.\nSixfold star-shaped magnetic vortex is observed with conventional Caroli-de\nGennes-Matricon bound states inside. Magnetic impurities suppress the\nsuperconductivity, while nonmagnetic impurities do not, suggesting the absence\nof sign-change in the superconducting order parameter. Moreover, the interplay\nbetween charge density waves and superconductivity differs on various bands,\nresulting in different density of state distributions. Our study provides\ncritical clues for further understanding the superconductivity and its relation\nto charge density waves in CsV3Sb5.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:44:19 GMT"}],"update_date":"2021-11-10"}
{"id":"2104.08811","submitter":"Anton Belyy","authors":"Noah Weber, Anton Belyy, Nils Holzenberger, Rachel Rudinger, Benjamin\n  Van Durme","title":"Human Schema Curation via Causal Association Rule Mining","comments":"12 pages, 6 figures, 6 tables","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Event schemas are structured knowledge sources defining typical real-world\nscenarios (e.g., going to an airport). We present a framework for efficient\nhuman-in-the-loop construction of a schema library, based on a novel script\ninduction system and a well-crafted interface that allows non-experts to\n\"program\" complex event structures. Associated with this work we release a\nschema library: a machine readable resource of 232 detailed event schemas, each\nof which describe a distinct typical scenario in terms of its relevant\nsub-event structure (what happens in the scenario), participants (who plays a\nrole in the scenario), fine-grained typing of each participant, and the implied\nrelational constraints between them. We make our schema library and the\nSchemaBlocks interface available online.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:48:26 GMT"},{"version":"v2","created":"Mon, 10 Jan 2022 04:19:51 GMT"},{"version":"v3","created":"Mon, 23 May 2022 14:26:36 GMT"}],"update_date":"2022-05-24"}
{"id":"2104.08812","submitter":"Wenxuan Zhou","authors":"Wenxuan Zhou, Fangyu Liu, Muhao Chen","title":"Contrastive Out-of-Distribution Detection for Pretrained Transformers","comments":"Accepted at EMNLP 2021. Code available at\n  https://github.com/wzhouad/Contra-OOD","journal-ref":null,"doi":"10.18653/v1/2021.emnlp-main.84","report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Pretrained Transformers achieve remarkable performance when training and test\ndata are from the same distribution. However, in real-world scenarios, the\nmodel often faces out-of-distribution (OOD) instances that can cause severe\nsemantic shift problems at inference time. Therefore, in practice, a reliable\nmodel should identify such instances, and then either reject them during\ninference or pass them over to models that handle another distribution. In this\npaper, we develop an unsupervised OOD detection method, in which only the\nin-distribution (ID) data are used in training. We propose to fine-tune the\nTransformers with a contrastive loss, which improves the compactness of\nrepresentations, such that OOD instances can be better differentiated from ID\nones. These OOD instances can then be accurately detected using the Mahalanobis\ndistance in the model's penultimate layer. We experiment with comprehensive\nsettings and achieve near-perfect OOD detection performance, outperforming\nbaselines drastically. We further investigate the rationales behind the\nimprovement, finding that more compact representations through margin-based\ncontrastive learning bring the improvement. We release our code to the\ncommunity for future research.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:51:47 GMT"},{"version":"v2","created":"Wed, 8 Sep 2021 04:01:01 GMT"}],"update_date":"2022-01-24"}
{"id":"2104.08813","submitter":"Abdul Karim Gizzini","authors":"Abdul Karim Gizzini, Marwa Chafii, Ahmad Nimr, Raed M. Shubair,\n  Gerhard Fettweis","title":"CNN aided Weighted Interpolation for Channel Estimation in Vehicular\n  Communications","comments":"16 pages","journal-ref":"IEEE Transactions on Vehicular Technology ( Early Access ) 2021","doi":"10.1109/TVT.2021.3120267","report-no":null,"categories":"cs.IT cs.AI math.IT","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  IEEE 802.11p standard defines wireless technology protocols that enable\nvehicular transportation and manage traffic efficiency. A major challenge in\nthe development of this technology is ensuring communication reliability in\nhighly dynamic vehicular environments, where the wireless communication\nchannels are doubly selective, thus making channel estimation and tracking a\nrelevant problem to investigate. In this paper, a novel deep learning\n(DL)-based weighted interpolation estimator is proposed to accurately estimate\nvehicular channels especially in high mobility scenarios. The proposed\nestimator is based on modifying the pilot allocation of the IEEE 802.11p\nstandard so that more transmission data rates are achieved. Extensive numerical\nexperiments demonstrate that the developed estimator significantly outperforms\nthe recently proposed DL-based frame-by-frame estimators in different vehicular\nscenarios, while substantially reducing the overall computational complexity.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:57:52 GMT"},{"version":"v2","created":"Mon, 17 Jan 2022 08:36:50 GMT"}],"update_date":"2022-01-19"}
{"id":"2104.08814","submitter":"Gang Yang","authors":"Gang Yang, Jialun Ping, Jorge Segovia","title":"Exotic resonances of fully-heavy tetraquarks in a lattice-QCD insipired\n  quark model","comments":"24 pages, 19 figures, 27 tables","journal-ref":"Phys. Rev. D 104, 014006 (2021)","doi":"10.1103/PhysRevD.104.014006","report-no":null,"categories":"hep-ph hep-ex hep-lat","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Fully-heavy tetraquark states, i.e. $cc\\bar{c}\\bar{c}$, $bb\\bar{b}\\bar{b}$,\n$bb\\bar{c}\\bar{c}$ ($cc\\bar{b}\\bar{b}$), $cb\\bar{c}\\bar{c}$,\n$cb\\bar{b}\\bar{b}$, and $cb\\bar{c}\\bar{b}$, are systematically investigated by\nmeans of a non-relativistic quark model based on lattice-QCD studies of the\ntwo-body $Q\\bar{Q}$ interaction, which exhibits a spin-independent Cornell\npotential along with a spin-spin term. The four-body problem is solved using\nthe Gaussian expansion method; additionally, the so-called complex scaling\ntechnique is employed so that bound, resonance, and scattering states can be\ntreated on the same footing. Moreover, a complete set of four-body\nconfigurations, including meson-meson, diquark-antidiquark, and K-type\nconfigurations, as well as their couplings, are considered for spin-parity\nquantum numbers $J^{P(C)}=0^{+(+)}$, $1^{+(\\pm)}$, and $2^{+(+)}$ in the\n$S$-wave channel. Several narrow resonances, with two-meson strong decay widths\nless than 30 MeV, are found in all of the tetraquark systems studied.\nParticularly, the fully-charm resonances recently reported by the LHCb\nCollaboration, at the energy range between 6.2 and 7.2 GeV in the di-$J/\\psi$\ninvariant spectrum, can be well identified in our calculation. Focusing on the\nfully-bottom tetraquark spectrum, resonances with masses between 18.9 and 19.6\nGeV are found. For the remaining charm-bottom cases, the masses are obtained\nwithin a energy region from 9.8 GeV to 16.4 GeV. All these predicted resonances\ncan be further examined in future experiments.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:59:46 GMT"}],"update_date":"2021-07-14"}
{"id":"2104.08815","submitter":"Bill Yuchen Lin","authors":"Bill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin Wang, Yufen Huang,\n  Christophe Dupuy, Rahul Gupta, Mahdi Soltanolkotabi, Xiang Ren, Salman\n  Avestimehr","title":"FedNLP: Benchmarking Federated Learning Methods for Natural Language\n  Processing Tasks","comments":"Accepted to NAACL 2022 Findings. Github:\n  https://github.com/FedML-AI/FedNLP","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Increasing concerns and regulations about data privacy and sparsity\nnecessitate the study of privacy-preserving, decentralized learning methods for\nnatural language processing (NLP) tasks. Federated learning (FL) provides\npromising approaches for a large number of clients (e.g., personal devices or\norganizations) to collaboratively learn a shared global model to benefit all\nclients while allowing users to keep their data locally. Despite interest in\nstudying FL methods for NLP tasks, a systematic comparison and analysis is\nlacking in the literature. Herein, we present the FedNLP, a benchmarking\nframework for evaluating federated learning methods on four different task\nformulations: text classification, sequence tagging, question answering, and\nseq2seq. We propose a universal interface between Transformer-based language\nmodels (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) under\nvarious non-IID partitioning strategies. Our extensive experiments with FedNLP\nprovide empirical comparisons between FL methods and helps us better understand\nthe inherent challenges of this direction. The comprehensive analysis points to\nintriguing and exciting future research aimed at developing FL methods for NLP\ntasks.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:04:49 GMT"},{"version":"v2","created":"Sat, 2 Oct 2021 03:58:13 GMT"},{"version":"v3","created":"Fri, 6 May 2022 10:05:52 GMT"}],"update_date":"2022-05-09"}
{"id":"2104.08816","submitter":"Zahra Bagheri","authors":"E. Peyghan, Z. Bagheri, I. Gultekin and A. Gezer","title":"Representations and Deformations of 3-Hom-$\\rho$-Lie algebras","comments":"15 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.RA math.AC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The aim of this paper is to introduce 3-Hom-$\\rho$-Lie algebra structures\ngeneralizing the algebras of 3-Hom-Lie algebra. Also, we investigate the\nrepresentations and deformations theory of this type of Hom-Lie algebras.\nMoreover, we introduce the definition of extensions and abelian extensions of\n3-Hom-$\\rho$-Lie algebras and show that associated to any abelian extension,\nthere is a representation and a 2-cocycle.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:14:02 GMT"},{"version":"v2","created":"Sat, 24 Apr 2021 17:37:12 GMT"}],"update_date":"2021-04-27"}
{"id":"2104.08817","submitter":"Javier Iranzo-S\\'anchez","authors":"Javier Iranzo-S\\'anchez and Jorge Civera and Alfons Juan","title":"Stream-level Latency Evaluation for Simultaneous Machine Translation","comments":"EMNLP 2021 Camera Ready","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Simultaneous machine translation has recently gained traction thanks to\nsignificant quality improvements and the advent of streaming applications.\nSimultaneous translation systems need to find a trade-off between translation\nquality and response time, and with this purpose multiple latency measures have\nbeen proposed. However, latency evaluations for simultaneous translation are\nestimated at the sentence level, not taking into account the sequential nature\nof a streaming scenario. Indeed, these sentence-level latency measures are not\nwell suited for continuous stream translation resulting in figures that are not\ncoherent with the simultaneous translation policy of the system being assessed.\nThis work proposes a stream-level adaptation of the current latency measures\nbased on a re-segmentation approach applied to the output translation, that is\nsuccessfully evaluated on streaming conditions for a reference IWSLT task.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:16:17 GMT"},{"version":"v2","created":"Wed, 8 Sep 2021 11:16:15 GMT"}],"update_date":"2021-09-09"}
{"id":"2104.08818","submitter":"Giacomo Cacciapaglia","authors":"Giacomo Cacciapaglia, Corentin Cot and Francesco Sannino","title":"Naturalness of lepton non-universality and muon g-2","comments":"5 pages, 2 figures","journal-ref":null,"doi":"10.1016/j.physletb.2021.136864","report-no":null,"categories":"hep-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We show that the observed anomalies in the lepton sector can be explained in\nextensions of the Standard Model that are natural and, therefore, resolve the\nHiggs sector hierarchy problem. The scale of new physics is around the TeV and\nTechnicolor-like theories are ideal candidate models.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:19:44 GMT"}],"update_date":"2022-01-05"}
{"id":"2104.08819","submitter":"Manjushree Laddha","authors":"Manjushree D. Laddha, Varsha T. Lokare, Arvind W. Kiwelekar and Laxman\n  D. Netak","title":"Classifications of the Summative Assessment for Revised Blooms Taxonomy\n  by using Deep Learning","comments":"8 pages, 7 figures, 2 tables","journal-ref":"International Journal of Engineering Trends and Technology\n  69.3(2021):211-218","doi":"10.14445/22315381/IJETT-V69I3P232","report-no":null,"categories":"cs.AI","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Education is the basic step of understanding the truth and the preparation of\nthe intelligence to reflect. Focused on the rational capacity of the human\nbeing the Cognitive process and knowledge dimensions of Revised Blooms Taxonomy\nhelps to differentiate the procedure of studying into six types of various\ncognitive processes and four types of knowledge dimensions. These types are\nsynchronized in the increasing level of difficulty. In this paper Software\nEngineering courses of B.Tech Computer Engineering and Information Technology\noffered by various Universities and Educational Institutes have been\ninvestigated for Revised Blooms Taxonomy RBT. Questions are a very useful\nconstituent. Knowledge intelligence and strength of the learners can be tested\nby applying questions.The fundamental goal of this paper is to create a\nrelative study of the classification of the summative assessment based on\nRevised Blooms Taxonomy using the Convolutional Neural Networks CNN Long\nShort-Term Memory LSTM of Deep Learning techniques in an endeavor to attain\nsignificant accomplishment and elevated precision levels.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:21:48 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08820","submitter":"Eliad Tsfadia","authors":"Niv Buchbinder, Iftach Haitner, Nissan Levi, Eliad Tsfadia","title":"Fair Coin Flipping: Tighter Analysis and the Many-Party Case","comments":"Published in SODA 2017","journal-ref":null,"doi":"10.1137/1.9781611974782.170","report-no":null,"categories":"cs.CR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In a multi-party fair coin-flipping protocol, the parties output a common\n(close to) unbiased bit, even when some adversarial parties try to bias the\noutput. In this work we focus on the case of an arbitrary number of corrupted\nparties. Cleve [STOC 1986] has shown that in any such $m$-round coin-flipping\nprotocol, the corrupted parties can bias the honest parties' common output bit\nby $\\Theta(1/m)$. For more than two decades, the best known coin-flipping\nprotocol was the one of Awerbuch et al. [Manuscript 1985], who presented a\n$t$-party, $m$-round protocol with bias $\\Theta(t/\\sqrt{m})$. This was changed\nby the breakthrough result of Moran et al. [TCC 2009], who constructed an\n$m$-round, two-party coin-flipping protocol with optimal bias $\\Theta(1/m)$.\nHaitner and Tsfadia [STOC 2014] constructed an $m$-round, three-party\ncoin-flipping protocol with bias $O(\\log^3m / m)$. Still for the case of more\nthan three parties, the best known protocol remained the\n$\\Theta(t/\\sqrt{m})$-bias protocol of Awerbuch et al.\n  We make a step towards eliminating the above gap, presenting a $t$-party,\n$m$-round coin-flipping protocol, with bias $O(\\frac{t^4 \\cdot 2^t \\cdot\n\\sqrt{\\log m}}{m^{1/2+1/\\left(2^{t-1}-2\\right)}})$ for any $t\\le \\tfrac12\n\\log\\log m$. This improves upon the $\\Theta(t/\\sqrt{m})$-bias protocol of\nAwerbuch et al., and in particular, for $t\\in O(1)$ it is an $1/m^{\\frac12 +\n\\Theta(1)}$-bias protocol. For the three-party case, it is an $O(\\sqrt{\\log\nm}/m)$-bias protocol, improving over the $O(\\log^3m / m)$-bias protocol of\nHaitner and Tsfadia.\n  Our protocol generalizes that of Haitner and Tsfadia, by presenting an\nappropriate recovery protocol for the remaining parties to interact in, in the\ncase that some parties abort or are caught cheating. We prove the fairness of\nthe new protocol by presenting a new paradigm for analyzing fairness of\ncoin-flipping protocols.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:25:26 GMT"},{"version":"v2","created":"Fri, 17 Jun 2022 14:56:55 GMT"}],"update_date":"2022-06-20"}
{"id":"2104.08821","submitter":"Tianyu Gao","authors":"Tianyu Gao, Xingcheng Yao, Danqi Chen","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings","comments":"Accepted to EMNLP 2021. The code and pre-trained models are available\n  at https://github.com/princeton-nlp/simcse","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper presents SimCSE, a simple contrastive learning framework that\ngreatly advances state-of-the-art sentence embeddings. We first describe an\nunsupervised approach, which takes an input sentence and predicts itself in a\ncontrastive objective, with only standard dropout used as noise. This simple\nmethod works surprisingly well, performing on par with previous supervised\ncounterparts. We find that dropout acts as minimal data augmentation, and\nremoving it leads to a representation collapse. Then, we propose a supervised\napproach, which incorporates annotated pairs from natural language inference\ndatasets into our contrastive learning framework by using \"entailment\" pairs as\npositives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on\nstandard semantic textual similarity (STS) tasks, and our unsupervised and\nsupervised models using BERT base achieve an average of 76.3% and 81.6%\nSpearman's correlation respectively, a 4.2% and 2.2% improvement compared to\nthe previous best results. We also show -- both theoretically and empirically\n-- that the contrastive learning objective regularizes pre-trained embeddings'\nanisotropic space to be more uniform, and it better aligns positive pairs when\nsupervised signals are available.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:27:08 GMT"},{"version":"v2","created":"Tue, 31 Aug 2021 17:47:06 GMT"},{"version":"v3","created":"Thu, 9 Sep 2021 22:09:51 GMT"},{"version":"v4","created":"Wed, 18 May 2022 12:29:49 GMT"}],"update_date":"2022-05-19"}
{"id":"2104.08822","submitter":"Sorin-Mihai Grad","authors":"Sorin-Mihai Grad, Felipe Lara","title":"An extension of the proximal point algorithm beyond convexity","comments":null,"journal-ref":"Journal of Global Optimization, 2021","doi":"10.1007/s10898-021-01081-4","report-no":null,"categories":"math.OC cs.NA math.NA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We introduce and investigate a new generalized convexity notion for functions\ncalled prox-convexity. The proximity operator of such a function is\nsingle-valued and firmly nonexpansive. We provide examples of (strongly)\nquasiconvex, weakly convex, and DC (difference of convex) functions that are\nprox-convex, however none of these classes fully contains the one of\nprox-convex functions or is included into it. We show that the classical\nproximal point algorithm remains convergent when the convexity of the proper\nlower semicontinuous function to be minimized is relaxed to prox-convexity.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:27:57 GMT"}],"update_date":"2021-11-30"}
{"id":"2104.08823","submitter":"Jun Mei","authors":"Lijuan Fan and Jun Mei","title":"Acoustic Metagrating Circulators: Nonreciprocal, Robust, and Tunable\n  Manipulation with Unitary Efficiency","comments":null,"journal-ref":"Phys. Rev. Applied 15, 064002 (2021)","doi":"10.1103/PhysRevApplied.15.064002","report-no":null,"categories":"physics.app-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Nonreciprocal signal operation is highly desired for various acoustic\napplications, where protection from unwanted backscattering can be realized so\nthat transmitting and receiving signals are processed in a full-duplex mode.\nHere we present the realization of a class of nonreciprocal circulators based\non simply structured acoustic metagratings, which consist only of a few solid\ncylinders and a steady fluid flow with low velocity. These innovative\nmetagratings are intelligently designed via a diffraction analysis of the\nlinearized potential flow equation and a genetic-algorithm-based optimization\nprocess. Unitary reflection efficiency between desired ports of the circulators\nare demonstrated through full-wave numerical simulations, confirming\nnonreciprocal and robust circulation of the acoustic signal over a broad range\nof flow velocity magnitude and profile. Our design provides a feasible degree\nof tunability, including switching from reciprocal to nonreciprocal operation\nand reversing the handedness of the circulator, presenting a convenient but\nefficient approach for the realization of nonreciprocal acoustic devices from\nwavelength-thick metagratings. It may find applications in various scenarios\nincluding underwater communication, energy harvesting, and acoustic sensing.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:28:20 GMT"}],"update_date":"2021-06-09"}
{"id":"2104.08824","submitter":"Xiaobo Qu","authors":"Yirong Zhou, Chen Qian, Yi Guo, Zi Wang, Jian Wang, Biao Qu, Di Guo,\n  Yongfu You, Xiaobo Qu","title":"XCloud-pFISTA: A Medical Intelligence Cloud for Accelerated MRI","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"eess.IV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Machine learning and artificial intelligence have shown remarkable\nperformance in accelerated magnetic resonance imaging (MRI). Cloud computing\ntechnologies have great advantages in building an easily accessible platform to\ndeploy advanced algorithms. In this work, we develop an open-access,\neasy-to-use and high-performance medical intelligence cloud computing platform\n(XCloud-pFISTA) to reconstruct MRI images from undersampled k-space data. Two\nstate-of-the-art approaches of the Projected Fast Iterative Soft-Thresholding\nAlgorithm (pFISTA) family have been successfully implemented on the cloud. This\nwork can be considered as a good example of cloud-based medical image\nreconstruction and may benefit the future development of integrated\nreconstruction and online diagnosis system.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:33:59 GMT"},{"version":"v2","created":"Fri, 11 Jun 2021 01:33:58 GMT"}],"update_date":"2021-06-14"}
{"id":"2104.08825","submitter":"Kaj Bostrom","authors":"Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, Greg Durrett","title":"Flexible Generation of Natural Language Deductions","comments":"Accepted to EMNLP 2021 (long paper). 9 pages (13 with references and\n  appendix), 8 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  An interpretable system for open-domain reasoning needs to express its\nreasoning process in a transparent form. Natural language is an attractive\nrepresentation for this purpose -- it is both highly expressive and easy for\nhumans to understand. However, manipulating natural language statements in\nlogically consistent ways is hard: models must cope with variation in how\nmeaning is expressed while remaining precise. In this paper, we describe\nParaPattern, a method for building models to generate deductive inferences from\ndiverse natural language inputs without direct human supervision. We train\nBART-based models (Lewis et al., 2020) to generate the result of applying a\nparticular logical operation to one or more premise statements. Crucially, we\ndevelop a largely automated pipeline for constructing suitable training\nexamples from Wikipedia. We evaluate our models using out-of-domain sentence\ncompositions from the QASC (Khot et al., 2020) and EntailmentBank (Dalvi et\nal., 2021) datasets as well as targeted perturbation sets. Our results show\nthat our models are substantially more accurate and flexible than baseline\nsystems. ParaPattern achieves 85% validity on examples of the 'substitution'\noperation from EntailmentBank without the use of any in-domain training data,\nmatching the performance of a model fine-tuned for EntailmentBank. The full\nsource code for our method is publicly available.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:36:26 GMT"},{"version":"v2","created":"Thu, 9 Sep 2021 17:06:26 GMT"}],"update_date":"2021-09-10"}
{"id":"2104.08826","submitter":"Kang Min Yoo","authors":"Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, Woomyeong Park","title":"GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation","comments":"Accepted to EMNLP2021 Findings; 11 pages, 7 tables, 2 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Large-scale language models such as GPT-3 are excellent few-shot learners,\nallowing them to be controlled via natural text prompts. Recent studies report\nthat prompt-based direct classification eliminates the need for fine-tuning but\nlacks data and inference scalability. This paper proposes a novel data\naugmentation technique that leverages large-scale language models to generate\nrealistic text samples from a mixture of real samples. We also propose\nutilizing soft-labels predicted by the language models, effectively distilling\nknowledge from the large-scale language models and creating textual\nperturbations simultaneously. We perform data augmentation experiments on\ndiverse classification tasks and show that our method hugely outperforms\nexisting text augmentation methods. Ablation studies and a qualitative analysis\nprovide more insights into our approach.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:39:33 GMT"},{"version":"v2","created":"Thu, 18 Nov 2021 07:56:58 GMT"}],"update_date":"2021-11-19"}
{"id":"2104.08827","submitter":"Jin-Hong Chen","authors":"Jin-Hong Chen and Rong-Feng Shen","title":"Light Curves of Partial Tidal Disruption Events","comments":"15 pages, 15 figures, accepted for publication in ApJ","journal-ref":null,"doi":"10.3847/1538-4357/abf9a7","report-no":null,"categories":"astro-ph.HE astro-ph.GA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Tidal disruption events (TDEs) can uncover the quiescent black holes (BHs) at\nthe center of galaxies and also offer a promising method to study them. In a\npartial TDE (PTDE), the BH's tidal force cannot fully disrupt the star, so the\nstellar core survives and only a varied portion of the stellar mass is bound to\nthe BH and feeds it. We calculate the event rate of PTDEs and full TDEs\n(FTDEs). In general, the event rate of PTDEs is higher than that of FTDEs,\nespecially for the larger BHs. And the detection rate of PTDEs is about dozens\nper year by Zwicky Transient Factory (ZTF). During the circularization process\nof the debris stream in PTDEs, no outflow can be launched due to the efficient\nradiative diffusion. The circularized debris ring then experiences viscous\nevolution and forms an accretion disk. We calculate the light curves of PTDEs\ncontributed by these two processes, along with their radiation temperature\nevolution. The light curves have double peaks and the spectra peak in UV.\nWithout obscuration or reprocessing of the radiation by an outflow, PTDEs\nprovide a clean environment to study the circularization and transient disk\nformation in TDEs.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:41:06 GMT"},{"version":"v2","created":"Thu, 22 Apr 2021 06:00:26 GMT"},{"version":"v3","created":"Wed, 9 Jun 2021 12:10:00 GMT"}],"update_date":"2021-06-23"}
{"id":"2104.08828","submitter":"Bai-Song Xie","authors":"Lie-Juan Li, Melike Mohamedsedik and Bai-Song Xie","title":"Enhanced dynamically assisted pair production in spatial inhomogeneous\n  electric fields with the frequency chirping","comments":"30 pages, 16 figures, 1 Table","journal-ref":"Phys. Rev. D 104, 036015 (2021)","doi":"10.1103/PhysRevD.104.036015","report-no":null,"categories":"hep-ph quant-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Chirped dynamically assisted pair production in spatial inhomogeneous\nelectric fields is studied by the Dirac-Heisenberg-Wigner formalism. The\neffects of the chirp parameter on the reduced momentum spectrum, the reduced\ntotal yield of the created pairs for either low or high frequency one-color\nfield and two-color dynamically assisted combinational fields are investigated\nin detail. Also, the enhancement factor is obtained in the later two-color\nfield case. It is found that for the low frequency field, no matter whether it\nis accompanied by the other high frequency field, its chirping has a little\neffect on the pair production. For the one-color high frequency field or/and\ntwo-color fields, the momentum spectrum exhibits incomplete interference and\nthe interference effect becomes more and more remarkable as chirp increases. We\nalso find that in the chirped dynamically assisted field, the reduced total\nyield is enhanced significantly when the chirps are acting on the two fields,\ncompared with that the chirp is acting only for the low frequency strong field.\nSpecifically, by the chirping, it is found the reduced pair number is increased\nby more than one order of magnitude in the field with a relative narrow spatial\nscale, while it is enhanced at least two times in other case of field with\nlarger spatial scales or even in the quasi-homogeneous region. We also obtain\nsome optimal chirp parameters and spatial scales for the total yield and\nenhancement factor in different scenarios of the studied external field. These\nresults may provide a theoretical basis for possible experiments in the future.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:44:46 GMT"},{"version":"v2","created":"Tue, 10 Aug 2021 09:13:30 GMT"}],"update_date":"2021-09-08"}
{"id":"2104.08829","submitter":"Valentin Hofmann","authors":"Valentin Hofmann, Xiaowen Dong, Janet B. Pierrehumbert, Hinrich\n  Sch\\\"utze","title":"Modeling Ideological Salience and Framing in Polarized Online Groups\n  with Graph Neural Networks and Structured Sparsity","comments":"NAACL 2022 (Findings)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.SI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The increasing polarization of online political discourse calls for\ncomputational tools that automatically detect and monitor ideological divides\nin social media. We introduce a minimally supervised method that leverages the\nnetwork structure of online discussion forums, specifically Reddit, to detect\npolarized concepts. We model polarization along the dimensions of salience and\nframing, drawing upon insights from moral psychology. Our architecture combines\ngraph neural networks with structured sparsity learning and results in\nrepresentations for concepts and subreddits that capture temporal ideological\ndynamics such as right-wing and left-wing radicalization.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:48:25 GMT"},{"version":"v2","created":"Thu, 29 Apr 2021 17:02:48 GMT"},{"version":"v3","created":"Wed, 14 Dec 2022 22:04:14 GMT"}],"update_date":"2022-12-16"}
{"id":"2104.08830","submitter":"Youmin Chen","authors":"Youmin Chen and Miaomiao Zhu","title":"Quantization for biharmonic maps from non-collapsed degenerating\n  Einstein 4-manifolds","comments":"72 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.DG math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  For a sequence of extrinsic or intrinsic biharmonic maps $u_j: M_j\\rightarrow\nN$ from a sequence of non-collapsed degenerating closed Einstein 4-manifolds\n$(M_j,g_j)$ with bounded Einstein constants, bounded diameters and bounded\n$L^2$ curvature energy into a compact Riemannian manifold $(N,h)$ with\nuniformly bounded biharmonic energy, we establish a compactness theory modular\nfinitely many bubbles, which are finite energy biharmonic maps from\n$\\mathbb{R}^4$, or from $\\mathbb{R}^4 / \\Gamma$ for some nontrivial finite\ngroup $\\Gamma \\subset SO(4)$, or from some complete, noncompact, Ricci flat,\nnon-flat ALE 4-manifold (orbifold). To achieve this, we develop a sophisticated\nasymptotic analysis for solutions over degenerating neck regions.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:53:24 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08831","submitter":"Abdeslem Lyaghfouri Prof.","authors":"Abdeslem Lyaghfouri","title":"Global Higher Integrability of the Gradient of the A-Laplace Equation\n  Solution","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.AP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, we establish higher integrability of the gradient of the\nsolution of the quasilinear elliptic equation\n$\\Delta_Au=\\text{div}\\left(\\frac{a(|F|)}{|F|}F\\right)$ in $\\mathbb{R}^n$, where\n$\\Delta_Au$ is the so called A-Laplace operator.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:54:45 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08832","submitter":"Manuel Lamp\\'on Gonz\\'alez-Albo","authors":"M. Lamp\\'on, M. L\\'opez-Puertas, S. Czesla, A. S\\'anchez-L\\'opez, L.\n  M. Lara, M. Salz, J. Sanz-Forcada, K. Molaverdikhani, A. Quirrenbach, E.\n  Pall\\'e, J. A. Caballero, Th. Henning, L. Nortmann, P. J. Amado, D. Montes,\n  A. Reiners, and I. Ribas","title":"Evidence of energy-, recombination-, and photon-limited escape regimes\n  in giant planet H/He atmospheres","comments":"Accepted to A&A. 9 Pages","journal-ref":null,"doi":"10.1051/0004-6361/202140423","report-no":null,"categories":"astro-ph.EP","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Hydrodynamic escape is the most efficient atmospheric mechanism of planetary\nmass loss and has a large impact on planetary evolution. Three hydrodynamic\nescape regimes have been identified theoretically: energy-limited,\nrecombination-limited, and photon-limited. However, no evidence of these\nregimes had been reported until now. Here, we report evidence of these three\nregimes via an analysis of helium I triplet at 10830 angstroms and Ly-$\\alpha$\nabsorption involving a 1D hydrodynamic model that allows us to estimate\nhydrogen recombination and advection rates. In particular, we show that HD\n209458 b is in the energy-limited regime, HD 189733 b is in the\nrecombination-limited regime, and GJ 3470 b is in the photon-limited regime.\nThese exoplanets can be considered as benchmark cases for their respective\nregimes.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:03:47 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08833","submitter":"Muhammet Cihat Da\\u{g}l{\\i}","authors":"Muhammet Cihat Da\\u{g}li","title":"Degenerate Fubini-type polynomials associated with degenerate\n  Apostol-Bernoulli and Apostol-Euler polynomials of order {\\alpha}","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.NT","license":"http://creativecommons.org/publicdomain/zero/1.0/","abstract":"  In this paper, by introducing the degenerate Fubini-type polynomials, we give\nseveral relations with the help of the Fa\\`a di Bruno formula and some\nproperties of Bell polynomials, and generating function methods. Also, we\nderive some new explicit formulas and recurrence relations for Fubini-type\npolynomials and numbers. Associating the degenerate Fubini-type polynomials\nnewly defined here with degenerate Apostol-Bernoulli polynomials and degenerate\nApostol-Euler polynomials of order {\\alpha} enables us to present additional\nrelations for some degenerate special polynomials and numbers.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:06:22 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08834","submitter":"Ali Esmaeily","authors":"Ali Esmaeily, Katina Kralevska","title":"Small-Scale 5G Testbeds for Network Slicing Deployment: A Systematic\n  Review","comments":"Accepted for publication in Wireless Communications and Mobile\n  Computing, 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.NI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Developing specialized cloud-based and open-source testbeds is a practical\napproach to investigate network slicing functionalities in the fifth-generation\n(5G) mobile networks. This paper provides a comprehensive review of most of the\nexisting cost-efficient and small-scale testbeds that partially or fully deploy\nnetwork slicing. First, we present relevant software packages for the three\nmain functional blocks of the ETSI NFV MANO framework and for emulating the\naccess and core network domains. Second, we define primary and secondary design\ncriteria for deploying network slicing testbeds. These design criteria are\nlater used for comparison between the testbeds. Third, we present the\nstate-of-the-art testbeds, including their design objectives, key technologies,\nnetwork slicing deployment, and experiments. Next, we evaluate the testbeds\naccording to the defined design criteria and present an in-depth summary table.\nThis assessment concludes with the superiority of some of them over the rest\nand the most dominant software packages satisfying the ETSI NFV MANO framework.\nFinally, challenges, potential solutions, and future works of network slicing\ntestbeds are discussed.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:09:57 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08835","submitter":"Qinyuan Ye","authors":"Qinyuan Ye, Bill Yuchen Lin, Xiang Ren","title":"CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in\n  NLP","comments":"Accepted to EMNLP 2021. Camera-ready version. Code:\n  https://github.com/INK-USC/CrossFit","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Humans can learn a new language task efficiently with only few examples, by\nleveraging their knowledge obtained when learning prior tasks. In this paper,\nwe explore whether and how such cross-task generalization ability can be\nacquired, and further applied to build better few-shot learners across diverse\nNLP tasks. We introduce CrossFit, a problem setup for studying cross-task\ngeneralization ability, which standardizes seen/unseen task partitions, data\naccess during different learning stages, and the evaluation protocols. To\ninstantiate different seen/unseen task partitions in CrossFit and facilitate\nin-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse\nfew-shot NLP tasks created from open-access NLP datasets and converted to a\nunified text-to-text format. Our analysis reveals that the few-shot learning\nability on unseen tasks can be improved via an upstream learning stage using a\nset of seen tasks. We also observe that the selection of upstream learning\ntasks can significantly influence few-shot performance on unseen tasks, asking\nfurther analysis on task similarity and transferability.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:14:46 GMT"},{"version":"v2","created":"Thu, 30 Sep 2021 22:36:50 GMT"}],"update_date":"2021-10-04"}
{"id":"2104.08836","submitter":"Lei Cui","authors":"Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei\n  Florencio, Cha Zhang, Furu Wei","title":"LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich\n  Document Understanding","comments":"Work in progress","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Multimodal pre-training with text, layout, and image has achieved SOTA\nperformance for visually-rich document understanding tasks recently, which\ndemonstrates the great potential for joint learning across different\nmodalities. In this paper, we present LayoutXLM, a multimodal pre-trained model\nfor multilingual document understanding, which aims to bridge the language\nbarriers for visually-rich document understanding. To accurately evaluate\nLayoutXLM, we also introduce a multilingual form understanding benchmark\ndataset named XFUND, which includes form understanding samples in 7 languages\n(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and\nkey-value pairs are manually labeled for each language. Experiment results show\nthat the LayoutXLM model has significantly outperformed the existing SOTA\ncross-lingual pre-trained models on the XFUND dataset. The pre-trained\nLayoutXLM model and the XFUND dataset are publicly available at\nhttps://aka.ms/layoutxlm.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:16:00 GMT"},{"version":"v2","created":"Fri, 27 Aug 2021 06:28:49 GMT"},{"version":"v3","created":"Thu, 9 Sep 2021 11:37:48 GMT"}],"update_date":"2021-09-10"}
{"id":"2104.08837","submitter":"Daizhan Cheng Dr","authors":"Daizhan Cheng, Lijun Zhang, Dongyao Bi","title":"Invariant Subspace Approach to Boolean (Control) Networks","comments":"23 pages, 5 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  A logical function can be used to characterizing a property of a state of\nBoolean network (BN), which is considered as an aggregation of states. To\nillustrate the dynamics of a set of logical functions, which characterize our\nconcerned properties of a BN, the invariant subspace containing the set of\nlogical functions is proposed, and its properties are investigated. Then the\ninvariant subspace of Boolean control network (BCN) is also proposed. The\ndynamics of invariant subspace of BCN is also invariant. Finally, using outputs\nas the set of logical functions, the minimum realization of BCN is proposed,\nwhich provides a possible solution to overcome the computational complexity of\nlarge scale BNs/BCNs.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:22:28 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08838","submitter":"Yuanzhi Wang","authors":"Yuanzhi Wang and Tao Lu and Yanduo Zhang and Yuntao Wu","title":"Multi-scale Self-calibrated Network for Image Light Source Transfer","comments":"8 pages,4 figures","journal-ref":"CVPR 2021","doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Image light source transfer (LLST), as the most challenging task in the\ndomain of image relighting, has attracted extensive attention in recent years.\nIn the latest research, LLST is decomposed three sub-tasks: scene reconversion,\nshadow estimation, and image re-rendering, which provides a new paradigm for\nimage relighting. However, many problems for scene reconversion and shadow\nestimation tasks, including uncalibrated feature information and poor semantic\ninformation, are still unresolved, thereby resulting in insufficient feature\nrepresentation. In this paper, we propose novel down-sampling feature\nself-calibrated block (DFSB) and up-sampling feature self-calibrated block\n(UFSB) as the basic blocks of feature encoder and decoder to calibrate feature\nrepresentation iteratively because the LLST is similar to the recalibration of\nimage light source. In addition, we fuse the multi-scale features of the\ndecoder in scene reconversion task to further explore and exploit more semantic\ninformation, thereby providing more accurate primary scene structure for image\nre-rendering. Experimental results in the VIDIT dataset show that the proposed\napproach significantly improves the performance for LLST.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:23:01 GMT"}],"update_date":"2021-04-22"}
{"id":"2104.08839","submitter":"Sergey Goryainov V.","authors":"Sergey Goryainov, Huiqiu Lin","title":"On balanced characteristic functions of canonical cliques in Paley\n  graphs of square order","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.CO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper we prove that balanced characteristic functions of canonical\ncliques in a Paley graph of square order $P(q^2)$ span the\n$\\frac{-1+q}{2}$-eigenspace of the graph. This is the first of two steps to a\nsecond proof of the analogue of Erd\\\"os-Ko-Rado theorem for Paley graphs of\nsquare order (the first proof was given by A. Blokhuis in 1984).\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:31:00 GMT"},{"version":"v2","created":"Wed, 21 Apr 2021 13:23:59 GMT"}],"update_date":"2021-04-22"}
{"id":"2104.08840","submitter":"Qinyuan Ye","authors":"Qinyuan Ye, Belinda Z. Li, Sinong Wang, Benjamin Bolte, Hao Ma,\n  Wen-tau Yih, Xiang Ren, Madian Khabsa","title":"On the Influence of Masking Policies in Intermediate Pre-training","comments":"Accepted to EMNLP 2021. Camera-ready version","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.LG","license":"http://creativecommons.org/licenses/by-sa/4.0/","abstract":"  Current NLP models are predominantly trained through a two-stage \"pre-train\nthen fine-tune\" pipeline. Prior work has shown that inserting an intermediate\npre-training stage, using heuristic masking policies for masked language\nmodeling (MLM), can significantly improve final performance. However, it is\nstill unclear (1) in what cases such intermediate pre-training is helpful, (2)\nwhether hand-crafted heuristic objectives are optimal for a given task, and (3)\nwhether a masking policy designed for one task is generalizable beyond that\ntask. In this paper, we perform a large-scale empirical study to investigate\nthe effect of various masking policies in intermediate pre-training with nine\nselected tasks across three categories. Crucially, we introduce methods to\nautomate the discovery of optimal masking policies via direct supervision or\nmeta-learning. We conclude that the success of intermediate pre-training is\ndependent on appropriate pre-train corpus, selection of output format (i.e.,\nmasked spans or full sentence), and clear understanding of the role that MLM\nplays for the downstream task. In addition, we find our learned masking\npolicies outperform the heuristic of masking named entities on TriviaQA, and\npolicies learned from one task can positively transfer to other tasks in\ncertain cases, inviting future research in this direction.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:32:23 GMT"},{"version":"v2","created":"Thu, 30 Sep 2021 23:52:48 GMT"}],"update_date":"2021-10-04"}
{"id":"2104.08841","submitter":"Benrong Mu","authors":"Jing Liang, Benrong Mu and Peng Wang","title":"Joule-Thomson expansion of Lower-dimensional black hole","comments":"18 pages, 3 figures and 1 table, some minor changes","journal-ref":null,"doi":"10.1103/PhysRevD.104.124003","report-no":null,"categories":"gr-qc hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The Joule-Thomson expansion is extended to the lower-dimensional regime by\nconsidering the rotating BTZ metric in the (2+1)-dimensional space-time.\nSpecifically, the properties of three important aspects of the Joule-Thomson\nexpansion, namely the Joule-Thomson coefficient, the inversion curve and the\nisenthalpic curve are focused on. The divergence point of the Joule-Thomson\ncoefficient and the zero point of the Hawking temperature are studied. The\ninversion temperature curves and isenthalpic curves in the $T-P$ plane are\nobtained and the cooling-heating regions are determined. Furthermore, the\nminimum inversion temperature is found to be zero, and the black hole becomes\nan extremal black hole. The ratio between the minimum inversion temperature and\nthe critical temperature for the BTZ black hole doesn't exist, since the BTZ\nblack hole does not have the critical behavior in $P_c$, $T_c$ and $V_c$.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:39:27 GMT"},{"version":"v2","created":"Tue, 20 Apr 2021 13:51:08 GMT"}],"update_date":"2021-12-15"}
{"id":"2104.08842","submitter":"Avijit Basak","authors":"Avijit Basak","title":"A Rank based Adaptive Mutation in Genetic Algorithm","comments":null,"journal-ref":"August 2020 International Journal of Computer Applications 175","doi":"10.5120/ijca2020920572","report-no":null,"categories":"cs.NE cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Traditionally Genetic Algorithm has been used for optimization of unimodal\nand multimodal functions. Earlier researchers worked with constant\nprobabilities of GA control operators like crossover, mutation etc. for tuning\nthe optimization in specific domains. Recent advancements in this field\nwitnessed adaptive approach in probability determination. In Adaptive mutation\nprimarily poor individuals are utilized to explore state space, so mutation\nprobability is usually generated proportionally to the difference between\nfitness of best chromosome and itself (fMAX - f). However, this approach is\nsusceptible to nature of fitness distribution during optimization. This paper\npresents an alternate approach of mutation probability generation using\nchromosome rank to avoid any susceptibility to fitness distribution.\nExperiments are done to compare results of simple genetic algorithm (SGA) with\nconstant mutation probability and adaptive approaches within a limited resource\nconstraint for unimodal, multimodal functions and Travelling Salesman Problem\n(TSP). Measurements are done for average best fitness, number of generations\nevolved and percentage of global optimum achievements out of several trials.\nThe results demonstrate that the rank-based adaptive mutation approach is\nsuperior to fitness-based adaptive approach as well as SGA in a multimodal\nproblem space.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:41:33 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08843","submitter":"Ravi Tomar","authors":"Ravi Tomar","title":"Boundaries of graphs of relatively hyperbolic groups with cyclic edge\n  groups","comments":"25 pages, Two new sections have been added. Typos and exposition of\n  the paper are improved. Lemma 2.5 and proposition 6.4 are added. arXiv admin\n  note: text overlap with arXiv:math/0203258 by other authors","journal-ref":null,"doi":null,"report-no":null,"categories":"math.GR","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We prove that the fundamental group of a finite graph of convergence groups\nwith parabolic edge groups is a convergence group. Using this result, under\nsome mild assumptions, we prove a combination theorem for a graph of\nconvergence groups with dynamically quasi-convex edge groups (Theorem 1.3). To\nprove these results, we use a modification of Dahmani's technique [Dah03]. Then\nwe show that the fundamental group of a graph of relatively hyperbolic groups\nwith edge groups either parabolic or infinite cyclic is relatively hyperbolic\nand construct Bowditch boundary. Finally, we show that the homeomorphism type\nof Bowditch boundary of the fundamental group of a graph of relatively\nhyperbolic groups with parabolic edge groups is determined by the homeomorphism\ntype of the Bowditch boundaries of vertex groups (under some additional\nhypotheses)(Theorem 7.1). In the last section of the paper, we give some\napplications and examples.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:46:51 GMT"},{"version":"v2","created":"Sat, 5 Feb 2022 04:49:25 GMT"}],"update_date":"2022-02-08"}
{"id":"2104.08844","submitter":"Li Zhang","authors":"Li Zhang, Yihao Yang, Yong Ge, Yi-jun Guan, Qiaolu Chen, Qinghui Yan,\n  Fujia Chen, Rui Xi, Yuanzhen Li, Ding Jia, Shou-qi Yuan, Hong-xiang Sun,\n  Hongsheng Chen, and Baile Zhang","title":"Acoustic non-Hermitian skin effect from twisted winding topology","comments":"14 pages, 4 figures","journal-ref":"Nature Communications 12, 6297 (2021)","doi":"10.1038/s41467-021-26619-8","report-no":null,"categories":"physics.app-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The recently discovered non-Hermitian skin effect (NHSE) manifests the\nbreakdown of current classification of topological phases in\nenergy-nonconservative systems, and necessitates the introduction of\nnon-Hermitian band topology. So far, all NHSE observations are based on one\ntype of non-Hermitian band topology, in which the complex energy spectrum winds\nalong a closed loop. As recently characterized along a synthetic dimension on a\nphotonic platform, non-Hermitian band topology can exhibit almost arbitrary\nwindings in momentum space, but their actual phenomena in real physical systems\nremain unclear. Here, we report the experimental realization of NHSE in a\none-dimensional (1D) non-reciprocal acoustic crystal. With direct acoustic\nmeasurement, we demonstrate that a twisted winding, whose topology consists of\ntwo oppositely oriented loops in contact rather than a single loop, will\ndramatically change the NHSE, following previous predictions of unique features\nsuch as the bipolar localization and the Bloch point for a Bloch-wave-like\nextended state. This work reveals previously unnoticed features of NHSE, and\nprovides the observation of physical phenomena originating from complex\nnon-Hermitian winding topology.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:51:24 GMT"},{"version":"v2","created":"Wed, 10 Nov 2021 02:28:06 GMT"}],"update_date":"2021-11-11"}
{"id":"2104.08845","submitter":"Kecheng Chen","authors":"Kecheng Chen, Kun Long, Yazhou Ren, Jiayu Sun and Xiaorong Pu","title":"Lesion-Inspired Denoising Network: Connecting Medical Image Denoising\n  and Lesion Detection","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Deep learning has achieved notable performance in the denoising task of\nlow-quality medical images and the detection task of lesions, respectively.\nHowever, existing low-quality medical image denoising approaches are\ndisconnected from the detection task of lesions. Intuitively, the quality of\ndenoised images will influence the lesion detection accuracy that in turn can\nbe used to affect the denoising performance. To this end, we propose a\nplay-and-plug medical image denoising framework, namely Lesion-Inspired\nDenoising Network (LIDnet), to collaboratively improve both denoising\nperformance and detection accuracy of denoised medical images. Specifically, we\npropose to insert the feedback of downstream detection task into existing\ndenoising framework by jointly learning a multi-loss objective. Instead of\nusing perceptual loss calculated on the entire feature map, a novel\nregion-of-interest (ROI) perceptual loss induced by the lesion detection task\nis proposed to further connect these two tasks. To achieve better optimization\nfor overall framework, we propose a customized collaborative training strategy\nfor LIDnet. On consideration of clinical usability and imaging characteristics,\nthree low-dose CT images datasets are used to evaluate the effectiveness of the\nproposed LIDnet. Experiments show that, by equipping with LIDnet, both of the\ndenoising and lesion detection performance of baseline methods can be\nsignificantly improved.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:53:36 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08846","submitter":"Geoffrey Stewart Morrison","authors":"Geoffrey Stewart Morrison","title":"Tutorial on logistic-regression calibration and fusion: Converting a\n  score to a likelihood ratio","comments":"26 pages, 11 figures","journal-ref":"Australian Journal of Forensic Sciences, 45, 173-197 (2013)","doi":"10.1080/00450618.2012.733025","report-no":null,"categories":"stat.AP","license":"http://creativecommons.org/licenses/by-nc-sa/4.0/","abstract":"  Logistic-regression calibration and fusion are potential steps in the\ncalculation of forensic likelihood ratios. The present paper provides a\ntutorial on logistic-regression calibration and fusion at a practical\nconceptual level with minimal mathematical complexity. A score is\nlog-likelihood-ratio like in that it indicates the degree of similarity of a\npair of samples while taking into consideration their typicality with respect\nto a model of the relevant population. A higher-valued score provides more\nsupport for the same-origin hypothesis over the different-origin hypothesis\nthan does a lower-valued score; however, the absolute values of scores are not\ninterpretable as log likelihood ratios. Logistic-regression calibration is a\nprocedure for converting scores to log likelihood ratios, and\nlogistic-regression fusion is a procedure for converting parallel sets of\nscores from multiple forensic-comparison systems to log likelihood ratios.\nLogistic-regression calibration and fusion were developed for automatic speaker\nrecognition and are popular in forensic voice comparison. They can also be\napplied in other branches of forensic science, a fingerprint/fingermark example\nis provided.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:55:25 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08847","submitter":"Liang Tang","authors":"Fang-Hui Yin, Wen-Ya Tian, Liang Tang, Zhi-Hui Guo","title":"Determination of the up/down-quark mass within QCD sum rules in the\n  scalar channel","comments":"25 pages,7 figures,2 tables","journal-ref":"Eur.Phys.J.C 81,818(2021)","doi":"10.1140/epjc/s10052-021-09599-3","report-no":null,"categories":"hep-ph hep-ex","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this work, we determine up/down-quark mass $m_{q=u/d}$ in the isoscalar\nscalar channel from both the Shifman-Vainshtein-Zakharov (SVZ) and the\nMonte-Carlo-based QCD sum rules. The relevant spectral function, including the\ncontributions from the $f_0(500)$, $f_0(980)$ and $f_0(1370)$ resonances, is\ndetermined from a sophisticated $U(3)$ chiral study. Via the traditional SVZ\nQCD sum rules, we give the prediction to the average light-quark mass $m_q(2 \\,\n\\text{GeV})=\\frac{1}{2}(m_u(2 \\, \\text{GeV}) + m_d(2 \\,\n\\text{GeV}))=(3.46^{+0.16}_{-0.22} \\pm 0.33) \\, \\text{MeV}$. Meanwhile, by\nconsidering the uncertainties of the input QCD parameters and the spectral\nfunctions of the isoscalar scalar channel, we obtain $m_q (2\\, \\text{GeV}) =\n(3.44 \\pm 0.14 \\pm 0.32) \\, \\text{MeV}$ from the Monte-Carlo-based QCD sum\nrules. Both results are perfectly consistent with each other, and nicely agree\nwith the Particle Data Group value within the uncertainties.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:56:43 GMT"},{"version":"v2","created":"Sat, 18 Sep 2021 01:38:46 GMT"}],"update_date":"2021-09-29"}
{"id":"2104.08848","submitter":"Kazuhiko Seki","authors":"Kazuhiko Seki","title":"On the definition of the domain growth rate constant on a two\n  dimensional substrate","comments":"1 figure","journal-ref":"Journal of Crystal Growth 570 (2021) 126222","doi":"10.1016/j.jcrysgro.2021.126222","report-no":null,"categories":"cond-mat.stat-mech cond-mat.mtrl-sci","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In chemical vapor deposition (CVD) methods, the domain grows by attachment of\ndiffusing surface bound species on the substrate to an island of solid domain.\nWe formulate the process of single domain growth under two-dimensional\ndiffusion by taking into account the movement of the domain boundary. We first\ndiscuss two types of definition of the domain area growth rate constant; the\none defined through the domain size divided by the time duration of CVD growth\nand the other defined through the area divided by time. Then, we show that the\ndomain size is proportional to time for the reaction limited growth and the\ndomain area is proportional to time for the diffusion limited growth. We also\nshow that the domain area growth rate changes from the reaction limited growth\nto the diffusion limited growth as the domain size increases beyond a\ncharacteristic size.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:12:49 GMT"},{"version":"v2","created":"Sun, 16 May 2021 09:32:51 GMT"}],"update_date":"2021-06-23"}
{"id":"2104.08849","submitter":"Alexey V. Lebedev","authors":"Alexey V. Lebedev","title":"Maximal Branching Processes in Random Environment","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.PR","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The work continues the author's many-year research in theory of maximal\nbranching processes, which are obtained from classical branching processes by\nreplacing the summation of descendant numbers with taking the maximum. One can\nsay that in each generation, descendants of only one particle survive, namely\nthose of the particle that has the largest number of descendants. Earlier, the\nauthor generalized processes with integer values to processes with arbitrary\nnonnegative values, investigated their properties, and proved limit theorems.\nThen processes with several types of particles were introduced and studied. In\nthe present paper we introduce the notion of maximal branching processes in\nrandom environment (with a single type of particles) and an important case of a\n\"power-law\" random environment. In the latter case, properties of maximal\nbranching processes are studied and the ergodic theorem is proved. As\napplications, we consider gated infinite-server queues.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:20:18 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08850","submitter":"Kemal Tugrul Yesilbek","authors":"Kemal Tugrul Yesilbek, T. Metin Sezgin","title":"On Training Sketch Recognizers for New Domains","comments":"Accepted for The 1st Workshop on Sketch-Oriented Deep Learning\n  (SketchDL) @ CVPR 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Sketch recognition algorithms are engineered and evaluated using publicly\navailable datasets contributed by the sketch recognition community over the\nyears. While existing datasets contain sketches of a limited set of generic\nobjects, each new domain inevitably requires collecting new data for training\ndomain specific recognizers. This gives rise to two fundamental concerns:\nFirst, will the data collection protocol yield ecologically valid data? Second,\nwill the amount of collected data suffice to train sufficiently accurate\nclassifiers? In this paper, we draw attention to these two concerns. We show\nthat the ecological validity of the data collection protocol and the ability to\naccommodate small datasets are significant factors impacting recognizer\naccuracy in realistic scenarios. More specifically, using sketch-based gaming\nas a use case, we show that deep learning methods, as well as more traditional\nmethods, suffer significantly from dataset shift. Furthermore, we demonstrate\nthat in realistic scenarios where data is scarce and expensive, standard\nmeasures taken for adapting deep learners to small datasets fall short of\ncomparing favorably with alternatives. Although transfer learning, and\nextensive data augmentation help deep learners, they still perform\nsignificantly worse compared to standard setups (e.g., SVMs and GBMs with\nstandard feature representations). We pose learning from small datasets as a\nkey problem for the deep sketch recognition field, one which has been ignored\nin the bulk of the existing literature.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:24:49 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08851","submitter":"Jacques Smulevici","authors":"Grigorios Fournodavlos, Jacques Smulevici","title":"The initial boundary value problem in General Relativity: the umbilic\n  case","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"gr-qc math.AP math.DG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We give a short proof of local well-posedness for the initial boundary value\nproblem in general relativity with sole boundary condition the requirement that\nthe boundary is umbilic. This includes as a special case the totally geodesic\nboundary condition that we had previously addressed in [8]. The proof is based\non wave coordinates and the key observation that the momentum constraint is\nalways valid for umbilic boundaries. This allows for a greater freedom in the\nchoice of boundary conditions, since imposing the umbilic condition also\nprovides Neumann boundary conditions for three of the four wave coordinates\nconditions. Moreover, the umbilic condition, being geometric, implies that\ngeometric uniqueness in the sense of Friedrich holds in this specific case.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:25:57 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08852","submitter":"Xiaoyu Li","authors":"Xiaoyu Li, Bo Zhang, Jing Liao, Pedro V. Sander","title":"Let's See Clearly: Contaminant Artifact Removal for Moving Cameras","comments":"10 pages, 11 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Contaminants such as dust, dirt and moisture adhering to the camera lens can\ngreatly affect the quality and clarity of the resulting image or video. In this\npaper, we propose a video restoration method to automatically remove these\ncontaminants and produce a clean video. Our approach first seeks to detect\nattention maps that indicate the regions that need to be restored. In order to\nleverage the corresponding clean pixels from adjacent frames, we propose a flow\ncompletion module to hallucinate the flow of the background scene to the\nattention regions degraded by the contaminants. Guided by the attention maps\nand completed flows, we propose a recurrent technique to restore the input\nframe by fetching clean pixels from adjacent frames. Finally, a multi-frame\nprocessing stage is used to further process the entire video sequence in order\nto enforce temporal consistency. The entire network is trained on a synthetic\ndataset that approximates the physical lighting properties of contaminant\nartifacts. This new dataset and our novel framework lead to our method that is\nable to address different contaminants and outperforms competitive restoration\napproaches both qualitatively and quantitatively.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:37:34 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08853","submitter":"Henrik Johannesson","authors":"Oleksandr Balabanov, Daniel Erkensten, Henrik Johannesson","title":"Topology of critical chiral phases: multiband insulators and\n  superconductors","comments":"16 pages, 2 figures; typos corrected, added text and references","journal-ref":"Phys. Rev. Research 3, 043048 (2021)","doi":"10.1103/PhysRevResearch.3.043048","report-no":null,"categories":"cond-mat.mes-hall cond-mat.quant-gas cond-mat.stat-mech","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Recent works have proved the existence of symmetry-protected edge states in\ncertain one-dimensional topological band insulators and superconductors at the\ngap-closing points which define quantum phase transitions between two\ntopologically nontrivial phases. We show how this picture generalizes to\nmultiband critical models belonging to any of the chiral symmetry classes AIII,\nBDI, or CII of noninteracting fermions in one dimension.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:38:19 GMT"},{"version":"v2","created":"Tue, 19 Oct 2021 14:10:32 GMT"}],"update_date":"2021-10-20"}
{"id":"2104.08854","submitter":"Jia Wang","authors":"Jia Wang, Ping Wang, Biao Li, Ruigang Fu, Junzheng Wu","title":"An Improved Discriminative Optimization for 3D Rigid Point Cloud\n  Registration","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The Discriminative Optimization (DO) algorithm has been proved much\nsuccessful in 3D point cloud registration. In the original DO, the feature\n(descriptor) of two point cloud was defined as a histogram, and the element of\nhistogram indicates the weights of scene points in \"front\" or \"back\" side of a\nmodel point. In this paper, we extended the histogram which indicate the sides\nfrom \"front-back\" to \"front-back\", \"up-down\", and \"clockwise-anticlockwise\". In\naddition, we reweighted the extended histogram according to the model points'\ndistribution. We evaluated the proposed Improved DO on the Stanford Bunny and\nOxford SensatUrban dataset, and compared it with six classical State-Of-The-Art\npoint cloud registration algorithms. The experimental result demonstrates our\nalgorithm achieves comparable performance in point registration accuracy and\nroot-mean-sqart-error.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:39:52 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08855","submitter":"Yilin Chen","authors":"Yilin Chen","title":"Sums of products of Bessel functions and order derivatives of Bessel\n  functions","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.CA math-ph math.MP","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this paper, sums represented in (3) are studied. The expressions are\nderived in terms of Bessel functions of the first and second kinds and their\nintegrals. Further, we point out the integrals can be written as a Meijer G\nfunction.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:47:34 GMT"},{"version":"v2","created":"Wed, 21 Apr 2021 15:46:52 GMT"}],"update_date":"2021-04-22"}
{"id":"2104.08856","submitter":"Peter Hugo Nelson","authors":"Peter Hugo Nelson","title":"Introductory models of COVID-19 in the United States","comments":"37 pages, 13 figures","journal-ref":"The Biophysicist (2021) 2 (3): 74-98","doi":"10.35459/tbp.2021.000200","report-no":null,"categories":"physics.ed-ph physics.soc-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Students develop and test simple models of the spread of COVID-19. Microsoft\nExcel is used as the modeling platform because it's non-threatening to students\nand because it's widely available. Students develop finite difference models\nand implement them in the cells of preformatted spreadsheets following a\nguided-inquiry pedagogy that introduces new model parameters in a scaffolded\nstep-by-step manner. That approach allows students to investigate the\nimplications of new model parameters in a systematic way. Students fit the\nresulting models to reported cases-per-day data for the United States using\nleast-squares techniques with Excel's Solver. Using their own spreadsheets,\nstudents discover for themselves that the initial exponential growth of\nCOVID-19 can be explained by a simplified unlimited growth model and by the SIR\nmodel. They also discover that the effects of social distancing can be modeled\nusing a Gaussian transition function for the infection rate coefficient and\nthat the summer surge was caused by prematurely relaxing social distancing and\nthen reimposing stricter social distancing. Students then model the effect of\nvaccinations and validate the resulting SIRV model by showing that it\nsuccessfully predicts the reported cases-per-day data from Thanksgiving through\nFebruary 14, 2021. The same SIRV model is then extended and successfully fits\nthe fourth peak up to June 1, 2021, caused by further relaxation of social\ndistancing measures. Finally, students extend the model up to the present day\nand successfully account for the appearance of the delta variant of SARS-CoV-2.\nThe fitted model also predicts that the delta-variant peak will be\ncomparatively short, and the cases-per-day data should begin to fall off in\nearly September 2021 - counter to current expectations. This case study would\nmake an excellent capstone experience for students interested in scientific\nmodeling.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:51:52 GMT"},{"version":"v2","created":"Sun, 29 Aug 2021 01:03:28 GMT"},{"version":"v3","created":"Mon, 4 Oct 2021 01:19:54 GMT"}],"update_date":"2021-12-14"}
{"id":"2104.08857","submitter":"Yu-Ping Ruan","authors":"Yu-Ping Ruan, and Zhen-Hua Ling","title":"Emotion-Regularized Conditional Variational Autoencoder for Emotional\n  Response Generation","comments":"Accepted by IEEE Transactions on Affective Computing","journal-ref":null,"doi":"10.1109/TAFFC.2021.3073809","report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper presents an emotion-regularized conditional variational\nautoencoder (Emo-CVAE) model for generating emotional conversation responses.\nIn conventional CVAE-based emotional response generation, emotion labels are\nsimply used as additional conditions in prior, posterior and decoder networks.\nConsidering that emotion styles are naturally entangled with semantic contents\nin the language space, the Emo-CVAE model utilizes emotion labels to regularize\nthe CVAE latent space by introducing an extra emotion prediction network. In\nthe training stage, the estimated latent variables are required to predict the\nemotion labels and token sequences of the input responses simultaneously.\nExperimental results show that our Emo-CVAE model can learn a more informative\nand structured latent space than a conventional CVAE model and output responses\nwith better content and emotion performance than baseline CVAE and\nsequence-to-sequence (Seq2Seq) models.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:53:20 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08858","submitter":"Svjetlana Terzic","authors":"Victor M. Buchstaber, Svjetlana Terzi\\'c","title":"The orbit spaces $G_{n,2}/T^n$ and the Chow quotients\n  $G_{n,2}\\!/\\!/(\\mathbb{C} ^{\\ast})^{n}$ of the Grassmann manifolds $G_{n,2}$","comments":"32 pages","journal-ref":null,"doi":null,"report-no":null,"categories":"math.AG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The focus of our paper is on the complex Grassmann manifolds $G_{n,2}$ which\nappear as one of the fundamental objects in developing the interaction between\nalgebraic geometry and algebraic topology. In his well-known paper Kapranov has\nproved that the Deligne-Mumford compactification $\\overline{\\mathcal{M}}(0,n)$\nof $n$-pointed curves of genus zero can be realized as the Chow quotient\n$G_{n,2}\\!/\\!/(\\mathbb{C} ^{\\ast})^{n}$. In our recent papers, the constructive\ndescription of the orbit space $G_{n,2}/T^n$ has been obtained. In getting this\nresult our notions of the CW-complex of the admissible polytopes and the\nuniversal space of parameters $\\mathcal{F}_{n}$ for $T^n$-action on $G_{n,2}$\nwere of essential use. Using technique of the wonderful compactification, in\nthis paper it is given an explicit construction of the space $\\mathcal{F}_{n}$.\nTogether with Keel's description of $\\overline{\\mathcal{M}}(0,n)$, this\nconstruction enabled us to obtain an explicit diffeomorphism between\n$\\mathcal{F}_{n}$ and $\\overline{\\mathcal{M}}(0,n)$. Thus, we showed that the\nspace $G_{n,2}\\!/\\!/(\\mathbb{C} ^{\\ast})^{n}$ can be realized as our universal\nspace of parameters $\\mathcal{F}_{n}$. In this way, we give description of the\nstructure in $G_{n,2}\\!/\\!/(\\mathbb{C} ^{\\ast})^{n}$, that is\n$\\overline{\\mathcal{M}}(0,n)$ in terms of the CW-complex of the admissible\npolytopes for $G_{n,2}$ and their spaces of parameters.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:53:54 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08859","submitter":"Fagner Cunha","authors":"Fagner Cunha, Eulanda M. dos Santos, Raimundo Barreto, Juan G. Colonna","title":"Filtering Empty Camera Trap Images in Embedded Systems","comments":"Accepted to CVPR 2021 (Mobile AI workshop and challenges)","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Monitoring wildlife through camera traps produces a massive amount of images,\nwhose a significant portion does not contain animals, being later discarded.\nEmbedding deep learning models to identify animals and filter these images\ndirectly in those devices brings advantages such as savings in the storage and\ntransmission of data, usually resource-constrained in this type of equipment.\nIn this work, we present a comparative study on animal recognition models to\nanalyze the trade-off between precision and inference latency on edge devices.\nTo accomplish this objective, we investigate classifiers and object detectors\nof various input resolutions and optimize them using quantization and reducing\nthe number of model filters. The confidence threshold of each model was\nadjusted to obtain 96% recall for the nonempty class, since instances from the\nempty class are expected to be discarded. The experiments show that, when using\nthe same set of images for training, detectors achieve superior performance,\neliminating at least 10% more empty images than classifiers with comparable\nlatencies. Considering the high cost of generating labels for the detection\nproblem, when there is a massive number of images labeled for classification\n(about one million instances, ten times more than those available for\ndetection), classifiers are able to reach results comparable to detectors but\nwith half latency.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:56:22 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08860","submitter":"Huaishao Luo","authors":"Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan,\n  Tianrui Li","title":"CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip\n  Retrieval","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Video-text retrieval plays an essential role in multi-modal research and has\nbeen widely used in many real-world web applications. The CLIP (Contrastive\nLanguage-Image Pre-training), an image-language pre-training model, has\ndemonstrated the power of visual concepts learning from web collected\nimage-text datasets. In this paper, we propose a CLIP4Clip model to transfer\nthe knowledge of the CLIP model to video-language retrieval in an end-to-end\nmanner. Several questions are investigated via empirical studies: 1) Whether\nimage feature is enough for video-text retrieval? 2) How a post-pretraining on\na large-scale video-text dataset based on the CLIP affect the performance? 3)\nWhat is the practical mechanism to model temporal dependency between video\nframes? And 4) The Hyper-parameters sensitivity of the model on video-text\nretrieval task. Extensive experimental results present that the CLIP4Clip model\ntransferred from the CLIP can achieve SOTA results on various video-text\nretrieval datasets, including MSR-VTT, MSVC, LSMDC, ActivityNet, and DiDeMo. We\nrelease our code at https://github.com/ArrowLuo/CLIP4Clip.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:59:50 GMT"},{"version":"v2","created":"Sat, 8 May 2021 08:25:57 GMT"}],"update_date":"2021-05-11"}
{"id":"2104.08861","submitter":"Theo O'Neill","authors":"Theo J. O'Neill, Giuliana Cosentino, Jonathan C. Tan, Yu Cheng,\n  Mengyao Liu","title":"The Core Mass Function Across Galactic Environments. III. Massive\n  Protoclusters","comments":"Accepted for publication in ApJ, 25 pages, 10 figures, 4 tables","journal-ref":null,"doi":"10.3847/1538-4357/ac062d","report-no":null,"categories":"astro-ph.GA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The stellar initial mass function (IMF) is fundamental for many areas of\nastrophysics, but its origin remains poorly understood. It may be inherited\nfrom the core mass function (CMF) or arise as a result of more chaotic,\ncompetitive accretion. Dense, gravitationally bound cores are seen in molecular\nclouds and some observations have suggested that the CMF is similar in shape to\nthe IMF, though translated to higher masses by a factor of $\\sim3$. Here we\nmeasure the CMF in 28 dense clumps within 3.5 kpc that are likely to be central\nregions of massive protoclusters, observed via $1.3\\:{\\rm{mm}}$ dust continuum\nemission by the ALMAGAL project. We identify 222 cores using the dendrogram\nalgorithm with masses ranging from 0.04 to $252\\:M_{\\odot}$. We apply\ncompleteness corrections for flux and number recovery, estimated from core\ninsertion and recovery experiments. At higher masses, the final derived CMF is\nwell described by a single power law of the form\n$dN/d\\:{\\textrm{log}}\\:M\\propto\\:M^{-\\alpha}$ with $\\alpha\\simeq0.94\\pm0.08$.\nHowever, we find evidence of a break in this power-law behavior between $\\sim5$\nand $15\\:M_{\\odot}$, which is, to our knowledge, the first time such a break\nhas been found in distant ($\\gtrsim 1$ kpc) regions by ALMA. We compare this\nmassive protocluster CMF with those derived using the same methods in the G286\nprotocluster and a sample of Infrared Dark Clouds. The massive protocluster CMF\nis significantly different, i.e., containing more massive cores, which is a\npotential indication of the role of environment on the CMF and IMF.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 14:03:53 GMT"},{"version":"v2","created":"Wed, 26 May 2021 17:51:12 GMT"}],"update_date":"2021-08-04"}
{"id":"2104.08862","submitter":"Hengli Wang","authors":"Hengli Wang, Peide Cai, Rui Fan, Yuxiang Sun, Ming Liu","title":"End-to-End Interactive Prediction and Planning with Optical Flow\n  Distillation for Autonomous Driving","comments":"10 pages, 5 figures and 4 tables. This paper is accepted by CVPRW\n  2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.RO cs.CV","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  With the recent advancement of deep learning technology, data-driven\napproaches for autonomous car prediction and planning have achieved\nextraordinary performance. Nevertheless, most of these approaches follow a\nnon-interactive prediction and planning paradigm, hypothesizing that a\nvehicle's behaviors do not affect others. The approaches based on such a\nnon-interactive philosophy typically perform acceptably in sparse traffic\nscenarios but can easily fail in dense traffic scenarios. Therefore, we propose\nan end-to-end interactive neural motion planner (INMP) for autonomous driving\nin this paper. Given a set of past surrounding-view images and a high\ndefinition map, our INMP first generates a feature map in bird's-eye-view\nspace, which is then processed to detect other agents and perform interactive\nprediction and planning jointly. Also, we adopt an optical flow distillation\nparadigm, which can effectively improve the network performance while still\nmaintaining its real-time inference speed. Extensive experiments on the\nnuScenes dataset and in the closed-loop Carla simulation environment\ndemonstrate the effectiveness and efficiency of our INMP for the detection,\nprediction, and planning tasks. Our project page is at\nsites.google.com/view/inmp-ofd.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 14:05:18 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08863","submitter":"Xi Chen","authors":"Jiabin Xie, Jianchao He, Yun Bao, Xi Chen","title":"A low-communication-overhead parallel method for the 3D incompressible\n  Navier-Stokes equations","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.flu-dyn","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  This paper presents a low-communication-overhead parallel method for solving\nthe 3D incompressible Navier-Stokes equations. A fully-explicit projection\nmethod with second-order space-time accuracy is adopted. Combined with fast\nFourier transforms, the parallel diagonal dominant (PDD) algorithm for the\ntridiagonal system is employed to solve the pressure Poisson equation,\ndiffering from its recent applications to compact scheme derivatives\ncomputation (Abide et al. 2017) and alternating-direction-implicit method (Moon\net al. 2020). The number of all-to-all communications is decreased to only two,\nin a 2D pencil-like domain decomposition. The resulting MPI/OpenMP hybrid\nparallel code shows excellent strong scalability up to $10^4$ cores and small\nwall-clock time per timestep. Numerical simulations of turbulent channel flow\nat different friction Reynolds numbers ($Re_{\\tau}$ = 550, 1000, 2000) have\nbeen conducted and the statistics are in good agreement with the reference\ndata. The proposed method allows massively simulation of wall turbulence at\nhigh Reynolds numbers as well as many other incompressible flows.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 14:15:05 GMT"},{"version":"v2","created":"Tue, 20 Apr 2021 08:11:55 GMT"}],"update_date":"2021-04-21"}
{"id":"2104.08864","submitter":"Arup Chattopadhyay","authors":"Arup Chattopadhyay, Soma Das and Chandan Pradhan","title":"Second order trace formulae","comments":"The title has been changed, substantially modified, and a new section\n  has been added","journal-ref":null,"doi":null,"report-no":null,"categories":"math.FA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Koplienko \\cite{Ko} found a trace formula for perturbations of self-adjoint\noperators by operators of Hilbert-Schmidt class $\\mathcal{B}_2(\\mathcal{H})$.\nLater, Neidhardt introduced a similar formula in the case of pair of unitaries\n$(U,U_0)$ via multiplicative path in \\cite{NH}. In 2012, Potapov and Sukochev\n\\cite{PoSu} obtained a trace formula like the Koplienko trace formula for pairs\nof contractions by answering an open question posed by Gesztesy, Pushnitski,\nand Simon in \\cite[Open Question 11.2]{GePu}. In this article, we supply a new\nproof of the Koplienko trace formula in the case of pair of contractions\n$(T,T_0)$, where the initial operator $T_0$ is normal, via linear path by\nreducing the problem to a finite-dimensional one as in the proof of Krein's\ntrace formula by Voiculescu \\cite{Voi}, Sinha and Mohapatra\n\\cite{MoSi94,MoSi96}. Consequently, we obtain the Koplienko trace formula for a\nclass of pairs of contractions using the Sch\\\"{a}ffer matrix unitary dilation.\nMoreover, we also obtain the Koplienko trace formula for a pair of self-adjoint\noperators and maximal dissipative operators using the Cayley transform. At the\nend, we extend the Koplienko-Neidhardt trace formula for a class of pair of\ncontractions $(T,T_0)$ via multiplicative path.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 14:17:07 GMT"},{"version":"v2","created":"Thu, 10 Mar 2022 12:00:24 GMT"}],"update_date":"2022-03-11"}
{"id":"2104.08865","submitter":"Jim Apple","authors":"Jim Apple","title":"HalftimeHash: Modern Hashing without 64-bit Multipliers or Finite Fields","comments":"To be published in the proceedings of the 17th Algorithm and Data\n  Structures Symposium (WADS) 2021. Code available at\n  https://github.com/jbapple/HalftimeHash","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DS","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  HalftimeHash is a new algorithm for hashing long strings. The goals are few\ncollisions (different inputs that produce identical output hash values) and\nhigh performance.\n  Compared to the fastest universal hash functions on long strings (clhash and\nUMASH) HalftimeHash decreases collision probability while also increasing\nperformance by over 50%, exceeding 16 bytes per cycle. In addition,\nHalftimeHash does not use any widening 64-bit multiplications or any finite\nfield arithmetic that could limit its portability.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 14:19:26 GMT"},{"version":"v2","created":"Wed, 21 Apr 2021 02:47:56 GMT"}],"update_date":"2021-04-22"}
{"id":"2104.08866","submitter":"Michael R. Buche","authors":"Michael R. Buche and Meredith N. Silberstein","title":"Chain breaking in the statistical mechanical constitutive theory of\n  polymer networks","comments":"Preprint submitted to Journal of the Mechanics and Physics of Solids.\n  For associated Python implementation, see\n  https://doi.org/10.5281/zenodo.4699349","journal-ref":"J. Mech. Phys. Solids 156, 104593 (2021)","doi":"10.1016/j.jmps.2021.104593","report-no":null,"categories":"cond-mat.soft cond-mat.stat-mech","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Elastomers are used in a wide range of applications because of their large\nstrain to failure, low density, and tailorable stiffness and toughness. The\nmechanical behavior of elastomers derives mainly from the entropic elasticity\nof the underlying network of polymer chains. Elastomers under large deformation\nexperience bonds breaking within the polymer network. This breaking of chains\ndamages the network, can lead to material failure, and can be utilized as an\nenergy dissipation mechanism. In the case of reversible bonds, broken chains\nmay reform and heal the damage in the network. If the reversible bonds are\ndynamic, chains constantly break and reform and create a transient network. A\nfundamental constitutive theory is developed to model the mechanics of these\npolymer networks. A statistical mechanical derivation is conducted to yield a\nframework that takes in an arbitrary single-chain model (a Hamiltonian) and\noutputs: the single-chain mechanical response, the breaking and reforming\nkinetics, the equilibrium distribution of chains in the network, and the\npartial differential equations governing the deformation-coupled network\nevolution. This statistical mechanical framework is then brought into the\ncontinuum scale by using macroscopic thermodynamic constitutive theory to\nobtain a constitutive relation for the Cauchy stress. The\npotential-supplemented freely jointed chain ($u$FJC) model is introduced, and a\nparametric study of its mechanical response and breaking kinetics is provided.\nThis single-chain model is then implemented within the constitutive framework,\nwhich we specialize and apply in two exemplary cases: the mechanical response\nand irreversible breakdown of a multinetwork elastomer, and the mechanical\nresponse of a dual crosslink gel. After providing a parametric study of the\ngeneral constitutive model, we apply it to a hydrogel with reversible\nmetal-coordination crosslinks.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 14:29:00 GMT"}],"update_date":"2022-08-24"}
{"id":"2104.08867","submitter":"Amin Mahmoudi","authors":"Amin Mahmoudi, Victoria Wai-lan Yeung, Eric W. K. See-To","title":"User Behavior Discovery in the COVID-19 Era through the Sentiment\n  Analysis of User Tweet Texts","comments":"24 pages, 7 figuers","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SI","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The coronavirus disease (COVID-19) outbreak was declared a pandemic in March\n2020 and since then it has had a significant effect on all aspects of life.\nAlthough we live in an information era, we do not have accurate information\nabout this disease. Online social networks (OSNs) play a vital role in society,\nespecially people who do not have trust in the government would tend to have\nmore confidence in the evidence that is formed by social networks. The\nadvantages of OSNs in the COVID-19 era are clear. For instance, social media\nenables people to connect with each other without the need for real-world\nface-to-face social interaction. Social media networks also act as a collective\nintelligence in the absence of world leadership. Therefore, in this study,\nconsidering the phenomenon of information diffusion in OSNs, we focus on the\neffects of COVID-19 on user sentiment and show the user behavior trend during\nthe early months of the pandemic through mining and analyzing OSN data.\nMoreover, we propose a data-driven model to demonstrate how user sentiment\nchanges over a period of time and how OSNs help us to obtain information on\nuser behavior that is very important for the accurate prediction of future\nbehavior. For this purpose, this study uses tweet texts about COVID-19 and the\nrelated network structure to extract significant features, and then presents a\nmodel attempting to provide a more comprehensive real picture of current and\nfuture conditions.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 14:32:47 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08868","submitter":"Senlin Wu","authors":"Senlin Wu, Keke Zhang, Chan He","title":"Homothetic covering of convex hulls of compact convex sets","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.MG math.CO","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  Let $K$ be a compact convex set and $m$ be a positive integer. The covering\nfunctional of $K$ with respect to $m$ is the smallest $\\lambda\\in[0,1]$ such\nthat $K$ can be covered by $m$ translates of $\\lambda K$. Estimations of the\ncovering functionals of convex hulls of two or more compact convex sets are\npresented. It is proved that, if a three-dimensional convex body $K$ is the\nconvex hull of two compact convex sets having no interior points, then the\nleast number $c(K)$ of smaller homothetic copies of $K$ needed to cover $K$ is\nnot greater than $8$ and $c(K)=8$ if and only if $K$ is a parallelepiped.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 14:40:47 GMT"},{"version":"v2","created":"Thu, 22 Apr 2021 08:40:20 GMT"},{"version":"v3","created":"Sun, 14 Nov 2021 15:08:24 GMT"}],"update_date":"2021-11-16"}
{"id":"2104.08869","submitter":"Clemens Damke","authors":"Clemens Damke and Eyke H\\\"ullermeier","title":"Ranking Structured Objects with Graph Neural Networks","comments":null,"journal-ref":"24th International Conference on Discovery Science (2021) 166-180","doi":"10.1007/978-3-030-88942-5","report-no":null,"categories":"cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Graph neural networks (GNNs) have been successfully applied in many\nstructured data domains, with applications ranging from molecular property\nprediction to the analysis of social networks. Motivated by the broad\napplicability of GNNs, we propose the family of so-called RankGNNs, a\ncombination of neural Learning to Rank (LtR) methods and GNNs. RankGNNs are\ntrained with a set of pair-wise preferences between graphs, suggesting that one\nof them is preferred over the other. One practical application of this problem\nis drug screening, where an expert wants to find the most promising molecules\nin a large collection of drug candidates. We empirically demonstrate that our\nproposed pair-wise RankGNN approach either significantly outperforms or at\nleast matches the ranking performance of the naive point-wise baseline\napproach, in which the LtR problem is solved via GNN-based graph regression.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 14:40:59 GMT"},{"version":"v2","created":"Mon, 11 Oct 2021 10:41:50 GMT"}],"update_date":"2021-10-12"}
{"id":"2104.08870","submitter":"Francis Watson","authors":"F. M. Watson and M. G. Crabb and W. R. B. Lionheart","title":"A polarization tensor approximation for the Hessian in iterative solvers\n  for non-linear inverse problems","comments":"28 pages, 11 figures. This revision includes minor improvements to\n  some notation and descriptions in the text, as well as fixing one missing\n  crossref. Some erroneous output due to the Bibtex style file has been\n  removed. There are no changes to any technical content","journal-ref":null,"doi":null,"report-no":null,"categories":"math.NA cs.NA","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  For many inverse parameter problems for partial differential equations in\nwhich the domain contains only well-separated objects, an asymptotic solution\nto the forward problem involving 'polarization tensors' exists. These are\nfunctions of the size and material contrast of inclusions, thereby describing\nthe saturation component of the non-linearity. As such, these asymptotic\nexpansions can allow fast and stable reconstruction of small isolated objects.\nIn this paper, we show how such an asymptotic series can be applied to\nnon-linear least-squares reconstruction problems, by deriving an approximate\ndiagonal Hessian matrix for the data misfit term.\n  Often, the Hessian matrix can play a vital role in dealing with the\nnon-linearity, generating good update directions which accelerate the solution\ntowards a global minimum which may lie in a long curved valley, but\ncomputational cost can make direct calculation infeasible. Since the\npolarization tensor approximation assumes sufficient separation between\ninclusions, our approximate Hessian does not account for non-linearity in the\nform of lack of superposition in the inverse problem. It does however account\nfor the non-linear saturation of the change in the data with increasing\nmaterial contrast. We therefore propose to use it as an initial Hessian for\nquasi-Newton schemes.\n  This is demonstrated for the case of electrical impedance tomography in\nnumerical experimentation, but could be applied to any other problem which has\nan equivalent asymptotic expansion. We present numerical experimentation into\nthe accuracy and reconstruction performance of the approximate Hessian,\nproviding a proof of principle of the reconstruction scheme.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 14:42:13 GMT"},{"version":"v2","created":"Tue, 4 May 2021 19:56:32 GMT"}],"update_date":"2021-05-06"}
{"id":"2104.08871","submitter":"Serkan S\\\"utl\\\"u","authors":"B. Ate\\c{s}li, O. Esen, S. S\\\"utl\\\"u","title":"Cohomologies and generalized derivation extensions of $n$-Lie algebras","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"math.RA","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  A cohomology theory, associated to a $n$-Lie algebra and a representation\nspace of it, is introduced. It is observed that this cohomology theory is\nqualified to encode the generalized derivation extensions, and that it\ncoincides, for $n=3$, with the known cohomology of $n$-Lie algebras. The\nabelian extensions and infinitesimal deformations of $n$-Lie algebras, on the\nother hand, are shown to be characterized by the usual cohomology of $n$-Lie\nalgebras. Furthermore, the Hochschild-Serre spectral sequence of the Lie\nalgebra cohomology is upgraded to the level of $n$-Lie algebras, and is applied\nto the cohomology of generalized derivation extensions.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 14:51:03 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08872","submitter":"Masahiro Morikawa","authors":"Masahiro Morikawa","title":"Low-Frequency Characterization of Music Sounds -- Ultra-Bass Richness\n  from the Sound Wave Beats","comments":"23 pages, 7 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.SD eess.AS physics.gen-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The orchestra performance is full of sublime rich sounds. In particular, the\nunison of violins sounds different from the solo violin. We try to clarify this\ndifference and similarity of unison and solo numerically analyzing the beat of\n`violins` with timbre, vibrato, melody, and resonance. Characteristic\nproperties appear in the very low-frequency part in the power spectrum of the\nwave amplitude squared. This ultra-buss richness (UBR) can be a new\ncharacteristic of sound on top of the well-known pitch, loudness, and timbre,\nalthough being inaudible directly. We find this UBR is always characterized by\na power-law at low-frequency with the index around -1 and appears everywhere in\nmusic and thus being universal. Furthermore, we explore this power-law property\ntowards much smaller frequency regions and suggest possible relation to the 1/f\nnoise often found in music and many other fields in nature.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 14:54:06 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08873","submitter":"Fan Yang","authors":"Cheng Chen, Fan Yang, Xiaoling Wu, Chuyang Shen, Meng Khoon Tey, and\n  Li You","title":"Two-Color Optical Nonlinearity in an Ultracold Rydberg Atom Gas Mixture","comments":"6 pages, 4 figures; accepted by Phys. Rev. A (link:\n  https://journals.aps.org/pra/accepted/1507cY42O3711c7412e951d48870d2d3554b4bc8e)","journal-ref":null,"doi":"10.1103/PhysRevA.103.053303","report-no":null,"categories":"physics.atom-ph cond-mat.quant-gas physics.optics quant-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We report the experimental observation of strong two-color optical\nnonlinearity in an ultracold gas of $^{85}\\mathrm{Rb}$-$^{87}\\mathrm{Rb}$ atom\nmixture. By simultaneously coupling two probe transitions of $^{85}$Rb and\n$^{87}$Rb atoms to Rydberg states in electromagnetically induced transparency\n(EIT) configurations, we observe significant suppression of the transparency\nresonance for one probe field when the second probe field is detuned at\n$\\sim1~\\mathrm{GHz}$ and hitting the EIT resonance of the other isotope. Such a\ncross-absorption modulation to the beam propagation dynamics can be described\nby two coupled nonlinear wave equations we develope. We further demonstrate\nthat the two-color optical nonlinearity can be tuned by varying the density\nratio of different atomic isotopes, which highlights its potential for\nexploring strongly interacting multi-component fluids of light.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 15:01:18 GMT"}],"update_date":"2021-05-19"}
{"id":"2104.08874","submitter":"Federico Bianchi","authors":"Federico Bianchi and Ciro Greco and Jacopo Tagliabue","title":"Language in a (Search) Box: Grounding Language Learning in Real-World\n  Human-Machine Interaction","comments":"Published as a conference paper at NAACL2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We investigate grounded language learning through real-world data, by\nmodelling a teacher-learner dynamics through the natural interactions occurring\nbetween users and search engines; in particular, we explore the emergence of\nsemantic generalization from unsupervised dense representations outside of\nsynthetic environments. A grounding domain, a denotation function and a\ncomposition function are learned from user data only. We show how the resulting\nsemantics for noun phrases exhibits compositional properties while being fully\nlearnable without any explicit labelling. We benchmark our grounded semantics\non compositionality and zero-shot inference tasks, and we show that it provides\nbetter results and better generalizations than SOTA non-grounded models, such\nas word2vec and BERT.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 15:03:16 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08875","submitter":"Eduardo Guendelman I","authors":"Eduardo Guendelman","title":"Cosmology and Warped Space Times in Dynamical String Tension Theories","comments":"18 pages, few typos corrected, a references added, discussions\n  improved","journal-ref":null,"doi":null,"report-no":null,"categories":"hep-th gr-qc","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  The string and brane tensions do not have to be put in by hand, they can be\ndynamically generated, as in the case when we formulate string and brane\ntheories in the modified measure formalism. Then string and brane tensions\nappears, but as an additional dynamical degree of freedom . It can be seen\nhowever that these string or brane tensions are not universal, but rather each\nstring and each brane generates its own tension, which can have a different\nvalue for each string or brane. We define a new background fields that can\ncouple to this new type of extended objects, the tension scalar is capable of\nchanging locally along the world sheet the value of the tension of the extended\nobject. When many strings probing the same region of space are considered this\ntension scalar is constrained by the requirement of quantum conformal\ninvariance. For the case of two strings probing the same region of space with\ndifferent dynamically generated tensions, there are two different metrics,\nassociated to the different strings, that have to satisfy vacuum Einstein\nequations and the consistency of these two Einstein equation determines the\ntension scalar. The universal metric, common to both strings generically does\nnot satisfy Einstein equation . The problem is analyzed for Schwarzschild\nbackground and for Kasner type space, for the Milne representation, for the\ncase of two types of string tensions, with solutions with negative string\ntension at the early universe and a positive string tension that appears for\nthe late universe . The universal metric is not flat, instead it represents a\nnon singular bounce cosmology. The case in a warped space time where positive\nand and negative string tensions are separated by a spontaneously generated\nwall is also studied.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 15:04:04 GMT"},{"version":"v2","created":"Tue, 20 Apr 2021 15:53:24 GMT"},{"version":"v3","created":"Sun, 6 Jun 2021 13:33:38 GMT"}],"update_date":"2021-06-08"}
{"id":"2104.08876","submitter":"Songan Zhang","authors":"Songan Zhang, Lu Wen, Huei Peng, H. Eric Tseng","title":"Quick Learner Automated Vehicle Adapting its Roadmanship to Varying\n  Traffic Cultures with Meta Reinforcement Learning","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.SY eess.SY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  It is essential for an automated vehicle in the field to perform\ndiscretionary lane changes with appropriate roadmanship - driving safely and\nefficiently without annoying or endangering other road users - under a wide\nrange of traffic cultures and driving conditions. While deep reinforcement\nlearning methods have excelled in recent years and been applied to automated\nvehicle driving policy, there are concerns about their capability to quickly\nadapt to unseen traffic with new environment dynamics. We formulate this\nchallenge as a multi-Markov Decision Processes (MDPs) adaptation problem and\ndeveloped Meta Reinforcement Learning (MRL) driving policies to showcase their\nquick learning capability. Two types of distribution variation in environments\nwere designed and simulated to validate the fast adaptation capability of\nresulting MRL driving policies which significantly outperform a baseline RL.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 15:04:37 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08877","submitter":"Ilya Sheikin","authors":"S. Mishra, D. Gorbunov, D.J. Campbell, D. LeBoeuf, J. Hornung, J.\n  Klotz, S. Zherlitsyn, H. Harima, J. Wosnitza, D. Aoki, A. McCollam, and I.\n  Sheikin","title":"Origin of the 30 T transition in CeRhIn$_5$ in tilted magnetic fields","comments":"9 pages, 10 figures. To be published in Phys. Bev. B","journal-ref":"Phys. Rev. B 103, 165124 (2021)","doi":"10.1103/PhysRevB.103.165124","report-no":null,"categories":"cond-mat.str-el","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We present a comprehensive ultrasound study of the prototypical heavy-fermion\nmaterial CeRhIn$_5$, examining the origin of the enigmatic 30 T transition. For\na field applied at 2$^\\circ$ from the $c$ axis, we observed two sharp anomalies\nin the sound velocity, at $B_m \\approx$ 20 T and $B^* \\approx$ 30 T, in all the\nsymmetry-breaking ultrasound modes at low temperatures. The lower-field anomaly\ncorresponds to the well-known first-order metamagnetic\nincommensurate-to-commensurate transition. The higher-field anomaly takes place\nat 30 T, where an electronic-nematic transition was previously suggested to\noccur. Both anomalies, observed only within the antiferromagnetic state, are of\nsimilar shape, but the corresponding changes of the ultrasound velocity have\nopposite signs. Based on our experimental results, we suggest that a\nfield-induced magnetic transition from a commensurate to another incommensurate\nantiferromagnetic state occurs at $B^*$. With further increasing the field\nangle from the $c$ axis, the anomaly at $B^*$ slowly shifts to higher fields,\nbroadens, and becomes smaller in magnitude. Traced up to 30$^\\circ$ from the\n$c$ axis, it is no longer observed at 40$^\\circ$ below 36 T.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 15:11:18 GMT"}],"update_date":"2021-04-21"}
{"id":"2104.08878","submitter":"Samuel Bell","authors":"Samuel J. Bell and Onno P. Kampman","title":"Perspectives on Machine Learning from Psychology's Reproducibility\n  Crisis","comments":"Added acknowledgements; assorted minor edits","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG cs.AI","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In the early 2010s, a crisis of reproducibility rocked the field of\npsychology. Following a period of reflection, the field has responded with\nradical reform of its scientific practices. More recently, similar questions\nabout the reproducibility of machine learning research have also come to the\nfore. In this short paper, we present select ideas from psychology's\nreformation, translating them into relevance for a machine learning audience.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 15:17:35 GMT"},{"version":"v2","created":"Fri, 23 Apr 2021 11:44:47 GMT"}],"update_date":"2021-04-26"}
{"id":"2104.08879","submitter":"Qiang Chen","authors":"Qiang Chen and Peifeng Fan and Jianyuan Xiao","title":"Unified perspective on single cyclotron electron with radiation-reaction\n  from classical to quantum","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"physics.gen-ph","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We show a unified physical picture of single cyclotron electron with\nradiation-reaction, which bridges the classical electron models and quantum\nmechanical self-consistent field theory. On a classical level, we suggest an\nimproved electrodynamical action, which build the classical electron models\ninto a first-principle framework. The link between dynamical defections and\nnon-physical action configurations emerges naturally. On a quantum level, a\nself-consistent description for electron gyro-motion with self-force is\nconstructed in the Schr\\\"odinger-Maxwell theory. We derive a class of\nasymptotic equations. The leading and next-to-leading orders give a good\nanalogue of a classical cyclotron electron, and the limit field theory avoids\nclassical electron induced defections gracefully. Beyond the Hamiltonian\nperturbation theory, we use state-of-the-art geometric simulator to observe\nsingle electron gyro-motions at quantum region. The non-linear and\nnon-perturbative features captured by simulations provide a complete physical\npicture in a very wide range. We show an optimal complementary relation between\nclassical and quantum cyclotron electrons, and find a strange and inexplicable\nelectron chimera state existing at strong non-linear regions, which may be\nobserved in astrophysical environments and strong magnetic experiments.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 15:20:04 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08880","submitter":"Ankur Majumdar","authors":"Ankur Majumdar, Sotirios Dimitrakopoulos and Omid Alizadeh-Mousavi","title":"Grid Monitoring for Efficient Flexibility Provision in Distribution\n  Grids","comments":"4 pages, 5 figures, CIRED workshop 2020","journal-ref":null,"doi":null,"report-no":null,"categories":"eess.SY cs.SY","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","abstract":"  The increased flexibility requirement needs a flexibility market at the\ndistribution grid level operated by the distribution system operators (DSOs) to\nresolve challenges to ensure secure operation and, to integrate new renewable\nproductions or loads in the grid. Therefore, the network visibility and\nmonitoring are paramount to distribution network operation. This paper presents\na methodology demonstrating the value distribution grid monitoring can bring\nfor the realisation of a local flexibility market. This paper further\nillustrates the reduction of costs of operation of a DSO with the help of grid\nmonitoring and local flexibility market while maintaining a secure and reliable\noperation. The methodology has been applied on a real 35 node MV network of a\nSwiss DSO with several GridEye measurement devices. The performance in terms of\nlosses and voltage and flow violation costs is compared with sub-optimal\noperation without monitoring.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 15:24:18 GMT"}],"update_date":"2021-04-20"}
{"id":"2104.08881","submitter":"Alejandro Perez","authors":"Lautaro Amadei and Alejandro Perez","title":"Planckian discreteness as seeds for cosmic structure","comments":"Peer reviewed version","journal-ref":null,"doi":"10.1103/PhysRevD.106.063528","report-no":null,"categories":"gr-qc hep-ph hep-th","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  We propose a model of inflation driven by the relaxation of an initially\nPlanckian cosmological constant due to diffusion. The model can generate a\n(approximately) scale invariant spectrum of (adiabatic) primordial\nperturbations with the correct amplitudes and red tilt without an inflaton. The\ninhomogeneities observable in the CMB arise from those associated to the\nfundamental Planckian granularity that are imprinted into the standard model\nHiggs scalar fluctuations during the inflationary phase. The process admits a\nsemiclassical interpretation and avoids the trans-Planckian problem of standard\ninflationary scenarios based on the role of vacuum fluctuations. The deviations\nfrom scale invariance observed in the CMB are controlled by the self coupling\nconstant of the Higgs scalar of the standard model of particle physics. The\nthermal production of primordial black holes can produce the amount of cold\ndark matter required by observations. For natural initial conditions set at the\nPlanck scale the amplitude and tilt of the power spectrum of perturbations\nobserved at the CMB depend only on known parameters of the standard model such\nas the self coupling of the Higgs scalar and its mass.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 15:25:13 GMT"},{"version":"v2","created":"Tue, 4 May 2021 10:58:39 GMT"},{"version":"v3","created":"Thu, 9 Jun 2022 09:27:58 GMT"},{"version":"v4","created":"Fri, 9 Sep 2022 08:17:44 GMT"}],"update_date":"2022-10-05"}
{"id":"2104.08882","submitter":"Francesco La Via","authors":"Cristiano Calabretta, Massimo Zimbone, Eric G. Barbagiovanni, Simona\n  Boninelli, Nico Piluso, Andrea Severino, Maria A. Di Stefano, Simona Lorenti,\n  Lucia Calcagno and Francesco La Via","title":"Thermal Annealing of High Dose P Implantation in 4H-SiC","comments":null,"journal-ref":"Materials Science Forum ISSN: 1662-9752, Vol. 963, pp 399-402\n  (2019)","doi":"10.4028/www.scientific.net","report-no":null,"categories":"cond-mat.mtrl-sci","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this work, we have studied the crystal defectiveness and doping activation\nsubsequent to ion implantation and post-annealing by using various techniques\nincluding photoluminescence (PL), Raman spectroscopy and transmission electron\nmicroscopy (TEM). The aim of this work was to test the effectiveness of double\nstep annealing to reduce the density of point defects generated during the\nannealing of a P implanted 4H-SiC epitaxial layer. The outcome of this work\nevidences that neither the first, 1 hour isochronal annealing at 1650 - 1700 -\n1750 {\\deg}C, nor the second one, at 1500 {\\deg}C for times between 4 hour and\n14 hour, were able to recover a satisfactory crystallinity of the sample and\nachieve dopant activations exceeding 1%.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 15:37:11 GMT"},{"version":"v2","created":"Wed, 28 Apr 2021 06:53:09 GMT"}],"update_date":"2021-04-29"}
{"id":"2104.08883","submitter":"Pierre Fleury","authors":"Pierre Fleury, Julien Larena, Jean-Philippe Uzan","title":"Line-of-sight effects in strong gravitational lensing","comments":"39+14 pages, 15 figures. v2: discussion improved in sec. 3.2; figs. 4\n  and 15 updated. v3: minor typos corrected, matches published version; v4, v5:\n  other typos corrected","journal-ref":"JCAP 08 (2021) 024","doi":"10.1088/1475-7516/2021/08/024","report-no":"IFT-UAM/CSIC-21-39","categories":"astro-ph.CO gr-qc","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  While most strong-gravitational-lensing systems may be roughly modelled by a\nsingle massive object between the source and the observer, in the details all\nthe structures near the light path contribute to the observed images. These\nadditional contributions, known as line-of-sight effects, are non-negligible in\npractice. This article proposes a new theoretical framework to model the\nline-of-sight effects, together with very promising applications at the\ninterface of weak and strong lensing. Our approach relies on the dominant-lens\napproximation, where one deflector is treated as the main lens while the others\nare treated as perturbations. The resulting framework is technically simpler to\nhandle than the multi-plane lensing formalism, while allowing one to\nconsistently model any sub-critical perturbation. In particular, it is not\nlimited to the usual external-convergence and external-shear parameterisation.\nAs a first application, we identify a specific notion of line-of-sight shear\nthat is not degenerate with the ellipticity of the main lens, and which could\nthus be extracted from strong-lensing images. This result supports and improves\nthe recent proposal that Einstein rings might be powerful probes of cosmic\nshear. As a second application, we investigate the distortions of\nstrong-lensing critical curves under line-of-sight effects, and more\nparticularly their correlations across the sky. We find that such correlations\nmay be used to probe, not only the large-scale structure of the Universe, but\nalso the dark-matter halo profiles of strong lenses. This last possibility\nwould be a key asset to improve the accuracy of the measurement of the\nHubble-Lema\\^itre constant via time-delay cosmography.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 15:38:01 GMT"},{"version":"v2","created":"Wed, 30 Jun 2021 10:27:45 GMT"},{"version":"v3","created":"Thu, 12 Aug 2021 12:19:51 GMT"},{"version":"v4","created":"Mon, 30 May 2022 15:14:40 GMT"},{"version":"v5","created":"Wed, 30 Nov 2022 10:47:11 GMT"}],"update_date":"2022-12-01"}
{"id":"2104.08884","submitter":"Mao Jing","authors":"Jing Mao, Qiang Tu","title":"A class of inverse curvature flows for star-shaped hypersurfaces\n  evolving in a cone","comments":"17 pages. This paper was finished in January 2019 when the first\n  author visited IST, University of Lisbon. We just put it on arXiv very\n  recently (the status of two references has been updated). Several related\n  works will also be put on arXiv soon. Comments are welcome. Two typos have\n  been corrected in Theorem 1.1","journal-ref":null,"doi":null,"report-no":null,"categories":"math.DG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Given a smooth convex cone in the Euclidean $(n+1)$-space ($n\\geq2$), we\nconsider strictly mean convex hypersurfaces with boundary which are star-shaped\nwith respect to the center of the cone and which meet the cone perpendicularly.\nIf those hypersurfaces inside the cone evolve by a class of inverse curvature\nflows, then, by using the convexity of the cone in the derivation of the\ngradient and H\\\"{o}lder estimates, we can prove that this evolution exists for\nall the time and the evolving hypersurfaces converge smoothly to a piece of a\nround sphere as time tends to infinity.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 15:57:04 GMT"},{"version":"v2","created":"Tue, 20 Apr 2021 05:00:54 GMT"}],"update_date":"2021-04-21"}
{"id":"2104.09499","submitter":"Yifeng Che","authors":"Yifeng Che, Joseph Yurko, Koroush Shirvan","title":"Machine learning-assisted surrogate construction for full-core fuel\n  performance analysis","comments":"31 pages, 16 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CE cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Accurately predicting the behavior of a nuclear reactor requires multiphysics\nsimulation of coupled neutronics, thermal-hydraulics and fuel thermo-mechanics.\nThe fuel thermo-mechanical response provides essential information for\noperational limits and safety analysis. Traditionally, fuel performance\nanalysis is performed standalone, using calculated spatial-temporal power\ndistribution and thermal boundary conditions from the coupled\nneutronics-thermal-hydraulics simulation as input. Such one-way coupling is\nresult of the high cost induced by the full-core fuel performance analysis,\nwhich provides more realistic and accurate prediction of the core-wide response\nthan the \"peak rod\" analysis. It is therefore desirable to improve the\ncomputational efficiency of full-core fuel performance modeling by constructing\nfast-running surrogate, such that fuel performance modeling can be utilized in\nthe core reload design optimization. This work presents methodologies for\nfull-core surrogate construction based on several realistic equilibrium PWR\ncore designs. As a fast and conventional approach, look-up tables are only\neffective for certain fuel performance quantities of interest (QoIs). Several\nrepresentative machine-learning algorithms are introduced to capture the\ncomplicated physics for other fuel performance QoIs. Rule-based model is useful\nas a feature extraction technique to account for the spatial-temporal\ncomplexity of operating conditions. Constructed surrogates achieve at least ten\nthousand time acceleration with satisfying prediction accuracy. Current work\nlays foundation for tighter coupling of fuel performance modeling into the core\ndesign optimization framework. It also sets stage for full-core fuel\nperformance analysis with BISON where the computational cost becomes more\nburdensome.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 17:02:50 GMT"}],"update_date":"2021-04-21"}
{"id":"2104.09500","submitter":"Arthur Bra\\v{z}inskas","authors":"Arthur Bra\\v{z}inskas, Mengwen Liu, Ramesh Nallapati, Sujith Ravi,\n  Markus Dreyer","title":"Transductive Learning for Abstractive News Summarization","comments":null,"journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CL cs.AI cs.LG","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  Pre-trained and fine-tuned news summarizers are expected to generalize to\nnews articles unseen in the fine-tuning (training) phase. However, these\narticles often contain specifics, such as new events and people, a summarizer\ncould not learn about in training. This applies to scenarios such as a news\npublisher training a summarizer on dated news and summarizing incoming recent\nnews. In this work, we explore the first application of transductive learning\nto summarization where we further fine-tune models on test set inputs.\nSpecifically, we construct pseudo summaries from salient article sentences and\ninput randomly masked articles. Moreover, this approach is also beneficial in\nthe fine-tuning phase, where we jointly predict extractive pseudo references\nand abstractive gold summaries in the training set. We show that our approach\nyields state-of-the-art results on CNN/DM and NYT datasets, improving ROUGE-L\nby 1.05 and 0.74, respectively. Importantly, our approach does not require any\nchanges of the original architecture. Moreover, we show the benefits of\ntransduction from dated to more recent CNN news. Finally, through human and\nautomatic evaluation, we demonstrate improvements in summary abstractiveness\nand coherence.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 17:33:12 GMT"},{"version":"v2","created":"Sat, 16 Apr 2022 20:23:12 GMT"}],"update_date":"2022-04-19"}
{"id":"2104.09501","submitter":"Aditya Varna Iyer Mr.","authors":"Aditya Iyer, Eduardo O. Dias, and Vlatko Vedral","title":"Signatures of causality and determinism in a quantum theory of events","comments":null,"journal-ref":"Phys. Rev. A 105, L010202, 14 January 2022","doi":"10.1103/PhysRevA.105.L010202","report-no":null,"categories":"quant-ph gr-qc","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  By representing an event as the joint state of a detector-timer couple that\ninteract with a system, we recover the familiar tensor product structure, used\nto describe spatially separated systems, in the context of timelike events.\nFurthermore, with this approach, we extend the superposition principle to the\nmoment of occurrence of events. We then outline quantum signatures of causality\nthat manifest through coherence in the detector state and correlation functions\nof time operators. Finally, we expand the scope of quantum information\ntheoretic measures of state discrimination and information content, commonly\nused to characterize spatially separated systems, to events in spacetime. For\ncausally connected events, we illustrate a deterministic relationship between\nevents (akin to spatially entangled physical systems) where observing a\nprevious event (one subsystem), enables us to delineate a later event (the\nother subsystem).\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 22:18:03 GMT"},{"version":"v2","created":"Tue, 1 Mar 2022 22:20:39 GMT"}],"update_date":"2022-03-03"}
{"id":"2104.09502","submitter":"A. Yavuz Oruc","authors":"A. Yavuz Oruc, A. Atmaca, Y. Nevzat Sengun, A. Semi Yenimol","title":"CodeAPeel: An Integrated and Layered Learning Technology For Computer\n  Architecture Courses","comments":"Minor revision and some typos are fixed","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.AR cs.DC","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper introduces a versatile, multi-layered technology to help support\nteaching and learning core computer architecture concepts. This technology,\ncalled CodeAPeel is already implemented in one particular form to describe\ninstruction processing in compiler, assembly, and machine layers of a generic\ninstruction set architecture by a comprehensive simulation of its\nfetch-decode-execute cycle as well as animation of the behavior of its CPU\nregisters, RAM, VRAM, STACK memories, various control registers, and graphics\nscreen. Unlike most educational CPU simulators that simulate a real processor\nsuch as MIPS or RISC-V, CodeAPeel is designed and implemented as a generic RISC\ninstruction set architecture simulator with both scalar and vector instructions\nto provide a dual-mode processor simulator as described by Flynn's\nclassification of SISD and SIMD processors. Vectorization of operations is\nbuilt into the instruction repertoire of CodeAPeel, making it straightforward\nto simulate such processors with powerful vector instructions.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 00:04:49 GMT"},{"version":"v2","created":"Wed, 21 Apr 2021 12:15:43 GMT"},{"version":"v3","created":"Thu, 24 Jun 2021 19:56:22 GMT"},{"version":"v4","created":"Mon, 29 Nov 2021 17:22:15 GMT"}],"update_date":"2021-11-30"}
{"id":"2104.09984","submitter":"Mikhail Shubov","authors":"Mikhail Shubov","title":"Ramjet Acceleration of Microscopic Black Holes Within Stellar Material","comments":null,"journal-ref":"Astrophysics and Space Science, 364(12), 2019","doi":"10.1007/s10509-019-3500-9","report-no":null,"categories":"astro-ph.HE","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  In this work we present a case that Microscopic Black Holes (MBH) of mass\n$10^{16}\\ kg$ to $3 \\cdot 10^{19}\\ kg$ experience acceleration as they move\nwithin stellar material at low velocities. The accelerating forces are caused\nby the fact that a MBH moving through stellar material leaves a trail of hot\nrarified gas. The rarified gas behind a MBH exerts lower gravitational force on\nthe MBH than the dense gas in front of it. The accelerating forces exceed the\ngravitational drag forces when MBH moves at Mach number\n$\\mathcal{M}<\\mathcal{M}_0$. The equilibrium Mach number $\\mathcal{M}_0$\ndepends on MBH mass and stellar material characteristics. Our calculations open\nthe possibility of MBH orbiting within stars including Sun at Mach number\n$\\mathcal{M}_0$. At the end of this work we list some unresolved problems which\nresult from our calculations.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 03:34:20 GMT"}],"update_date":"2021-04-28"}
{"id":"2104.10022","submitter":"Bilal Farooq","authors":"Sayed Mehdi Meshkani and Bilal Farooq","title":"A Decentralized Shared CAV System Design and Application","comments":"Presented at TRISTAN X, 17-21 June 2019, Hamilton Island, Australia","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.DC cs.SY eess.SY","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  In this study, we propose a novel heuristic two-step algorithm for shared\nridehailing in which users can share their rides with only one more user. The\nalgorithm, which is centrally formulated, starts with matching users and\ncreating a set of passenger pairs in step 1 and is followed by solving an\nassignment problem to assign passenger pairs to the vehicles. To solve the\nproblem of high computational time in dynamic ride-matching problems, we\npropose a distributed system that is based on vehicle to infrastructure (V2I)\nand infrastructure to infrastructure (I2I) communication. To evaluate the\ndistributed system's performance, we compare it with the proposed centralized\nridehailing algorithm. Both centralized and distributed systems are implemented\nin a micro-traffic simulator to assess their performance and their impact on\ntraffic congestion. Downtown Toronto road network was chosen as the study area.\nBased on our obtained results, the service rate of the distributed system was\n91.59% which is close to 95.80% in the centralized system. However, the\ndistributed system yielded much lower computational time compared to\ncentralized. Furthermore, the scalability of the distributed system was shown\nby testing it on a small network and comparing with the entire network.\n","versions":[{"version":"v1","created":"Sat, 17 Apr 2021 18:40:45 GMT"}],"update_date":"2021-04-21"}
{"id":"2104.10482","submitter":"Alexandre Duval","authors":"Alexandre Duval and Fragkiskos D. Malliaros","title":"GraphSVX: Shapley Value Explanations for Graph Neural Networks","comments":"ECML PKDD 2021","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.LG","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Graph Neural Networks (GNNs) achieve significant performance for various\nlearning tasks on geometric data due to the incorporation of graph structure\ninto the learning of node representations, which renders their comprehension\nchallenging. In this paper, we first propose a unified framework satisfied by\nmost existing GNN explainers. Then, we introduce GraphSVX, a post hoc local\nmodel-agnostic explanation method specifically designed for GNNs. GraphSVX is a\ndecomposition technique that captures the \"fair\" contribution of each feature\nand node towards the explained prediction by constructing a surrogate model on\na perturbed dataset. It extends to graphs and ultimately provides as\nexplanation the Shapley Values from game theory. Experiments on real-world and\nsynthetic datasets demonstrate that GraphSVX achieves state-of-the-art\nperformance compared to baseline models while presenting core theoretical and\nhuman-centric properties.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:40:37 GMT"},{"version":"v2","created":"Tue, 13 Jul 2021 07:33:30 GMT"}],"update_date":"2021-07-14"}
{"id":"2104.12551","submitter":"Tao Zhang","authors":"Tao Zhang","title":"Zinbiel 2-algebras","comments":"14pages, an improvement of arxiv:2104.12551v1 with some words and\n  formulas corrected. This is also a primary edition, something should be\n  modified in the future","journal-ref":null,"doi":null,"report-no":null,"categories":"math.RA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The notions of Zinbiel 2-algebras and 2-term $Z_{\\infty}$-algebras are\nintroduced. It is proved that the category of Zinbiel 2-algebras and the\ncategory of $2$-term $Z_{\\infty}$-algebras are equivalent to each other.\nCrossed module extensions of Zinbiel algebras are studied. Relationship between\nZinbiel 2-algebras and dendriform 2-algebras are found.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:02:42 GMT"},{"version":"v2","created":"Sat, 12 Jun 2021 08:42:59 GMT"}],"update_date":"2021-06-15"}
{"id":"2104.12861","submitter":"Hengli Wang","authors":"Hengli Wang, Peide Cai, Yuxiang Sun, Lujia Wang, Ming Liu","title":"Learning Interpretable End-to-End Vision-Based Motion Planning for\n  Autonomous Driving with Optical Flow Distillation","comments":"7 pages, 5 figures and 1 table. This paper is accepted by ICRA 2021.\n  arXiv admin note: text overlap with arXiv:2104.08862","journal-ref":null,"doi":null,"report-no":null,"categories":"cs.CV cs.RO","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Recently, deep-learning based approaches have achieved impressive performance\nfor autonomous driving. However, end-to-end vision-based methods typically have\nlimited interpretability, making the behaviors of the deep networks difficult\nto explain. Hence, their potential applications could be limited in practice.\nTo address this problem, we propose an interpretable end-to-end vision-based\nmotion planning approach for autonomous driving, referred to as IVMP. Given a\nset of past surrounding-view images, our IVMP first predicts future egocentric\nsemantic maps in bird's-eye-view space, which are then employed to plan\ntrajectories for self-driving vehicles. The predicted future semantic maps not\nonly provide useful interpretable information, but also allow our motion\nplanning module to handle objects with low probability, thus improving the\nsafety of autonomous driving. Moreover, we also develop an optical flow\ndistillation paradigm, which can effectively enhance the network while still\nmaintaining its real-time performance. Extensive experiments on the nuScenes\ndataset and closed-loop simulation show that our IVMP significantly outperforms\nthe state-of-the-art approaches in imitating human drivers with a much higher\nsuccess rate. Our project page is available at\nhttps://sites.google.com/view/ivmp.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 13:51:25 GMT"}],"update_date":"2021-04-28"}
{"id":"2104.13945","submitter":"Andriy Olenko","authors":"Phil Broadbridge, Ravindi Nanayakkara, Andriy Olenko","title":"On Multifractionality of Spherical Random Fields with Cosmological\n  Applications","comments":"27 pages, 11 figures","journal-ref":null,"doi":null,"report-no":null,"categories":"physics.gen-ph","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  This paper studies random fields on the unit sphere. Traditionally, isotropic\nGaussian random fields are considered as the underlying statistical model of\nthe cosmic microwave background (CMB) data. This paper discusses the\ngeneralized multifractional Brownian motion and its pointwise H\\\"older exponent\non the sphere. The multifractional approach is used to investigate the CMB data\nfrom the Planck mission. These data consist of CMB radiation measurements at\nnarrow angles of the sky sphere. The obtained results suggest that the\nestimated H\\\"older exponents for different CMB regions do change from location\nto location. Therefore, CMB data are multifractional. Then the developed\nmethodology is used to suggest two approaches for detecting regions with\nanomalies in cleaned CMB maps.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 10:05:30 GMT"}],"update_date":"2021-04-30"}
{"id":"2104.14485","submitter":"Tao Zhang","authors":"Tao Zhang, Shuxian Cui and Jing Si","title":"Unified products for alternative and pre-alternative algebras","comments":"25pages, 0figures. some typos are corrected. arXiv admin note:\n  substantial text overlap with arXiv:1511.08571 by other authors","journal-ref":null,"doi":null,"report-no":null,"categories":"math.RA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  The theory of unified product and extending structures for alternative and\npre-alternative algebras are developed. It is proved that the extending\nstructures of these algebras can be classified by using some non-abelian\ncohomology and deformation map theory.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 12:43:31 GMT"},{"version":"v2","created":"Sun, 22 Aug 2021 16:40:10 GMT"}],"update_date":"2021-08-24"}
{"id":"2105.03203","submitter":"Zahra Bagheri","authors":"Zahra Bagheri and Esmaeil Peyghan","title":"Quadratic and symplectic structures on 3-(Hom)-$\\rho$-Lie algebras","comments":"24 pages","journal-ref":"J. Math. Phys. 62, 081702 (2021)","doi":"10.1063/5.0057379","report-no":null,"categories":"math.RA","license":"http://arxiv.org/licenses/nonexclusive-distrib/1.0/","abstract":"  Our purpose in this paper is the generalization of the notions of quadratic\nand symplectic structures to the case of 3-(Hom)-$\\rho$-Lie algebras. We\ndescribe some properties of them by expressing the related lemmas and theorems.\nAlso, we introduce the concept of 3-pre-(Hom)-$\\rho$-Lie algebras and define\ntheir representation.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 11:23:35 GMT"},{"version":"v2","created":"Wed, 11 Aug 2021 13:31:29 GMT"}],"update_date":"2021-08-12"}
{"id":"2105.04362","submitter":"Scott Hoffmann PhD","authors":"Scott E. Hoffmann","title":"Incorporating the Coulomb potential into a finite, unitary perturbation\n  theory","comments":"17 pages, 5 figures, revised","journal-ref":null,"doi":null,"report-no":null,"categories":"quant-ph nucl-th","license":"http://creativecommons.org/licenses/by/4.0/","abstract":"  We have constructed a perturbation theory to treat interactions that can\ninclude the Coulomb interaction, describing a physical problem that is often\nencountered in nuclear physics. The Coulomb part is not treated perturbatively;\nthe exact solutions are employed. The method is an extension of the results\npresented in Hoffmann (2021 J. Math. Phys. 62 032105). It is designed to\ncalculate phase shifts directly rather than the full form of the wavefunctions\nin position space. We present formulas that allow calculation of the phase\nshifts to second order in the perturbation. The phase shift results to second\norder, for a short-range potential, were compared with the exact solution,\nwhere we found an error of third order in the coupling strength. A different\nmodel, meant as a simple approximation of nuclear scattering of a proton on\nHelium-4 and including a Coulomb potential and a spherical well, was\nconstructed to test the theory. The wavepacket scattering formalism of Hoffmann\n(2017 J. Phy. B: At. Mol. Opt. Phys 50 215302), known to give everywhere finite\nresults, was employed. We found physically acceptable results and a cross\nsection of the correct order of magnitude.\n","versions":[{"version":"v1","created":"Sun, 18 Apr 2021 06:30:46 GMT"},{"version":"v2","created":"Mon, 8 May 2023 05:12:40 GMT"}],"update_date":"2023-05-09"}
